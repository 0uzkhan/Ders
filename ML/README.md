# <center>BIL365 - Makine Ã–ÄŸrenmesi</center>

- *DeÄŸerlendirme Sistemi*:
    - Ara SÄ±nav: 40%
        - SÄ±nav: 100%
    - Final: 60%
        - SÄ±nav: 60%
        - Proje: 40%
<hr>

# Ders NotlarÄ±

## Ä°Ã§indekiler

- [BIL365 - Makine Ã–ÄŸrenmesi](#bil365---makine-Ã¶ÄŸrenmesi)
- [Ders NotlarÄ±](#ders-notlarÄ±)
  - [Ä°Ã§indekiler](#iÌ‡Ã§indekiler)
    - [GiriÅŸ ve Temel Kavramlar](#giriÅŸ-ve-temel-kavramlar)
      - [Dersin AmacÄ±](#dersin-amacÄ±)
      - [Ã–ÄŸrenme Ã‡Ä±ktÄ±larÄ±](#Ã¶ÄŸrenme-Ã§Ä±ktÄ±larÄ±)
      - [GiriÅŸ](#giriÅŸ)
      - [Neden Makine Ã–ÄŸrenimine Ä°htiyaÃ§ VardÄ±r?](#neden-makine-Ã¶ÄŸrenimine-iÌ‡htiyaÃ§-vardÄ±r)
        - [Makine Ã–ÄŸrenimi Modeli Nedir?](#makine-Ã¶ÄŸrenimi-modeli-nedir)
      - [Makine Ã–ÄŸrenmesi SÃ¼reci](#makine-Ã¶ÄŸrenmesi-sÃ¼reci)
      - [Temel Kavramlar](#temel-kavramlar)
        - [Ã–zellikler ve Ã–zellik VektÃ¶rÃ¼](#Ã¶zellikler-ve-Ã¶zellik-vektÃ¶rÃ¼)
  - [Model DeÄŸerlendirme](#model-deÄŸerlendirme)
    - [Model BaÅŸarÄ±larÄ±nÄ± Ã–lÃ§mek iÃ§in KullanÄ±lan Teknikler](#model-baÅŸarÄ±larÄ±nÄ±-Ã¶lÃ§mek-iÃ§in-kullanÄ±lan-teknikler)
    - [Ä°Ã§ Ä°Ã§e (Nested) Ã‡apraz DoÄŸrulama](#iÌ‡Ã§-iÌ‡Ã§e-nested-Ã§apraz-doÄŸrulama)
    - [Hiperparametre Optimizasyonu](#hiperparametre-optimizasyonu)
        - [Karar SÄ±nÄ±rÄ± ve Model KarmaÅŸÄ±klÄ±ÄŸÄ±](#karar-sÄ±nÄ±rÄ±-ve-model-karmaÅŸÄ±klÄ±ÄŸÄ±)
    - [Bir Modelin SeÃ§ilmesi iÃ§in Gereken Kriterler](#bir-modelin-seÃ§ilmesi-iÃ§in-gereken-kriterler)
      - [Performans ve DoÄŸruluk Kriterleri](#performans-ve-doÄŸruluk-kriterleri)
      - [Operasyonel ve Verimlilik Kriterleri](#operasyonel-ve-verimlilik-kriterleri)
      - [YapÄ±sal ve Uygulama Kriterleri](#yapÄ±sal-ve-uygulama-kriterleri)
      - [BaÄŸlama GÃ¶re Ã–nceliklendirme Ã–rnekleri](#baÄŸlama-gÃ¶re-Ã¶nceliklendirme-Ã¶rnekleri)
  - [Makine Ã¶ÄŸrenme sorunu](#makine-Ã¶ÄŸrenme-sorunu)
  - [Lineer Regresyon](#lineer-regresyon)
    - [Notasyon (sÃ¼rekli kullanÄ±lacaktÄ±r)](#notasyon-sÃ¼rekli-kullanÄ±lacaktÄ±r)
  - [Ã‡ok deÄŸiÅŸkenli (Ã§ok boyutlu) Lineer Regresyon](#Ã§ok-deÄŸiÅŸkenli-Ã§ok-boyutlu-lineer-regresyon)
    - [Maliyet fonksiyonu](#maliyet-fonksiyonu)
  - [Dereceli azaltma metodu](#dereceli-azaltma-metodu)
  - [SÄ±nÄ±flandÄ±rma (Classification) â€” GiriÅŸ](#sÄ±nÄ±flandÄ±rma-classification--giriÅŸ)
  - [SÄ±nÄ±flandÄ±rma BaÅŸarÄ± Metrikleri](#sÄ±nÄ±flandÄ±rma-baÅŸarÄ±-metrikleri)
    - [KarmaÅŸÄ±klÄ±k Matrisi (Confusion Matrix)](#karmaÅŸÄ±klÄ±k-matrisi-confusion-matrix)
    - [Temel Ã–lÃ§Ã¼tler](#temel-Ã¶lÃ§Ã¼tler)
    - [ROC EÄŸrisi ve AUC](#roc-eÄŸrisi-ve-auc)
  - [UzaklÄ±k Ã–lÃ§Ã¼leri](#uzaklÄ±k-Ã¶lÃ§Ã¼leri)
    - [Ã–klid UzaklÄ±ÄŸÄ± (Euclidean Distance)](#Ã¶klid-uzaklÄ±ÄŸÄ±-euclidean-distance)
    - [Karesel Ã–klid UzaklÄ±ÄŸÄ± (Squared Euclidean)](#karesel-Ã¶klid-uzaklÄ±ÄŸÄ±-squared-euclidean)
      - [Ã–rnek: SÄ±caklÄ±k ve Nem OranÄ± ile Ã–klid UzaklÄ±ÄŸÄ±](#Ã¶rnek-sÄ±caklÄ±k-ve-nem-oranÄ±-ile-Ã¶klid-uzaklÄ±ÄŸÄ±)
    - [Karl Pearson UzaklÄ±ÄŸÄ± (StandartlaÅŸtÄ±rÄ±lmÄ±ÅŸ Ã–klid)](#karl-pearson-uzaklÄ±ÄŸÄ±-standartlaÅŸtÄ±rÄ±lmÄ±ÅŸ-Ã¶klid)
      - [Ã–rnek: SÄ±caklÄ±k ve Nem OranÄ± ile Karl Pearson UzaklÄ±ÄŸÄ±](#Ã¶rnek-sÄ±caklÄ±k-ve-nem-oranÄ±-ile-karl-pearson-uzaklÄ±ÄŸÄ±)
    - [Manhattan (City-Block) UzaklÄ±ÄŸÄ±](#manhattan-city-block-uzaklÄ±ÄŸÄ±)
      - [Ã–rnek: SÄ±caklÄ±k ve Nem OranÄ± ile Manhattan UzaklÄ±ÄŸÄ±](#Ã¶rnek-sÄ±caklÄ±k-ve-nem-oranÄ±-ile-manhattan-uzaklÄ±ÄŸÄ±)
    - [Minkowski (Lp) UzaklÄ±ÄŸÄ±](#minkowski-lp-uzaklÄ±ÄŸÄ±)
    - [Ã–zellik Ã–lÃ§ekleme (Feature Scaling) â€” Mesafe TabanlÄ± YÃ¶ntemler](#Ã¶zellik-Ã¶lÃ§ekleme-feature-scaling--mesafe-tabanlÄ±-yÃ¶ntemler)
      - [NormalleÅŸtirme (Minâ€“Max)](#normalleÅŸtirme-minmax)
        - [Ã–rnek: Minâ€“Max NormalleÅŸtirme (min=2, max=90)](#Ã¶rnek-minmax-normalleÅŸtirme-min2-max90)
  - [K-en YakÄ±n KomÅŸu (K-NN) AlgoritmasÄ±](#k-en-yakÄ±n-komÅŸu-k-nn-algoritmasÄ±)
    - [AdÄ±mlar](#adÄ±mlar)
    - [Pratik Notlar](#pratik-notlar)
    - [Ã–rnek 1: (8,4) Ä°Ã§in K-NN SÄ±nÄ±flandÄ±rma (k = 4)](#Ã¶rnek-1-84-iÌ‡Ã§in-k-nn-sÄ±nÄ±flandÄ±rma-k--4)
    - [Ã–rnek 2: (7,8,5) Ä°Ã§in K-NN â€” Minâ€“Max Normalizasyon ile (k = 3)](#Ã¶rnek-2-785-iÌ‡Ã§in-k-nn--minmax-normalizasyon-ile-k--3)
    - [AÄŸÄ±rlÄ±klÄ± Oylama (Weighted Voting)](#aÄŸÄ±rlÄ±klÄ±-oylama-weighted-voting)
  - [Destek VektÃ¶r Makineleri (Support Vector Machines â€” SVM)](#destek-vektÃ¶r-makineleri-support-vector-machines--svm)
    - [Karar DÃ¼zlemi, Marjin ve Destek VektÃ¶rleri](#karar-dÃ¼zlemi-marjin-ve-destek-vektÃ¶rleri)
    - [Sert ve YumuÅŸak Kenar FormÃ¼lasyonu](#sert-ve-yumuÅŸak-kenar-formÃ¼lasyonu)
    - [Kernel Hilesi (Kernel Trick)](#kernel-hilesi-kernel-trick)
    - [Avantajlar (Ã¶zet)](#avantajlar-Ã¶zet)
  - [Karar AÄŸaÃ§larÄ±](#karar-aÄŸaÃ§larÄ±)
    - [Dallanma Kriterleri](#dallanma-kriterleri)
    - [Ã–rnek: KÃ¶k DÃ¼ÄŸÃ¼m SeÃ§imi ve Dallanma Ãœzerine](#Ã¶rnek-kÃ¶k-dÃ¼ÄŸÃ¼m-seÃ§imi-ve-dallanma-Ã¼zerine)
    - [Karar aÄŸacÄ± oluÅŸturma adÄ±mlarÄ±](#karar-aÄŸacÄ±-oluÅŸturma-adÄ±mlarÄ±)
    - [En iyi bÃ¶len niteliÄŸi belirleme (Ä°yilik Fonksiyonu)](#en-iyi-bÃ¶len-niteliÄŸi-belirleme-iÌ‡yilik-fonksiyonu)
    - [Entropi](#entropi)
      - [Ã–rnek: Entropi HesabÄ±](#Ã¶rnek-entropi-hesabÄ±)
      - [Ã–rnek: MEMNUN Entropisi](#Ã¶rnek-memnun-entropisi)
    - [Bilgi KazancÄ± (ID3 / C4.5)](#bilgi-kazancÄ±-id3--c45)
      - [Ä°ÅŸlem AdÄ±mlarÄ±](#iÌ‡ÅŸlem-adÄ±mlarÄ±)
      - [Ã–rnek: Bilgi KazancÄ± HesabÄ± (BORÃ‡ Ã¶zelliÄŸi)](#Ã¶rnek-bilgi-kazancÄ±-hesabÄ±-borÃ§-Ã¶zelliÄŸi)
      - [Ã–rnek: Bilgi KazancÄ± â€” MEMNUN Veri KÃ¼mesi](#Ã¶rnek-bilgi-kazancÄ±--memnun-veri-kÃ¼mesi)
  - [Uygulama: Hava Problemi (ID3 / C4.5)](#uygulama-hava-problemi-id3--c45)
    - [AdÄ±m 1: Birinci Dallanma](#adÄ±m-1-birinci-dallanma)
    - [AdÄ±m 2: HAVA = â€œgÃ¼neÅŸliâ€ dalÄ± iÃ§in dallanma](#adÄ±m-2-hava--gÃ¼neÅŸli-dalÄ±-iÃ§in-dallanma)
    - [AdÄ±m 3: HAVA = â€œbulutluâ€ dalÄ± iÃ§in dallanma](#adÄ±m-3-hava--bulutlu-dalÄ±-iÃ§in-dallanma)
    - [SonuÃ§: OluÅŸturulan Karar AÄŸacÄ±](#sonuÃ§-oluÅŸturulan-karar-aÄŸacÄ±)
    - [C4.5 AlgoritmasÄ±](#c45-algoritmasÄ±)
    - [SÄ±nÄ±flandÄ±rma ve Regresyon AÄŸaÃ§larÄ± (CART)](#sÄ±nÄ±flandÄ±rma-ve-regresyon-aÄŸaÃ§larÄ±-cart)
      - [BÃ¶lme (Split) NasÄ±l SeÃ§ilir?](#bÃ¶lme-split-nasÄ±l-seÃ§ilir)
      - [SÄ±nÄ±flandÄ±rma iÃ§in Ã–lÃ§Ã¼tler](#sÄ±nÄ±flandÄ±rma-iÃ§in-Ã¶lÃ§Ã¼tler)
      - [Twoing AlgoritmasÄ± (detay)](#twoing-algoritmasÄ±-detay)
        - [Ã–rnek: Twoing ile Karar AÄŸacÄ± (MEMNUN â€” 11 gÃ¶zlem)](#Ã¶rnek-twoing-ile-karar-aÄŸacÄ±-memnun--11-gÃ¶zlem)
      - [Gini AlgoritmasÄ± (detay)](#gini-algoritmasÄ±-detay)
        - [Ã–rnek: Gini ile Karar AÄŸacÄ±](#Ã¶rnek-gini-ile-karar-aÄŸacÄ±)
        - [SayÄ±sal DeÄŸerler iÃ§in Gini AlgoritmasÄ±](#sayÄ±sal-deÄŸerler-iÃ§in-gini-algoritmasÄ±)
      - [Regresyon iÃ§in Ã–lÃ§Ã¼tler](#regresyon-iÃ§in-Ã¶lÃ§Ã¼tler)
        - [Regresyon AÄŸaÃ§larÄ± (detay)](#regresyon-aÄŸaÃ§larÄ±-detay)
          - [Ã–rnek (kÃ¼Ã§Ã¼k veri seti)](#Ã¶rnek-kÃ¼Ã§Ã¼k-veri-seti)
      - [Regresyon BaÅŸarÄ± Metrikleri](#regresyon-baÅŸarÄ±-metrikleri)
        - [Ortalama Kare Hata (MSE) ve KÃ¶k Ortalama Kare Hata (RMSE)](#ortalama-kare-hata-mse-ve-kÃ¶k-ortalama-kare-hata-rmse)
        - [Belirleme KatsayÄ±sÄ± (RÂ²)](#belirleme-katsayÄ±sÄ±-r)
      - [Uygulama NotlarÄ±](#uygulama-notlarÄ±)
    - [Kural Ã‡Ä±kartma (Rule Extraction)](#kural-Ã§Ä±kartma-rule-extraction)
    - [Bilinmeyen Ã–znitelik DeÄŸerleri (Missing Values)](#bilinmeyen-Ã¶znitelik-deÄŸerleri-missing-values)
    - [Ezber (Overfitting) ve Budama](#ezber-overfitting-ve-budama)
      - [AÄŸaÃ§ Budama](#aÄŸaÃ§-budama)
        - [Ã–rnek: Hata toleransÄ± ile budama](#Ã¶rnek-hata-toleransÄ±-ile-budama)

### GiriÅŸ ve Temel Kavramlar

#### Dersin AmacÄ±

Makine Ã¶ÄŸrenimi (ML) gÃ¼nÃ¼mÃ¼zÃ¼n IoT ve BÃ¼yÃ¼k Veri uygulamalarÄ±nda anormallik algÄ±lama, tahmin, sÄ±nÄ±flandÄ±rma, kÃ¼meleme, iliÅŸkilendirme, boyut azaltma, desen tanÄ±ma, eÄŸilim analizi ve Ã¶ngÃ¶rÃ¼cÃ¼ bakÄ±m gibi birÃ§ok alanda kullanÄ±lmaktadÄ±r. Bu dersin amacÄ±, makine Ã¶ÄŸrenmesi kavramlarÄ±nÄ± teorik temelleri ve pratik uygulamalarÄ±yla birlikte kapsamlÄ± ÅŸekilde Ã¶ÄŸretmektir.

**Ã–nemli Noktalar**

- KullanÄ±m alanlarÄ±: anormallik tespiti, regresyon (tahmin), sÄ±nÄ±flandÄ±rma, kÃ¼meleme, iliÅŸkilendirme kurallarÄ±, boyut azaltma, desen tanÄ±ma, eÄŸilim analizi, Ã¶ngÃ¶rÃ¼cÃ¼ bakÄ±m.
- IoT ve BÃ¼yÃ¼k Veri gibi modern veri ekosistemleri ML uygulamalarÄ±nÄ±n Ã¶nemini artÄ±rÄ±r.
- Ders hem algoritma/matematiksel model geliÅŸtirme (optimizasyon, modelleme) hem de pratik uygulama Ã¶rneklerine odaklanacaktÄ±r.

**Ã–nemli TanÄ±mlar**

- Makine Ã–ÄŸrenmesi: Veriden Ã¶ÄŸrenen ve performansÄ±nÄ± veriyle iyileÅŸtirebilen algoritmalar ve modeller bÃ¼tÃ¼nÃ¼.
- Anormallik AlgÄ±lama: Normal davranÄ±ÅŸtan sapmalarÄ± tespit etmeye yÃ¶nelik yÃ¶ntemler.
- Ã–ngÃ¶rÃ¼cÃ¼ BakÄ±m: SensÃ¶r verileri ve modeller kullanÄ±larak ekipman arÄ±zalarÄ±nÄ± Ã¶nceden tahmin etme sÃ¼reci.

#### Ã–ÄŸrenme Ã‡Ä±ktÄ±larÄ±

- Makine Ã¶ÄŸrenmesi kavramÄ±nÄ± Ã¶ÄŸrenir.
- Makine Ã¶ÄŸrenmesi tÃ¼rlerini Ã¶ÄŸrenir.
- En Ã§ok kullanÄ±lan makine Ã¶ÄŸrenmesi algoritmalarÄ±nÄ± Ã¶ÄŸrenir.
- FarklÄ± alanlardaki problemlere makine Ã¶ÄŸrenmesi Ã§Ã¶zÃ¼mlerini uygular ve yorumlar.
- FarklÄ± makine Ã¶ÄŸrenmesi yÃ¶ntemlerinin avantaj ve dezavantajlarÄ±nÄ± kÄ±yaslayarak yorumlar.
- Makine Ã¶ÄŸrenmesi sonuÃ§larÄ±nÄ± (model Ã§Ä±ktÄ±larÄ±) yorumlar.

#### GiriÅŸ

- Ã–ÄŸrenme, deneyim yoluyla bilgi, anlayÄ±ÅŸ veya beceri kazanma; davranÄ±ÅŸsal eÄŸilimlerin deneyime gÃ¶re dÃ¼zeltilmesi, gÃ¼ncellenmesi ve iyileÅŸtirilmesidir.
- Bilgi sayÄ±sallaÅŸtÄ±rÄ±lÄ±p veriye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼ÄŸÃ¼nde, veri toplayan, sÄ±nÄ±flandÄ±ran, birleÅŸtiren ve kÄ±yaslama yapan algÄ±layÄ±cÄ±larÄ±n doÄŸru karar verebilmesi iÃ§in insan ve hayvanlardaki algÄ±lama/Ã¶ÄŸrenme sÃ¼reÃ§lerinin iyi anlaÅŸÄ±lmasÄ± Ã¶nemlidir.
- DoÄŸadaki Ã¶rnekler: baykuÅŸlar bir farenin ayak sesini kilometrelerce Ã¶teden algÄ±layabilir; filler uzak mesafelerden gelen sismik sinyalleri hisseder; yaban arÄ±larÄ± tehditleri Ã¶nceden fark edip topluca saldÄ±rÄ±ya geÃ§ebilir â€” bunlar algÄ±lama ve Ã¶ÄŸrenmenin farklÄ± Ã¶rÃ¼ntÃ¼leridir.

- Bu bÃ¶lÃ¼mde hedef: makine Ã¶ÄŸrenmesinin temel kavramlarÄ±nÄ±, Ã¶ÄŸrenmenin tanÄ±mÄ±nÄ± ve biyolojik algÄ±lama Ã¶rneklerinin ML iÃ§in taÅŸÄ±dÄ±ÄŸÄ± ilhamÄ± kÄ±sa ve somut bir ÅŸekilde sunmak.

**Makine Ã–ÄŸrenimi â€” KullanÄ±m DurumlarÄ±**

- Anormallik algÄ±lama
- Tahmin (regresyon)
- SÄ±nÄ±flandÄ±rma
- KÃ¼meleme
- Ä°liÅŸkilendirme (association rules)
- Boyut azaltma
- Desen bulma
- EÄŸilim ya da yÃ¶n bulma
- Ã–ngÃ¶rÃ¼cÃ¼ bakÄ±m

Bu kullanÄ±m durumlarÄ±, geliÅŸmiÅŸ biliÅŸim uygulamalarÄ± (IoT, BÃ¼yÃ¼k Veri vb.) iÃ§inde makine Ã¶ÄŸrenmesinin yaygÄ±n olarak kullanÄ±lmasÄ±nÄ±n nedenlerini gÃ¶sterir.

**Ã–rnek â€” E-posta (Spam) Filtreleme**

- Kural tabanlÄ± yaklaÅŸÄ±m: GeÃ§erli e-posta ile istenmeyen postalarÄ± ayÄ±rt etmek iÃ§in sabit kurallar (Ã¶r. sahte baÅŸlÄ±k, belirli kelimeler) yazÄ±labilir.
- Zorluklar: Hangi metnin geÃ§erli olduÄŸunu doÄŸru ÅŸekilde tanÄ±mlayan kurallar yazmak zordur; bu, ya Ã§ok sayÄ±da kaÃ§Ä±rÄ±lmÄ±ÅŸ spam (false negatives) ya da yanlÄ±ÅŸ yere bloklanan geÃ§erli e-postalar (false positives) ile sonuÃ§lanÄ±r.
- Uyarlanma problemi: Spam gÃ¶nderenler stratejilerini deÄŸiÅŸtirerek kurallarÄ± atlatabilir; kurallarÄ± yazmak ve gÃ¼ncel tutmak yorucu ve etkisiz hale gelir.
- Neden ML: Makine Ã¶ÄŸrenimi sistemleri veriden Ã¶ÄŸrenerek farklÄ± Ã¶rÃ¼ntÃ¼leri yakalayabilir, zamanla gÃ¼ncellenebilir ve kural yazma yÃ¼kÃ¼nÃ¼ azaltarak daha esnek bir Ã§Ã¶zÃ¼m sunar.

#### Neden Makine Ã–ÄŸrenimine Ä°htiyaÃ§ VardÄ±r?

- TanÄ±m: Makine Ã¶ÄŸrenimi, Ã¶rnek veriler veya geÃ§miÅŸ deneyimleri kullanarak belirli bir performans kriterini optimize etmek iÃ§in bilgisayarlarÄ± programlama yaklaÅŸÄ±mÄ±dÄ±r.
- AmaÃ§: OlasÄ±lÄ±ksal modeller kurarak bÃ¼yÃ¼k veri kÃ¼tlelerinden faydalÄ± bilgileri otomatik Ã§Ä±karmak ve karar sÃ¼reÃ§lerini desteklemek.
- Uygunluk: Genel bir teorinin/kapalÄ± form Ã§Ã¶zÃ¼mÃ¼n olmadÄ±ÄŸÄ±; ancak Ã§ok miktarda verinin bulunduÄŸu alanlarda Ã¶zellikle etkilidir.
- ML Ã§alÄ±ÅŸmasÄ±: Deneyimle bazÄ± gÃ¶revlerde performansÄ± artÄ±ran algoritmalarÄ±n geliÅŸtirilmesidir; Ã§Ä±karÄ±m, model uydurma (fit) ve Ã¶rneklerden Ã¶ÄŸrenme yoluyla veriden teoriyi otomatik olarak â€œÃ¶ÄŸrenmekâ€.

- Ã–rnekler dÄ±ÅŸÄ±nda bazÄ± gÃ¶revler iyi tanÄ±mlanamaz (Ã¶rn. yÃ¼zlerin veya insanlarÄ±n tanÄ±nmasÄ±); bu yÃ¼zden kural tabanlÄ± aÃ§Ä±k kodlama yetersiz kalÄ±r.
- BÃ¼yÃ¼k miktarda veride gizli iliÅŸkiler ve korelasyonlar bulunabilir; bunlarÄ± gÃ¼venilir biÃ§imde ortaya Ã§Ä±karmak iÃ§in otomatik/ML tabanlÄ± yaklaÅŸÄ±mlar gerekir.
- Belirli bir sorun hakkÄ±nda aÃ§Ä±k kurallar yazmak iÃ§in gereken bilgi miktarÄ± insanlar iÃ§in Ã§ok bÃ¼yÃ¼k olabilir (Ã¶rn. tÄ±bbi teÅŸhis); ML bu yÃ¼kÃ¼ veriden Ã¶ÄŸrenerek azaltÄ±r.
- Ortamlar ve veri daÄŸÄ±lÄ±mlarÄ± zamanla deÄŸiÅŸir (concept drift); sistemleri â€œelleâ€ sÃ¼rekli yeniden tasarlamak zor ve maliyetlidir, ML modelleri gÃ¼ncellenerek uyum saÄŸlayabilir.

**Makine Ã¶ÄŸrenmesinde:**

- Ã–ÄŸrenme gÃ¶revi: Ne Ã¶ÄŸrenmek ya da tahmin etmek istiyoruz?
- Veriler ve varsayÄ±mlar: Elimizde hangi veriler var, kaliteleri nedir; problem hakkÄ±nda neleri varsayabiliriz?
- Temsil (representation): Ã–rneklerin/Ã¶zniteliklerin uygun temsili nedir? (Ã¶zellik mÃ¼hendisliÄŸi, embedding vb.)
- YÃ¶ntem ve tahmin: Uygun hipotez sÄ±nÄ±fÄ± ve Ã¶ÄŸrenme algoritmasÄ± nedir; tahminlerimizi gÃ¶zlenen sonuÃ§lara gÃ¶re ayarlayabiliyor muyuz? (optimizasyon)
- DeÄŸerlendirme: YÃ¶ntem ne kadar iyi; alternatif bir yaklaÅŸÄ±m/model daha iyi performans gÃ¶sterebilir mi? (doÄŸruluk, F1, MSE; Ã§apraz doÄŸrulama)

Temel gÃ¶rev tÃ¼rleri:

- SÄ±nÄ±flandÄ±rma: Bir Ã¶ÄŸenin sÄ±nÄ±fÄ±nÄ±n tahmini (denetimli).
- Ã–ngÃ¶rÃ¼ (regresyon): SayÄ±sal bir parametre deÄŸerinin tahmini (denetimli).
- Karakterizasyon: Ã–ÄŸe gruplarÄ±nÄ± tanÄ±mlayan hipotezlerin/kurallarÄ±n bulunmasÄ± (Ã¶rn. iliÅŸkilendirme kurallarÄ±).
- KÃ¼meleme: EtiketlenmemiÅŸ veri kÃ¼mesinin ortak Ã¶zelliklere gÃ¶re kÃ¼melere ayrÄ±lmasÄ± (denetimsiz).
##### Makine Ã–ÄŸrenimi Modeli Nedir?

Bir makine Ã¶ÄŸrenimi modeli; belirli bir gÃ¶revi (tahmin, sÄ±nÄ±flandÄ±rma vb.) veriden Ã¶ÄŸrenerek yerine getiren ve girdileri yanÄ±tlara dÃ¶nÃ¼ÅŸtÃ¼ren hesaplama sistemidir. AmaÃ§, iÅŸ sonuÃ§larÄ±nÄ± iyileÅŸtirecek iÃ§gÃ¶rÃ¼ler Ã¼retmek ve hangi parametrelerin/Ã¶zniteliklerin daha etkili olduÄŸunu veriye dayanarak belirlemektir.

Disiplinler (Ã§ok-disiplinli temel):

- MÃ¼hendislikler: Bilgisayar, Elektronik, Makine
- Matematik: OlasÄ±lÄ±k, Ä°statistik, NÃ¼merik analiz, Ä°ÅŸaret iÅŸleme, UygulamalÄ± matematik, Stokastik, Algoritma
- YaÅŸam/DoÄŸa Bilimleri: Antropoloji, Zooloji, Biyoloji, Bakteriyoloji, Genetik, NÃ¶robiyoloji, Paleontoloji
- Uzay bilimleri

**Ã–rnek â€” Kedi/KÃ¶pek SÄ±nÄ±flandÄ±rma**

- AmaÃ§: Kedi ve kÃ¶pek gÃ¶rsellerini doÄŸru sÄ±nÄ±flara ayÄ±rmak.
- Aday Ã¶znitelikler:
    - Her hayvanÄ±n kaÃ§ gÃ¶zÃ¼ vardÄ±r?
    - Her hayvanÄ±n gÃ¶z rengi nedir?
    - Her bir hayvanÄ±n boyu kaÃ§tÄ±r?
    - Her bir hayvanÄ±n uzunluÄŸu nedir?
    - Her bir hayvanÄ±n bacak uzunluklarÄ± nedir?
    - Her bir hayvanÄ±n aÄŸÄ±rlÄ±ÄŸÄ± nedir?
    - Her hayvan genellikle ne yer?
- Not: GÃ¶rÃ¼ntÃ¼ problemlerinde bu tÃ¼r Ã¶znitelikler Ã§oÄŸu zaman doÄŸrudan Ã¶lÃ§Ã¼lemez; klasik yaklaÅŸÄ±mlarda Ã¶zellik Ã§Ä±karÄ±mÄ± gerekirken, derin Ã¶ÄŸrenme (CNN) ham piksellerden ayÄ±rt edici temsilleri otomatik olarak Ã¶ÄŸrenebilir.

- Ã–znitelik vektÃ¶rÃ¼ ve kurallar: YukarÄ±daki sorularÄ±n her biri bir Ã¶zniteliÄŸe karÅŸÄ±lÄ±k gelir ve x âˆˆ â„^d vektÃ¶rÃ¼nde temsil edilir. Basit kural tabanlÄ± sÄ±nÄ±flandÄ±rmada, Ã¶rn. â€œboy > 30 cm ve aÄŸÄ±rlÄ±k > 2 kg ise kÃ¶pekâ€ gibi eÅŸik kurallarÄ± kullanÄ±labilir.
- Karar aÄŸaÃ§larÄ±: if/else if/else yapÄ±larÄ±yla kurulan karar aÄŸaÃ§larÄ±, bu kurallarÄ± hiyerarÅŸik biÃ§imde uygular ve Ã¶rneÄŸin hangi kategoriye girdiÄŸini test eder.
- MLâ€™nin katkÄ±sÄ±: Veriyi farklÄ± algoritmalarla iÅŸleyip hangi Ã¶zniteliklerin sÄ±nÄ±flandÄ±rmada daha Ã¶nemli/etkin olduÄŸunu ortaya koyar (Ã¶r. aÄŸaÃ§ tabanlÄ± Ã¶nem, lojistik regresyon katsayÄ±larÄ±, permÃ¼tasyon Ã¶nemi).
- Ã–zellik seÃ§imi ve boyut indirgeme: Sonuca etkisi yÃ¼ksek birkaÃ§ Ã¶zniteliÄŸi seÃ§mek (Ã¶zellik seÃ§imi) veya temsili 2â€“3 boyuta indirgemek (Ã¶r. PCA ile) modeli basitleÅŸtirir, genellikle daha iyi genelleme ve daha kararlÄ± doÄŸruluk saÄŸlar.

> \
> ![Kedi/KÃ¶pek â€” CNN ile Ã¶zellik Ã§Ä±karÄ±mÄ±](Images/1.jpg)
>
> AÃ§Ä±klama: Solda giriÅŸ gÃ¶rÃ¼ntÃ¼leri (kedi/kÃ¶pek), ortada evriÅŸim katmanlarÄ± (Ã¶ÄŸrenilen filtreler; kenarlar/tekstÃ¼rler â†’ parÃ§alar â†’ bÃ¼tÃ¼n), saÄŸda yoÄŸun (fully-connected) katman ve Ã§Ä±ktÄ±. Ä°kili sÄ±nÄ±flandÄ±rmada son katman genelde sigmoid olasÄ±lÄ±k verir: p(cat|x) = Ïƒ(wáµ€h + b). KayÄ±p: Ã§apraz entropi L = âˆ’[y log p + (1âˆ’y) log(1âˆ’p)].

**Veri GÃ¶rselleÅŸtirme â€” x1, x2 Ã¶rneÄŸi**

- Veri boyutu bÃ¼yÃ¼dÃ¼kÃ§e insanlarÄ±n ham sayÄ± tablosundan faydalÄ± bilgi Ã§Ä±karmasÄ± zorlaÅŸÄ±r.
- Verinin sunuluÅŸ biÃ§imi (uzun tablolar, ham sayÄ±lar) anlamayÄ± gÃ¼Ã§leÅŸtirebilir.
- AynÄ± veriyi bir grafik Ã¼zerinde (Ã¶r. saÃ§Ä±lÄ±m diyagramÄ±) gÃ¶stermek; kÃ¼meleri, eÄŸilimleri ve olasÄ± karar sÄ±nÄ±rlarÄ±nÄ± daha anlaÅŸÄ±lÄ±r hale getirir.

>\
> ![x1-x2 â€” sÄ±nÄ±flarÄ±n gÃ¶rselleÅŸtirilmesi](Images/2.jpg)
>
> AÃ§Ä±klama: Solda x1, x2 ve sÄ±nÄ±f etiketini iÃ§eren bir tablo; saÄŸda x1â€“x2 dÃ¼zleminde Ã¶rneklerin daÄŸÄ±lÄ±mÄ±. GÃ¶rselleÅŸtirme sÄ±nÄ±f kÃ¼melerini sezgisel hale getirir ve model seÃ§imi/karar sÄ±nÄ±rÄ± tasarÄ±mÄ±na yardÄ±mcÄ± olur.

**Boyut ve Boyut Azaltma**

- YÃ¼ksek boyut (Ã§ok sayÄ±da Ã¶znitelik) insanlar iÃ§in kavramsal olarak zordur; bilgisayarlar iÅŸleyebilir fakat veri gereksinimi artar ve "boyutlanma laneti" riski doÄŸar.
- Ä°ki yaklaÅŸÄ±m: (a) Boyut azaltÄ±p problemi kendin Ã§Ã¶zmek, (b) Boyut azaltmadan makine Ã¶ÄŸrenmesi ile bilgisayara Ã§Ã¶zdÃ¼rmek. Hangisinin avantajlÄ± olduÄŸu probleme, veri miktarÄ±na ve bilgi kaybÄ± riskine baÄŸlÄ±dÄ±r.
- Boyut azaltmak faydalÄ± bilgileri de ortadan kaldÄ±rabilir; bu nedenle seÃ§iminizi doÄŸrulamak iÃ§in her zaman Ã§apraz doÄŸrulama ile performans karÅŸÄ±laÅŸtÄ±rmasÄ± yapÄ±n.
- Pratik Ã¶neri:
    - HÄ±zlÄ± keÅŸif ve gÃ¶rselleÅŸtirme iÃ§in boyut azaltma (PCA, tâ€‘SNE, UMAP) kullanÄ±n.
    - Modelleme iÃ§in Ã¶nce gÃ¼Ã§lÃ¼ bir temel model deneyin (aÄŸaÃ§ tabanlÄ± yÃ¶ntemler, dÃ¼zenlemeli lineer modeller, derin aÄŸlar) ve Ã¶znitelik Ã¶nemleri/katkÄ±larÄ±nÄ± inceleyin.
    - Gerekirse denetimli Ã¶zellik seÃ§imi veya boyut azaltma uygulayÄ±n; bilgi kaybÄ±nÄ± ve genelleme baÅŸarÄ±mÄ±nÄ± metriklerle izleyin.

**Genelleme Kapasitesi**

- AmaÃ§: Modelin yalnÄ±zca gÃ¶rdÃ¼ÄŸÃ¼ Ã¶rnekleri ezberlemesi deÄŸil, gÃ¶rmediÄŸi yeni Ã¶rneklerde de yÃ¼ksek performans gÃ¶stermesidir (genelleme).
- SatranÃ§ Ã¶rneÄŸi: BaÅŸlarda yendiÄŸiniz bilgisayarÄ± zamanla yenememeniz Ã§oÄŸunlukla sizin kÃ¶tÃ¼leÅŸmeniz deÄŸil, modelin daha iyi stratejiler Ã¶ÄŸrenip farklÄ± rakiplere ve pozisyonlara da uygulayabilmesidir.
- â€œSiz mi kÃ¶tÃ¼ oynuyorsunuz, bilgisayar mÄ± Ã¶ÄŸreniyor?â€ â€” Bilgisayar deneyimledikÃ§e Ã¶ÄŸrendiÄŸi Ã¶rÃ¼ntÃ¼leri genelleyip yeni durumlara aktarÄ±r; bu genelleme kapasitesidir.
- Tehlikeler: AÅŸÄ±rÄ± uyum (overfitting) â†’ eÄŸitimde Ã§ok iyi, yeni veride zayÄ±f; yetersiz uyum (underfitting) â†’ eÄŸitim ve testte birden zayÄ±f.
- NasÄ±l geliÅŸtiririz? Yeterli ve Ã§eÅŸitli veri, uygun model kapasitesi, dÃ¼zenlileÅŸtirme (L1/L2, dropout), erken durdurma, veri artÄ±rma (augmentation), Ã§apraz doÄŸrulama ve ayrÄ± doÄŸrulama/test setleri.
- Ã–lÃ§Ã¼m: EÄŸitimâ€“doÄŸrulama/test hatalarÄ±; genelleme aÃ§Ä±ÄŸÄ± = eÄŸitim hatasÄ± âˆ’ doÄŸrulama/test hatasÄ±. Hedef, bu farkÄ± ve toplam hatayÄ± kÃ¼Ã§Ã¼ltmektir.

**Ã–ÄŸrenme TÃ¼rleri**

- â€œBilgisayar daha iyi bir konuma geldiÄŸini nasÄ±l bilir?â€ â€” AldÄ±ÄŸÄ± geri bildirim tÃ¼rÃ¼ne gÃ¶re: etiketli hedefe gÃ¶re hata (denetimli), verinin iÃ§ yapÄ±sÄ±na dair Ã¶lÃ§Ã¼tler (denetimsiz) ya da Ã¶dÃ¼l sinyali (takviyeli).
- DanÄ±ÅŸmanlÄ± (Supervised) Ã–ÄŸrenme:
    - Girdi x ile hedef y verilir; amaÃ§ kaybÄ± (Ã¶rn. MSE, Ã§apraz entropi) minimize ederek doÄŸru fonksiyonu/karar sÄ±nÄ±rÄ±nÄ± Ã¶ÄŸrenmektir.
    - Performans, doÄŸrulama/test metrikleriyle izlenir (doÄŸruluk, F1, RMSE). Ã–rnekler: spam tespiti, fiyat tahmini.
    - EÄŸitim setinde her giriÅŸ iÃ§in doÄŸru Ã§Ä±kÄ±ÅŸ (etiket) verilir; algoritma bu veriden Ã¶ÄŸrenerek olasÄ± tÃ¼m giriÅŸler iÃ§in doÄŸru cevabÄ± vermeye (genellemeye) Ã§alÄ±ÅŸÄ±r.
    - GÃ¶revler: sÄ±nÄ±flandÄ±rma (kategori tahmini) ve regresyon (sayÄ±sal deÄŸer tahmini).
    - Veri bÃ¶lmeleri: eÄŸitim/doÄŸrulama/test; ayrÄ±ca k-katlÄ± Ã§apraz doÄŸrulama ile gÃ¼venilir performans tahmini.
    - Geri bildirim: Etiketâ€“tahmin farkÄ±ndan hesaplanan kayÄ±p ve gradyanla parametreler gÃ¼ncellenir (Ã¶rn. gradyan iniÅŸi). AÅŸÄ±rÄ± uyumu Ã¶nlemek iÃ§in dÃ¼zenlileÅŸtirme ve erken durdurma kullanÄ±lÄ±r.
    
> \
> ![DanÄ±ÅŸmanlÄ± Ã–ÄŸrenme â€” ÅŸema](Images/4.jpg)
>
> AÃ§Ä±klama: Etiketli veri (x, y) â†’ model f_Î¸ â†’ kayÄ±p L(y, f_Î¸(x)) â†’ optimizasyon (Ã¶ÄŸrenme) â†’ deÄŸerlendirme (train/val/test). AmaÃ§, gÃ¶rÃ¼lmeyen Ã¶rneklerde de dÃ¼ÅŸÃ¼k hata (iyi genelleme) elde etmektir.

**DanÄ±ÅŸmanlÄ± Ã–ÄŸrenme â€” Regresyon**

- Hedef: SÃ¼rekli sayÄ±sal deÄŸiÅŸkenin tahmini (Ã¶r. fiyat, sÄ±caklÄ±k, talep, Ã¶mÃ¼r).
- KayÄ±p fonksiyonlarÄ±: MSE (ortalama karesel hata), MAE (ortalama mutlak hata). MAE aykÄ±rÄ± deÄŸerlere karÅŸÄ± daha dayanÄ±klÄ±dÄ±r; MSE gradyan tabanlÄ± optimizasyonla uyumludur.
- DeÄŸerlendirme metrikleri: RMSE, MAE, RÂ². RÂ² â‰ˆ 1â€™e yakÄ±nsa iyi uyum; < 0 ise basit ortalama tahmininden kÃ¶tÃ¼.
- Modeller: DoÄŸrusal regresyon, polinomsal Ã¶zellikler; dÃ¼zenlileÅŸtirme (Ridge=L2, Lasso=L1, Elastic Net); aÄŸaÃ§/ansamble (Random Forest, GBM), SVR, yapay sinir aÄŸlarÄ±.
- Ã–n iÅŸleme: Ã–lÃ§ekleme (standart/minâ€‘max), kategorik kodlama, aykÄ±rÄ± deÄŸer analizi, Ã¶zellik mÃ¼hendisliÄŸi.
- DeÄŸerlendirme: Train/val/test bÃ¶lmesi; kâ€‘katlÄ± Ã§apraz doÄŸrulama; artÄ±k (residual) analizi ile hatalarÄ±n desenini kontrol etme; biasâ€‘variance dengesi.

- Temel ilkeler (Ã¶zet):
    - Ã‡Ä±kÄ±ÅŸ kategorik deÄŸil, gerÃ§ek (sÃ¼rekli) deÄŸerlerdir.
    - Xâ€™in bir fonksiyonu olan Yâ€™yi tanÄ±mlayan bir model seÃ§ilir: y â‰ˆ f_Î¸(x).
    - Tahmin edilen ile gerÃ§ek Ã§Ä±kÄ±ÅŸ arasÄ±ndaki farkÄ± minimize eden parametre/katsayÄ±lar (Î¸) Ã¶ÄŸrenilir.

> \
> ![DanÄ±ÅŸmanlÄ± Ã–ÄŸrenme â€” Regresyon](Images/5.jpg)
>
> AÃ§Ä±klama: SÃ¼rekli hedef y iÃ§in f_Î¸(x) ile tahmin; kaybÄ± (MSE/MAE) minimize ederek parametreler Ã¶ÄŸrenilir. RÂ²/RMSE ile performans raporlanÄ±r; amaÃ§, gÃ¶rÃ¼lmeyen veride dÃ¼ÅŸÃ¼k hata ve kararlÄ± genellemedir.

> \
> ![Regresyon â€” Ã¶rnek gÃ¶rsel](Images/6.jpg)
>
> AÃ§Ä±klama: Regresyonda hedef gerÃ§ek deÄŸerlerdir; bir fonksiyon ailesi seÃ§ilir ve parametreler, tahminâ€“gerÃ§ek farkÄ±nÄ± en aza indirecek ÅŸekilde Ã¶ÄŸrenilir (ERM: ampirik risk minimizasyonu).

**DanÄ±ÅŸmanlÄ± Ã–ÄŸrenme â€” SÄ±nÄ±flandÄ±rma**

- Hedef: AyrÄ±k/kategorik sÄ±nÄ±f etiketinin tahmini (Ã¶r. spam/ham, hastalÄ±k var/yok, tÃ¼r A/B/C).
- KayÄ±p fonksiyonu: Ã‡apraz entropi (log loss); ikili sÄ±nÄ±flandÄ±rmada sigmoid + BCE, Ã§ok sÄ±nÄ±flÄ±da softmax + CCE.
- Metrikler: DoÄŸruluk (accuracy), kesinlik (precision), duyarlÄ±lÄ±k (recall), F1; dengesiz veride ROCâ€‘AUC/PRâ€‘AUC ve sÄ±nÄ±f bazlÄ± raporlar kullanÄ±n.
- Modeller: Lojistik regresyon, kâ€‘NN, Naive Bayes, SVM, karar aÄŸaÃ§larÄ±, Random Forest, Gradient Boosting (XGBoost/LightGBM), yapay sinir aÄŸlarÄ±.
- Ã–n iÅŸleme: Ã–lÃ§ekleme (Ã¶zellikle LR/SVM/kâ€‘NN), kategorik kodlama, sÄ±nÄ±f dengesi (class_weight, yeniden Ã¶rnekleme/SMOTE), Ã¶zellik seÃ§imi.
- DeÄŸerlendirme: KarÄ±ÅŸÄ±klÄ±k matrisi, eÅŸik ayarÄ± (threshold tuning), kalibrasyon (Platt/Isotonic), kâ€‘katlÄ± Ã§apraz doÄŸrulama.

> \
> ![DanÄ±ÅŸmanlÄ± Ã–ÄŸrenme â€” SÄ±nÄ±flandÄ±rma](Images/7.jpg)
>
> AÃ§Ä±klama: Girdi Ã¶zellikleri â†’ model â†’ sÄ±nÄ±f olasÄ±lÄ±klarÄ±/prediksiyon; Ã§apraz entropi ile Ã¶ÄŸrenme, F1/ROCâ€‘AUC gibi metriklerle deÄŸerlendirme. Hedef, yeni Ã¶rneklerde yÃ¼ksek doÄŸruluk ve dengeli hata profili.

- KÄ±sa yanÄ±tlar:
    - SÄ±nÄ±flandÄ±rma nedir? Her veri noktasÄ± iÃ§in en olasÄ±/doÄŸru sÄ±nÄ±fÄ± tahmin etme problemidir.
    - Neye dayanÄ±r? Etiketli verilerden, giriÅŸ Ã¶zniteliklerini kullanarak Ã¶ÄŸrenir.
    - Neden kullanÄ±lÄ±r? Daha Ã¶nce gÃ¶rÃ¼lmemiÅŸ verileri doÄŸru sÄ±nÄ±flara ayÄ±rmak ve otomatik karar vermek iÃ§in.
    - NasÄ±l yapÄ±lÄ±r? BirÃ§ok algoritma (LR, SVM, aÄŸaÃ§lar, kâ€‘NN, NN vb.) ile tahmin modeli kurulur; eÄŸitim verisinin kalitesi ve temsiliyeti sonuÃ§ iÃ§in kritiktir.

> \
> ![SÄ±nÄ±flandÄ±rma â€” ek Ã¶rnek](Images/8.jpg)
>
> AÃ§Ä±klama: SÄ±nÄ±flandÄ±rmada amaÃ§, Ã¶znitelik uzayÄ±nda karar sÄ±nÄ±rlarÄ± Ã¶ÄŸrenerek yeni Ã¶rnekleri doÄŸru sÄ±nÄ±fa atamaktÄ±r; veri kalitesi, sÄ±nÄ±f dengesi ve uygun metrik seÃ§imi baÅŸarÄ±yÄ± belirler.

> \
> ![SÄ±nÄ±flandÄ±rma â€” karar sÄ±nÄ±rlarÄ±/Ã¶rnekler](Images/9.jpg)
>
> AÃ§Ä±klama: Karar sÄ±nÄ±rlarÄ± ve sÄ±nÄ±f ayrÄ±labilirliÄŸi Ã¶rnekleri; model/parametre seÃ§imi ve eÅŸik ayarÄ±yla birlikte, dengesiz veri iÃ§in uygun metrikler (ROCâ€‘AUC/PRâ€‘AUC) baÅŸarÄ±yÄ± etkiler.

- Ã–rnek â€” Bozuk para tanÄ±yan makine:
    - AmaÃ§: FarklÄ± madeni paralarÄ± (Ã¶r. 1 TL, 50 kr, 25 kr) otomatik olarak doÄŸru sÄ±nÄ±fa ayÄ±rmak.
    - Aday Ã¶znitelikler: Ã‡ap (mm), aÄŸÄ±rlÄ±k (g), kalÄ±nlÄ±k, kenar tÄ±rtÄ±klÄ±lÄ±ÄŸÄ± (dairesellik), materyal/alaÅŸÄ±m (manyetik tepki), renk/yansÄ±ma, gÃ¶rsel desen/doku.
    - Hangi Ã¶znitelikler? Basit ve gÃ¼venilir Ã¶lÃ§Ã¼lebilenlerden baÅŸlayÄ±n: Ã§ap ve aÄŸÄ±rlÄ±k Ã§oÄŸu zaman gÃ¼Ã§lÃ¼ ayÄ±rÄ±cÄ±dÄ±r; kalÄ±nlÄ±k/kenar bilgisi yardÄ±mcÄ± olabilir. Kamera varsa ÅŸekil/tekstÃ¼r eklenebilir.
    - Veri hazÄ±rlama: Ã–lÃ§ekleme (standart/minâ€‘max), Ã¶lÃ§Ã¼m gÃ¼rÃ¼ltÃ¼sÃ¼ ve toleranslar, sÄ±nÄ±f dengesi; farklÄ± Ã¼retim yÄ±llarÄ±/yÄ±pranma koÅŸullarÄ± iÃ§in temsili veri toplayÄ±n.
    - Model ve eÅŸik: Lojistik regresyon veya kâ€‘NN ile baÅŸlayÄ±n; sÄ±nÄ±flar zor ayrÄ±lÄ±yorsa SVM/AÄŸaÃ§ tabanlÄ± yÃ¶ntemleri deneyin. KarÄ±ÅŸÄ±klÄ±k matrisi ve sÄ±nÄ±fâ€‘bazlÄ± F1 ile eÅŸiÄŸi ayarlayÄ±n.
    - Ã–zellik seÃ§imi: Korelasyon/mutual information, aÄŸaÃ§ Ã¶nemleri veya permÃ¼tasyon Ã¶nemiyle gereksiz/tekrarlÄ± (kollinear) Ã¶zellikleri eleyin.
    - Basit temel (baseline): Sadece Ã§ap ile ayÄ±rmayÄ± deneyin â†’ aÄŸÄ±rlÄ±ÄŸÄ± ekleyin â†’ gerekiyorsa kalÄ±nlÄ±k/ÅŸekil; her adÄ±mda Ã§apraz doÄŸrulama ile kazancÄ± Ã¶lÃ§Ã¼n.
    - Ã–nemli notlar:
        - Parametre seÃ§imi zor bir sÃ¼reÃ§tir: Ã‡ok sayÄ±da parametre kullanmak istemeyiz; ancak ayrÄ±m yapmayÄ± saÄŸlayacak kadar da parametre olmalÄ±dÄ±r.
        - Sadece renk kullansak baÅŸarÄ±sÄ±z oluruz; farklÄ± paralar aynÄ± renkte olabilir.
        - TÃ¼rk lirasÄ± Ã¶rneÄŸinde tÃ¼m paralar daire ÅŸeklinde olduÄŸundan â€œdaireâ€ bilgisinin ayÄ±rt ediciliÄŸi dÃ¼ÅŸÃ¼ktÃ¼r; tek baÅŸÄ±na bize bir ÅŸey katmaz.

> \
> ![SÄ±nÄ±flandÄ±rma â€” bozuk para Ã¶rneÄŸi](Images/10.jpg)
>
> AÃ§Ä±klama: Ã‡apâ€“aÄŸÄ±rlÄ±k (ve gerekirse kalÄ±nlÄ±k) uzayÄ±nda madeni paralarÄ±n ayrÄ±labilirliÄŸi. Basit, gÃ¼venilir sensÃ¶rlerden gelen birkaÃ§ Ã¶zellik, yÃ¼ksek doÄŸruluk iÃ§in Ã§oÄŸu zaman yeterlidir; ihtiyaÃ§ halinde ek Ã¶zelliklerle karar sÄ±nÄ±rÄ± rafine edilir.

- Karar sÄ±nÄ±rlarÄ± (decision boundaries):
    - FarklÄ± sÄ±nÄ±flandÄ±rma algoritmalarÄ±nÄ±n ortak amacÄ±, Ã¶znitelik uzayÄ±nda karar sÄ±nÄ±rlarÄ±nÄ± Ã¶ÄŸrenmektir.
    - BÃ¶ylece yeni bir giriÅŸin hangi sÄ±nÄ±fa ait olacaÄŸÄ± belirlenmiÅŸ olur.
    - Åekilde soldaki doÄŸrulardan oluÅŸan karar sÄ±nÄ±rlarÄ±, saÄŸdaki eÄŸrisel karar sÄ±nÄ±rlarÄ±na kÄ±yasla daha dÃ¼ÅŸÃ¼k performans gÃ¶sterebilir (karmaÅŸÄ±k daÄŸÄ±lÄ±mlarda doÄŸrusal modeller yetersiz kalÄ±r).

> \
> ![SÄ±nÄ±flandÄ±rma â€” karar sÄ±nÄ±rÄ± karÅŸÄ±laÅŸtÄ±rmasÄ±](Images/11.jpg)
>
> AÃ§Ä±klama: Solda doÄŸrusal (lineer) karar sÄ±nÄ±rlarÄ±, saÄŸda doÄŸrusal olmayan (eÄŸrisel) karar sÄ±nÄ±rlarÄ±. Veri daÄŸÄ±lÄ±mÄ± karmaÅŸÄ±ksa Ã§ekirdekli SVM, aÄŸaÃ§/ansamble yÃ¶ntemler veya Ã¶zellik dÃ¶nÃ¼ÅŸÃ¼mleri ile daha esnek sÄ±nÄ±rlar Ã¶ÄŸrenilebilir; ancak model karmaÅŸÄ±klÄ±ÄŸÄ±â€“genelleme dengesini Ã§apraz doÄŸrulama ve ROCâ€‘AUC/PRâ€‘AUC gibi metriklerle doÄŸrulayÄ±n.

- DanÄ±ÅŸmansÄ±z (Unsupervised) Ã–ÄŸrenme:
    - Etiket yoktur; amaÃ§ yapÄ±yÄ± keÅŸfetmek (kÃ¼meler, dÃ¼ÅŸÃ¼k boyutlu temsil, yoÄŸunluk). Ã–lÃ§Ã¼m: siluet skoru, rekonstrÃ¼ksiyon hatasÄ± vb.; Ã§oÄŸu zaman alan bilgisinin yorumu gerekir.
    - Ã–rnekler: mÃ¼ÅŸteri segmentasyonu, anomali tespiti, PCA ile boyut indirgeme.
    - EÄŸitim setinde giriÅŸlere karÅŸÄ±lÄ±k Ã§Ä±kÄ±ÅŸ verilmez; algoritma benzerlikleri kendisi keÅŸfeder ve giriÅŸleri kategorize/gruplandÄ±rÄ±r.
 
**DanÄ±ÅŸmansÄ±z Ã–ÄŸrenme**

- AmaÃ§: Etiket olmadan verinin iÃ§ yapÄ±sÄ±nÄ±/Ã¶rÃ¼ntÃ¼lerini ortaya Ã§Ä±karmak; kÃ¼meler, yoÄŸunluk bÃ¶lgeleri veya dÃ¼ÅŸÃ¼k boyutlu temsiller keÅŸfetmek.
- YaygÄ±n gÃ¶revler:
    - KÃ¼meleme: kâ€‘means, hiyerarÅŸik (agglomerative), DBSCAN/HDBSCAN, GMM (Gaussian Mixture Models).
    - Boyut indirgeme/temsil Ã¶ÄŸrenme: PCA, tâ€‘SNE, UMAP, Autoencoder.
    - Anomali/yoÄŸunluk: Isolation Forest, Local Outlier Factor, KDE.
- DeÄŸerlendirme:
    - Ä°Ã§sel metrikler (etiket yok): Silhouette, Daviesâ€“Bouldin, Calinskiâ€“Harabasz; rekonstrÃ¼ksiyon hatasÄ± (AE/PCA).
    - DÄ±ÅŸsal (varsa alan doÄŸrulamasÄ±): Etiketler/yargÄ±larla ARI/NMI; Ã§oÄŸu zaman gÃ¶rselleÅŸtirme ve uzman yorumu gerekir.
- Ã–n iÅŸleme: Ã–lÃ§ekleme (Ã¶zellikle kâ€‘means/LOF), aykÄ±rÄ± deÄŸer analizi, uygun mesafe Ã¶lÃ§Ã¼tÃ¼ (Euclidean/Cosine/Manhattan), Ã¶zellik seÃ§imi.
- Ä°puÃ§larÄ±:
    - KÃ¼me sayÄ±sÄ± k belirsizse: Dirsek yÃ¶ntemi, Silhouette; hiyerarÅŸik yÃ¶ntemlerde dendrogram incelemesi.
    - tâ€‘SNE/UMAP gÃ¶rsel amaÃ§lÄ±dÄ±r; komÅŸuluk yapÄ±sÄ±nÄ± Ã¶ne Ã§Ä±karÄ±r ama metrik karÅŸÄ±laÅŸtÄ±rma iÃ§in temkinli kullanÄ±n.
    - AmaÃ§, veriyi anlamlandÄ±rmak ve downstream denetimli gÃ¶revler iÃ§in faydalÄ± temsiller/Ã¶zellikler Ã¼retmektir.

> \
> ![DanÄ±ÅŸmansÄ±z Ã–ÄŸrenme â€” Ã¶zet](Images/12.jpg)
>
> AÃ§Ä±klama: Etiket olmadan yapÄ±/Ã¶rÃ¼ntÃ¼ keÅŸfi. Solda kÃ¼meler ve yoÄŸunluk bÃ¶lgeleri, saÄŸda boyut indirgeme ile iki boyutta gÃ¶rselleÅŸtirme; iÃ§sel metrikler ve uzman yorumu birlikte kullanÄ±lÄ±r.

**DanÄ±ÅŸmansÄ±z Ã–ÄŸrenme (KÃ¼meleme)**

- Modelin Ã¶ÄŸrenmesi sÄ±rasÄ±nda giriÅŸlere karÅŸÄ±lÄ±k beklenen Ã§Ä±kÄ±ÅŸlar verilmez; kullanÄ±lan veri etiketsizdir.
- KÃ¼meleme yÃ¶ntemleri danÄ±ÅŸmansÄ±z Ã¶ÄŸrenme yÃ¶ntemleri iÃ§inde deÄŸerlendirilir.
- Belli bir kritere gÃ¶re birbirine benzer Ã¶rnekler aynÄ± kÃ¼meye, birbirinden uzak olanlar farklÄ± kÃ¼melere yerleÅŸtirilmeye Ã§alÄ±ÅŸÄ±lÄ±r.
- KÃ¼me sayÄ±sÄ± genellikle dÄ±ÅŸarÄ±dan (k) olarak verilir ya da bir seÃ§im yÃ¶ntemiyle belirlenir (dirsek, Silhouette).
- AmaÃ§lar: Ã–zellikler arasÄ±ndaki ortak noktalarÄ± bulmak ve gerekirse boyut azaltma ile veriyi daha anlamlÄ± temsil etmek.

> \
> ![KÃ¼meleme â€” temel fikir](Images/13.jpg)
>
> AÃ§Ä±klama: Solda etiketsiz Ã¶rneklerin benzerliÄŸe gÃ¶re kÃ¼melenmesi, saÄŸda farklÄ± k deÄŸerlerinin etkisi. KÃ¼meleme, veri yapÄ±sÄ±nÄ± Ã¶zetler ve sonraki denetimli gÃ¶revler iÃ§in faydalÄ± Ã¶znitelikler saÄŸlayabilir.

- Ã–rnek (k=3):
    - (a) Etiketsiz Ã¶zellik vektÃ¶rleri (etiket yok, sadece x âˆˆ â„^d).
    - (b) KÃ¼meleme algoritmasÄ± Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda veriden 3 grup ortaya Ã§Ä±kar; benzer Ã¶rnekler aynÄ± kÃ¼mede toplanÄ±r.

> \
> ![KÃ¼meleme â€” k=3 Ã¶rneÄŸi](Images/14.jpg)
>
> AÃ§Ä±klama: Etiketsiz veri Ã¼zerinde kâ€‘means (k=3) benzeri bir sÃ¼reÃ§le Ã¼Ã§ kÃ¼me elde edilmesi. KÃ¼me merkezleri ve atamalar gÃ¶rselleÅŸtirilmiÅŸtir; k seÃ§imi performansÄ± ve yorumlanabilirliÄŸi etkiler.
- Takviyeli (Reinforcement) Ã–ÄŸrenme:
    - Ajanâ€“Ã§evre etkileÅŸimi; durum sâ€™te eylem a seÃ§er, Ã¶dÃ¼l r alÄ±r. Hedef, beklenen indirgenmiÅŸ toplam Ã¶dÃ¼lÃ¼ maksimize eden politika Ï€â€™yi Ã¶ÄŸrenmektir: \( J(\pi) = \mathbb{E}\big[\sum_{t=0}^{\infty} \gamma^{t} r_t\big] \), \(0 \leq \gamma < 1\).
    - SatranÃ§/oyunlar: Hamle/oyun sonu Ã¶dÃ¼lleriyle stratejiler geliÅŸir; Ã¶ÄŸrenilen stratejiler farklÄ± rakip ve pozisyonlara genellenir.
    - DoÄŸru Ã§Ä±kÄ±ÅŸ verilmez; bunun yerine eylemden sonra bir â€œkritik iÅŸaretâ€ (Ã¶dÃ¼l/ceza sinyali) gelir. Algoritma olumsuz iÅŸaret aldÄ±ÄŸÄ±nda doÄŸruyu bulana kadar farklÄ± olasÄ±lÄ±klarÄ± dener (keÅŸif) ve zamanla toplam Ã¶dÃ¼lÃ¼ artÄ±ran politikayÄ± Ã¶ÄŸrenir.
- Hangi tÃ¼r? Etiketli hedef varsa denetimli; yoksa ve yapÄ± aranÄ±yorsa denetimsiz; sÄ±ralÄ± karar ve Ã¶dÃ¼l varsa takviyeli Ã¶ÄŸrenme uygundur. Uygulamada hibrit yaklaÅŸÄ±mlar da yaygÄ±ndÄ±r.

> \
> ![Ã–ÄŸrenme TÃ¼rleri â€” denetimli/denetimsiz/takviyeli](Images/3.jpg)
>
> AÃ§Ä±klama: Denetimli â€” etiketli hedefle kayÄ±p minimize edilir; Denetimsiz â€” etiket olmadan yapÄ±/temsil keÅŸfi; Takviyeli â€” ajanâ€“Ã§evre etkileÅŸimiyle Ã¶dÃ¼l maksimize edilir. Model, aldÄ±ÄŸÄ± geri bildirime gÃ¶re kendini daha iyi bir konuma taÅŸÄ±r.

#### Makine Ã–ÄŸrenmesi SÃ¼reci

- Veri toplama ve algÄ±lama (Data acquisition and sensing)
    - Fiziksel deÄŸiÅŸkenlerin Ã¶lÃ§Ã¼mleri; sensÃ¶r seÃ§imi, Ã¶rnekleme hÄ±zÄ±, kalibrasyon, zaman senkronizasyonu ve veri bÃ¼tÃ¼nlÃ¼ÄŸÃ¼.
- Ã–n iÅŸleme (Preâ€‘processing)
    - GÃ¼rÃ¼ltÃ¼lÃ¼ verilerin temizlenmesi (filtreleme, aykÄ±rÄ± deÄŸer, eksik veri iÅŸlemleri), normalize/Ã¶lÃ§ekleme; ilgi gÃ¶rÃ¼ntÃ¼lerinin arka plandan ayrÄ±lmasÄ± (segmentasyon, arka plan Ã§Ä±karma).
- Ã–zellik Ã§Ä±karÄ±mÄ± (Feature extraction)
    - Ã–rÃ¼ntÃ¼yÃ¼ temsil edecek Ã¶zelliklerin bulunmasÄ±; alan bilgisi ile mÃ¼hendislik (istatistiksel/ÅŸekil/tekstÃ¼r) veya otomatik yÃ¶ntemler (PCA, Autoencoder/CNN).
- SÄ±nÄ±flandÄ±rma (Classification)
    - Yeni bir Ã¶rÃ¼ntÃ¼den elde edilen Ã¶zellikleri ve eÄŸitilmiÅŸ modeli kullanarak ilgili Ã¶rÃ¼ntÃ¼yÃ¼ bir kategoriye atamak; model seÃ§imi, eÄŸitim, doÄŸrulama ve eÅŸik ayarÄ±.
- Son iÅŸleme (Postâ€‘processing)
    - Verilen kararlarÄ±n doÄŸruluÄŸunun deÄŸerlendirilmesi; zaman iÃ§inde yumuÅŸatma/oylama, kural entegrasyonu, izleme ve geri besleme ile sÃ¼rekli iyileÅŸtirme.

#### Temel Kavramlar

- Sistem: AmaÃ§ doÄŸrultusunda Ã§Ä±kÄ±ÅŸ sinyalleri Ã¼retmek iÃ§in giriÅŸ sinyallerini iÅŸleyen; giriÅŸ sinyalini baÅŸka bir sinyale dÃ¶nÃ¼ÅŸtÃ¼ren birimlerdir.
- Zeka: Ä°nsanÄ±n dÃ¼ÅŸÃ¼nme, akÄ±l yÃ¼rÃ¼tme, algÄ±lama, kavrama, yargÄ±lama ve sonuÃ§ Ã§Ä±karma yeteneklerinin tÃ¼mÃ¼dÃ¼r.
- AkÄ±l: DÃ¼ÅŸÃ¼nme, kavrama, anlama yetisidir; doÄŸru ile yanlÄ±ÅŸÄ±, yalan ile gerÃ§eÄŸi ayÄ±rt edebilme yetisidir.

- Sinyal: Genellikle zaman iÃ§inde Ã¼retilen deÄŸerler dizisidir ve bilgi taÅŸÄ±r; matematiksel olarak deÄŸiÅŸkenlerin fonksiyonu ÅŸeklinde gÃ¶sterilir. Ä°letilecek veriler elektromanyetik veya elektriksel sinyallere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r. Sinyaller, bilgi taÅŸÄ±yan deÄŸiÅŸkenlerin fonksiyonel gÃ¶sterimleridir.
- Veri ve Bilgi / BÃ¼yÃ¼k Veri: Bilgi veriden doÄŸar; veriler, iÃ§erik iÅŸleme ve analiz ile deÄŸere dÃ¶nÃ¼ÅŸerek bilgi haline gelir. GÃ¼nÃ¼mÃ¼zde verinin hÄ±z, Ã§eÅŸitlilik ve hacim (kapasite) olarak bÃ¼yÃ¼k artÄ±ÅŸ gÃ¶stermesi ve teknolojinin bunu desteklemesiyle "BÃ¼yÃ¼k Veri" kavramÄ± ortaya Ã§Ä±kmÄ±ÅŸtÄ±r.

- Veri (Data): BilgisayarÄ±n belleÄŸine aktarÄ±lan sinyaller, resimler, gÃ¶rÃ¼ntÃ¼ler, ÅŸekiller, rakamlar, metinler ve ses gibi sembollerdir. Veri, bilgi taÅŸÄ±yan fakat henÃ¼z anlamlandÄ±rÄ±lmamÄ±ÅŸ, iliÅŸkilendirilmemiÅŸ ve iÅŸlenmemiÅŸ ham gerÃ§eklerdir; tek baÅŸÄ±na yorum iÃ§ermez ve doÄŸrudan karar vermede etkili olmayabilir.
- Bilgi (Information): Ä°ÅŸlenmiÅŸ, dÃ¼zenlenmiÅŸ ve anlamlandÄ±rÄ±lmÄ±ÅŸ verilerdir. Bilgi organize, anlamlÄ± ve kullanÄ±ÅŸlÄ± verilerdir; rapor, grafik ve gÃ¶rseller ÅŸeklinde sunulabilir ve ileride kullanÄ±lmak Ã¼zere saklanÄ±r.

- Yetenek - TecrÃ¼be (Knowledge): Karar verme, kestirim yapma ve doÄŸruyu arama sÃ¼reÃ§lerinde performansÄ± yÃ¼kseltme yetisi/deneyimidir.
- Understand (BilinÃ§): Anlayarak, kavrayarak ve hissederek anlamlandÄ±rma (bilinÃ§li kavrayÄ±ÅŸ) sÃ¼recidir.
- Wisdom (Bilgelik): DeÄŸerlendirilmiÅŸ anlayÄ±ÅŸtÄ±r; sorgulama ve kestirimle karar vermede ve yorumlamada etkin kullanÄ±lÄ±r.
- Veri gÃ¼rÃ¼ltÃ¼sÃ¼: AlgoritmanÄ±n amacÄ±yla alakalÄ± olmayan, modeli yanÄ±ltabilecek veya verimliliÄŸi dÃ¼ÅŸÃ¼rebilecek veridir; sonuÃ§larÄ±n kesinliÄŸini/kararlÄ±lÄ±ÄŸÄ±nÄ± olumsuz etkiler.

- Ã–rÃ¼ntÃ¼ (Pattern): Bir nesnenin ya da bir olayÄ±n davranÄ±ÅŸÄ±nÄ±n iki veya Ã¼Ã§ boyutlu, uzaysal ve geometrik olarak gÃ¶sterildiÄŸi desenlerdir; ilgilenilen varlÄ±ÄŸÄ±n davranÄ±ÅŸÄ±yla ilgili uzayda gÃ¶zlenebilir veya Ã¶lÃ§Ã¼lebilir geometrik bilgilerdir.
- Olgu: DoÄŸruluÄŸu ispatlanmÄ±ÅŸ Ã¶nerme veya beklenen eylemdir.
- Olay: Vaka; Ã¶rn. â€œyaÄŸmur yaÄŸacakâ€ olmasÄ± olgu, yaÄŸmurun gerÃ§ekten yaÄŸmasÄ± olaydÄ±r.
- Hipotez: Bir problemin Ã§Ã¶zÃ¼mÃ¼nÃ¼n veya doÄŸruluÄŸunun araÅŸtÄ±rÄ±lmasÄ±na yÃ¶n veren temel dÃ¼ÅŸÃ¼nceler, varsayÄ±mlar ve Ã¶nermelerdir.

##### Ã–zellikler ve Ã–zellik VektÃ¶rÃ¼

- Nesnelerin Ã¶zellikleri:
    - Belli tÃ¼rde bir nesnenin temsilcisidir (ayÄ±rt edici bilgi taÅŸÄ±r).
    - HafÄ±za tÃ¼ketimini azaltÄ±r: ham veriye kÄ±yasla daha sÄ±kÄ±/faydalÄ± temsiller saÄŸlar.
    - Hesaplama maliyetini dÃ¼ÅŸÃ¼rÃ¼r: daha az/bilinÃ§li boyut â†’ daha hÄ±zlÄ± eÄŸitim ve Ã§Ä±karÄ±m.
- Ã‡Ä±karÄ±lan/SeÃ§ilen Ã¶zellikler, farklÄ± sÄ±nÄ±flardaki nesneleri iyi ayÄ±rt edebilecek nitelikte olmalÄ±dÄ±r (discriminative power).
- Ã–zellik vektÃ¶rÃ¼, bu Ã¶zniteliklerin birlikte sayÄ±sal temsili olup x âˆˆ â„^d ile gÃ¶sterilir; d (boyut) seÃ§imi performansâ€“maliyetâ€“genelleme arasÄ±nda bir denge gerektirir.

- Ã–rnek â€” Bir grup grafiksel nesne:
    - OlasÄ± Ã¶zellikler: Åekil, renk, boyut.
    - Bu Ã¶zellikler, nesneleri farklÄ± sÄ±nÄ±flara ayÄ±rmayÄ± saÄŸlayacak gÃ¼Ã§te olmalÄ±dÄ±r; gerekirse ek Ã¶zellikler (doku, kenar sayÄ±sÄ±, oranlar) eklenir.

> \
> ![Ã–zellikler â€” grafiksel nesneler](Images/15.jpg)
>
> AÃ§Ä±klama: Grafiksel nesneler iÃ§in temel Ã¶zellikler (ÅŸekil/renk/boyut). AmaÃ§, sÄ±nÄ±flar arasÄ± ayrÄ±mÄ± artÄ±racak temsil seÃ§mek ve x âˆˆ â„^d Ã¶zellik vektÃ¶rÃ¼nÃ¼ bu doÄŸrultuda tasarlamaktÄ±r.

- Ã–zellik vektÃ¶rÃ¼ Ã¶rneklemesi:
    - Genellikle bir nesneyi temsil etmek iÃ§in birden fazla Ã¶zellik kullanÄ±lÄ±r.
    - x1 = ÅŸekil (Ã¶rn. kenar sayÄ±sÄ±)
    - x2 = boyut (Ã¶rn. sayÄ±sal bir deÄŸer)
    - x3 = renk (Ã¶rn. RGB renk uzayÄ±ndaki deÄŸerler)
    - ...
    - x_d = diÄŸer sayÄ±sal Ã¶zellikler
    - ğ’™, Ã¶zellik vektÃ¶rÃ¼dÃ¼r; d-boyutlu Ã¶zellik uzayÄ±nda bir noktayÄ± temsil eder.

> \
> ![Ã–zellik vektÃ¶rÃ¼ â€” bileÅŸenler](Images/16.jpg)
>
> AÃ§Ä±klama: Bir nesnenin x = [x1, x2, x3, ..., x_d] biÃ§iminde temsil edilmesi. DoÄŸru Ã¶zellik seÃ§imi, sÄ±nÄ±flar arasÄ± ayrÄ±mÄ± ve model performansÄ±nÄ± doÄŸrudan etkiler.

- Ã–zellik Ã§Ä±karÄ±mÄ± ve sÄ±nÄ±flandÄ±rma akÄ±ÅŸÄ±:
    - Ã–zellik Ã§Ä±karÄ±cÄ±, ham veriden (Ã¶rn. gÃ¶rÃ¼ntÃ¼, ses) Ã¶zellikleri Ã§Ä±kartÄ±r ve ğ’™ Ã¶zellik vektÃ¶rÃ¼nÃ¼ oluÅŸturur.
    - SÄ±nÄ±flandÄ±rÄ±cÄ±, ğ’™ Ã¶zellik vektÃ¶rÃ¼nÃ¼ alÄ±r ve mevcut kategorilerden birine atar.

> \
> ![Ã–zellik Ã§Ä±karÄ±mÄ± â†’ sÄ±nÄ±flandÄ±rma](Images/17.jpg)
>
> AÃ§Ä±klama: Ham veri â†’ Ã¶zellik Ã§Ä±karÄ±mÄ± â†’ x Ã¶zellik vektÃ¶rÃ¼ â†’ sÄ±nÄ±flandÄ±rÄ±cÄ± â†’ kategori. UÃ§tan uca (CNN/AE) yaklaÅŸÄ±mlar Ã¶zellik Ã§Ä±karÄ±mÄ±nÄ± otomatikleÅŸtirirken, klasik yaklaÅŸÄ±mlar el ile seÃ§ilmiÅŸ Ã¶znitelikleri kullanÄ±r.

- Grafiksel nesneleri ÅŸekillerine gÃ¶re sÄ±nÄ±flandÄ±rma:
    - Ã‡Ä±karÄ±lan Ã¶zellik: Kenar sayÄ±sÄ± (x1).
    - SÄ±nÄ±flandÄ±rÄ±cÄ± kurallarÄ± (basit kural tabanlÄ± Ã¶rnek):
        - 0 kenar â†’ Ã§ember
        - 3 kenar â†’ Ã¼Ã§gen
        - 4 kenar â†’ dikdÃ¶rtgen

> \
> ![Åekle gÃ¶re sÄ±nÄ±flandÄ±rma â€” kenar sayÄ±sÄ±](Images/18.jpg)
>
> AÃ§Ä±klama: Tek bir Ã¶zellik (kenar sayÄ±sÄ±) ile basit bir kural seti. GerÃ§ek uygulamalarda benzer ÅŸekilleri ayÄ±rmak iÃ§in oranlar (enâ€‘boy), kÃ¶ÅŸe aÃ§Ä±larÄ±, doku gibi ek Ã¶zellikler gerekebilir.

- BalÄ±klarÄ± sÄ±nÄ±flandÄ±rma (Somon vs Levrek):
    - Problem: TaÅŸÄ±yÄ±cÄ± bant Ã¼zerindeki balÄ±klarÄ± tÃ¼rlerine gÃ¶re ayÄ±rmak (ikili sÄ±nÄ±flandÄ±rma).
    - SÄ±nÄ±flar: Somon, Levrek.
    - OlasÄ± Ã¶zellikler: Uzunluk/kalÄ±nlÄ±k oranÄ±, aÄŸÄ±rlÄ±k, renk/tekstÃ¼r, yÃ¼zgeÃ§/baÅŸ oranlarÄ±, ÅŸekil tanÄ±mlayÄ±cÄ±larÄ±.
    - Not: AydÄ±nlatma ve poz referansÄ± gibi koÅŸullarÄ± sabitlemek; Ã¶lÃ§ekleme/normalizasyon ve veri artÄ±rma ile genellemeyi gÃ¼Ã§lendirmek gerekir.

> \
> ![BalÄ±k sÄ±nÄ±flandÄ±rma â€” Somon vs Levrek](Images/19.jpg)
>
> AÃ§Ä±klama: KonveyÃ¶r Ã¼zerindeki balÄ±klarÄ±n temel Ã¶lÃ§Ã¼mlerinden oluÅŸturulan x Ã¶zellik vektÃ¶rÃ¼ ile Somon/Levrek ayrÄ±mÄ±. Kurala dayalÄ± eÅŸikler basit senaryolarda yeterli olabilir; genel Ã§Ã¶zÃ¼m iÃ§in denetimli modeller (LR/SVM/AÄŸaÃ§/NN) ve uygun metriklerle doÄŸrulama Ã¶nerilir.

- Hangi bilgiler tÃ¼rleri ayÄ±rt eder?
    - Uzunluk, geniÅŸlik, aÄŸÄ±rlÄ±k, yÃ¼zgeÃ§ sayÄ±sÄ± ve ÅŸekli, kuyruk ÅŸekli vb.
- AlgÄ±lama sÄ±rasÄ±nda karÅŸÄ±laÅŸÄ±labilecek problemler:
    - IÅŸÄ±k durumu, konveyÃ¶rdeki pozisyon (dÃ¶nÃ¼klÃ¼k/Ã¶lÃ§ek), kamera gÃ¼rÃ¼ltÃ¼sÃ¼ ve bulanÄ±klÄ±k, kÄ±smi Ã¶rtÃ¼ÅŸme.
- SÄ±nÄ±flandÄ±rma sÃ¼reci (Ã¶rnek iÅŸ akÄ±ÅŸÄ±):
    - Resmi Ã§ek â†’ balÄ±ÄŸÄ± resimden Ã§Ä±kar (segmentasyon) â†’ Ã¶lÃ§Ã¼mleri yap (Ã¶zellik Ã§Ä±karÄ±mÄ±) â†’ karar ver (sÄ±nÄ±flandÄ±rÄ±cÄ±).

- Notlar:
    - Levrek ve somon tÃ¼rleri (doÄŸru Ã¶zellikler seÃ§ildiÄŸinde) Ã¶znitelik uzayÄ±nda genellikle aÃ§Ä±kÃ§a ayrÄ±labilir.
    - TÃ¼m balÄ±klar uzayÄ± (mÃ¼mkÃ¼n Ã¶zelliklerin evreni) Ã§ok geniÅŸtir; her boyut bir Ã¶zelliÄŸi temsil eder. Ancak bu Ã¶zelliklerin Ã§oÄŸu tek bir kameradan gÃ¼venilir ÅŸekilde Ã¶lÃ§Ã¼lemez; sensÃ¶r ve kurulum kÄ±sÄ±tlarÄ± seÃ§ilen Ã¶zellik kÃ¼mesini belirler.

> \
> ![BalÄ±klar â€” Ã¶znitelik uzayÄ± ve ayrÄ±m](Images/20.jpg)
>
> AÃ§Ä±klama: YÃ¼ksek boyutlu Ã¶znitelik uzayÄ±nda sÄ±nÄ±flarÄ±n ayrÄ±labilirliÄŸi; pratikte gÃ¶zlemlenebilir/Ã¶lÃ§Ã¼lebilir alt uzayda Ã§alÄ±ÅŸÄ±lÄ±r. DoÄŸru sensÃ¶rler ve saÄŸlam Ã¶n iÅŸleme, Ã¶lÃ§Ã¼m hatalarÄ±na raÄŸmen ayrÄ±mÄ± korur.

- Ã–zellik seÃ§imi = Uzay projeksiyonu:
    - MÃ¼mkÃ¼n (yÃ¼ksek boyutlu) Ã¶znitelik uzayÄ±ndan anlamlÄ± bir alt kÃ¼me seÃ§mek, veriyi daha dÃ¼ÅŸÃ¼k boyutlu bir alt uzaya yansÄ±tmak (projeksiyon) demektir.
    - AmaÃ§: Gereksiz/gÃ¼rÃ¼ltÃ¼lÃ¼ boyutlarÄ± atarak ayrÄ±mÄ± korumak ya da artÄ±rmak; bellek ve hesaplama maliyetini dÃ¼ÅŸÃ¼rmek; aÅŸÄ±rÄ± uyumu azaltmak.
    - YÃ¶ntemler: El ile seÃ§im (alan bilgisi), filtre/sarmalayÄ±cÄ±/gÃ¶mÃ¼lÃ¼ yÃ¶ntemler; PCA/LDA gibi dÃ¶nÃ¼ÅŸÃ¼mlerle boyut azaltma.

> \
> ![Ã–zellik seÃ§imi â€” projeksiyon](Images/21.jpg)
>
> AÃ§Ä±klama: YÃ¼ksek boyutlu Ã¶znitelik uzayÄ±ndan seÃ§ilen Ã¶zelliklerle daha dÃ¼ÅŸÃ¼k boyutlu bir alt uzaya yansÄ±tma. DoÄŸru projeksiyon, sÄ±nÄ±flarÄ±n ayrÄ±mÄ±nÄ± korur ve Ã¶ÄŸrenmeyi kolaylaÅŸtÄ±rÄ±r.

- Ã–zelliklerin marjinal daÄŸÄ±lÄ±mlarÄ± (1B projeksiyon):
    - Her bir Ã¶znitelik x_i, Ã§ok boyutlu daÄŸÄ±lÄ±mÄ±n tekil marjinali p(x_i) olarak dÃ¼ÅŸÃ¼nÃ¼lebilir.
    - BaÅŸka bir deyiÅŸle: x_i eksenine tek boyutlu yansÄ±tma (projeksiyon) alÄ±yoruz; histogram ya da yoÄŸunluk kestirimiyle incelenir.
    - SÄ±nÄ±flara gÃ¶re marjinal daÄŸÄ±lÄ±mlar, ayrÄ±m gÃ¼cÃ¼ ve olasÄ± eÅŸikler hakkÄ±nda doÄŸrudan sezgi verir; Ã¶rtÃ¼ÅŸme varsa yeni Ã¶zellik(ler) veya dÃ¶nÃ¼ÅŸÃ¼mler gerekir.

> \
> ![Marjinal daÄŸÄ±lÄ±m â€” tek boyuta yansÄ±tma](Images/22.jpg)
>
> AÃ§Ä±klama: Ã‡ok boyutlu Ã¶znitelik uzayÄ±ndan tek bir Ã¶zelliÄŸin marjinal daÄŸÄ±lÄ±mÄ± p(x_i). Tek boyutlu projeksiyonlar, basit karar kurallarÄ± ve eÅŸik seÃ§imleri iÃ§in gÃ¼Ã§lÃ¼ bir araÃ§tÄ±r.

- Model = SeÃ§ilmiÅŸ Ã¶zelliklerle yaklaÅŸÄ±k temsil:
    - SÄ±nÄ±flandÄ±rmak istediÄŸimiz her olgu/nesne iÃ§in bir model kurarÄ±z; bu model, seÃ§tiÄŸimiz Ã¶znitelikler baÄŸlamÄ±nda yaklaÅŸÄ±k bir temsildir.
    - AmaÃ§: GerÃ§ek dÃ¼nyanÄ±n sonsuz detayÄ±nÄ±, gÃ¶rev iÃ§in yeterli ayÄ±rt ediciliÄŸi koruyacak sÄ±nÄ±rlÄ± bir Ã¶zellik kÃ¼mesiyle yakalamak.
    - SonuÃ§: Ä°yi seÃ§ilmiÅŸ Ã¶znitelikler, basit modellerin dahi yÃ¼ksek doÄŸrulukla Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlayabilir; zayÄ±f Ã¶znitelikler ise karmaÅŸÄ±k modellerle bile zayÄ±f performans verir.

> \
> ![Ã–zellik tabanlÄ± model â€” yaklaÅŸÄ±k temsil](Images/23.jpg)
>
> AÃ§Ä±klama: SeÃ§ilen Ã¶zniteliklere gÃ¶re olgunun kurulan modeli sadece bir yaklaÅŸÄ±mdÄ±r; temsil gÃ¼cÃ¼ seÃ§ime baÄŸlÄ±dÄ±r ve gÃ¶revle uyumlu olmalÄ±dÄ±r.

- Ã–zellik seÃ§imi â€” Uzunluk ile eÅŸikleme (Levrek vs Somon):
    - Alan bilgisi: â€œLevrek, somondan genellikle daha uzundur.â€ O halde tek bir Ã¶zellik (uzunluk, l) ile basit bir karar kuralÄ± kurulabilir.
    - Karar kuralÄ± (Ã¶rnek): l â‰¥ Ï„ ise Levrek, l < Ï„ ise Somon.
    - EÅŸik Ï„ nasÄ±l seÃ§ilir?
        - Veri temelli seÃ§im: EÄŸitim/validasyon verisindeki uzunluk daÄŸÄ±lÄ±mlarÄ±nÄ± sÄ±nÄ±fa gÃ¶re ayÄ±rÄ±n; hatayÄ± en aza indiren Ï„ deÄŸerini seÃ§in (grid arama ya da sÄ±ralÄ± tarama ile).
        - Gaussian varsayÄ±mÄ± ve eÅŸit varyans: sÄ±nÄ±f ortalamalarÄ±nÄ±n ortasÄ± Ã¶nerilir: $\tau = (\mu_{Levrek}+\mu_{Somon})/2$.
        - Genel kural: sÄ±nÄ±f-ÅŸartlÄ± yoÄŸunluklarÄ±n kesiÅŸtiÄŸi noktada $p(l\mid Levrek)\,P(Levrek) = p(l\mid Somon)\,P(Somon)$ eÅŸitliÄŸini saÄŸlayan Ï„ (eÅŸit Ã¶ncÃ¼l ve maliyetlerde Bayes optimal eÅŸik).
        - Maliyet/Ã¶ncÃ¼l farklÄ±ysa: karar eÅŸiÄŸini sÄ±nÄ±f Ã¶ncÃ¼lleri ve hata maliyetlerine gÃ¶re kaydÄ±rÄ±n (ROC/Youden J ile nokta seÃ§imi, FPR/TPR dengesi).
    - Pratik notlar: Ã–lÃ§ekleme (zâ€‘score), uÃ§ deÄŸerlerin etkisi, Ã¶lÃ§Ã¼m hatasÄ±; gerekiyorsa uzunlukla birlikte oran tabanlÄ± baÅŸka bir Ã¶zellik ekleyin (Ã¶rn. uzunluk/geniÅŸlik) ve 2B karar sÄ±nÄ±rÄ± kurun.

- Uzunluk histogramlarÄ± ve gÃ¼venilir eÅŸik l* seÃ§imi:
    - Ä°ki sÄ±nÄ±f iÃ§in uzunluk histogramlarÄ±nÄ± Ã¼st Ã¼ste Ã§izin; daÄŸÄ±lÄ±mlar genelde Ã¶rtÃ¼ÅŸÃ¼r (bazÄ± somonlar bazÄ± levreklerden daha uzun olabilir ve tersi).
    - EÅŸik l* nasÄ±l seÃ§ilir?
        - Hata enazlama: Toplam yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmayÄ± en aza indiren l* (validasyonla tarama veya ROC Ã¼zerinden Youden J maksimumu).
        - SÄ±nÄ±f-ÅŸartlÄ± kesiÅŸim: p(l|Levrek)P(Levrek) = p(l|Somon)P(Somon) noktasÄ±na yakÄ±n l* (eÅŸit maliyet/Ã¶ncÃ¼l varsayÄ±mÄ±).
        - Maliyet/Ã¶ncÃ¼l ayarÄ±: YanlÄ±ÅŸ pozitif/negatif maliyetlerine ve P(sÄ±nÄ±f) dengesine gÃ¶re l*â€™yi kaydÄ±rÄ±n.
    - Ã–rtÃ¼ÅŸme bÃ¼yÃ¼kse tek Ã¶zellik yetmez: Ä°kinci bir Ã¶zelliÄŸe bakÄ±n (Ã¶rn. balÄ±k pullarÄ±nÄ±n parlaklÄ±ÄŸÄ±) ve 2B uzayda karar sÄ±nÄ±rÄ± Ã¶ÄŸrenin; basit modeller bile ayrÄ±mÄ± iyileÅŸtirir.

> \
> ![Uzunluk histogramlarÄ± â€” eÅŸik seÃ§imi](Images/24.jpg)
>
> AÃ§Ä±klama: Levrek ve Somon iÃ§in uzunluk histogramlarÄ±; gÃ¼venilir karar iÃ§in l*â€™nin daÄŸÄ±mlarÄ±n Ã¶rtÃ¼ÅŸmesini dikkate alacak ÅŸekilde seÃ§ilmesi ve gerekirse ek bir Ã¶zellik (parlaklÄ±k) eklenmesi gerekir.

- ParlaklÄ±k histogramlarÄ± â€” daha kolay eÅŸik, yine de sÄ±nÄ±rlÄ± ayÄ±rÄ±m:
    - Ä°ki sÄ±nÄ±f iÃ§in parlaklÄ±k (Ã¶r. yansÄ±ma/Ã¶lÃ§eklendirilmiÅŸ gri seviye) histogramlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±n; Ã§oÄŸu durumda uzunluÄŸa gÃ¶re eÅŸik seÃ§imi daha kolay hale gelir.
    - Neden yine de yetersiz olabilir?
        - AydÄ±nlatma deÄŸiÅŸimi, Ä±slaklÄ±k/specular parlamalar, sensÃ¶r gÃ¼rÃ¼ltÃ¼sÃ¼ â†’ daÄŸÄ±lÄ±mlarda Ã¶rtÃ¼ÅŸme sÃ¼rer.
        - FarklÄ± Ã§ekim aÃ§Ä±larÄ± ve segmentasyon hatalarÄ± parlaklÄ±k Ã¶lÃ§Ã¼mÃ¼nÃ¼ saptÄ±rÄ±r.
    - Ne yapÄ±labilir?
        - AydÄ±nlatma/poz kontrolÃ¼, beyaz dengeleme, Ä±ÅŸÄ±klandÄ±rma normalizasyonu (Ã¶rn. histogram eÅŸitleme) uygulayÄ±n.
        - ParlaklÄ±ÄŸÄ± uzunluk ile birleÅŸtirerek 2B Ã¶zellik uzayÄ±nda karar sÄ±nÄ±rÄ± Ã¶ÄŸrenin; doÄŸrusal olmayan ayÄ±rÄ±cÄ±lar (SVM/RBF, aÄŸaÃ§lar) Ã¶rtÃ¼ÅŸmeyi daha iyi yÃ¶netebilir.

> \
> ![ParlaklÄ±k histogramlarÄ± â€” eÅŸik seÃ§imi](Images/25.jpg)
>
> AÃ§Ä±klama: Levrek ve Somon iÃ§in parlaklÄ±k histogramlarÄ±. EÅŸik seÃ§imi uzunluÄŸa gÃ¶re daha kolay gÃ¶rÃ¼nse de kusursuz ayrÄ±m saÄŸlamaz; aydÄ±nlatma normalizasyonu ve Ã§oklu Ã¶zellikler ile ayÄ±rÄ±m gÃ¼Ã§lendirilir.

- Ä°ki Ã¶zellikli karar: ParlaklÄ±k + GeniÅŸlik (2B Ã¶zellik uzayÄ±)
    - GÃ¶zlem: Levrek, somona gÃ¶re tipik olarak daha geniÅŸtir. ParlaklÄ±k (x1) ile birlikte geniÅŸlik (x2) Ã¶zelliÄŸini de ekleyerek karar verebiliriz.
    - Temsil: Her balÄ±k gÃ¶rÃ¼ntÃ¼sÃ¼ iki boyutlu Ã¶zellik uzayÄ±nda bir nokta ile temsil edilir; Ã¶zellik vektÃ¶rÃ¼ $\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \in \mathbb{R}^2$.
    - Yarar: 2B uzayda sÄ±nÄ±flar Ã§oÄŸu zaman daha iyi ayrÄ±lÄ±r; uygun bir karar sÄ±nÄ±rÄ± (doÄŸrusal ya da doÄŸrusal olmayan) ile hata azalÄ±r.
    - Pratik: Her ekseni Ã¶lÃ§ekleyin/standartlaÅŸtÄ±rÄ±n (Ã¶rn. zâ€‘skor); gerekiyorsa veri arttÄ±rma ve gÃ¼rÃ¼ltÃ¼ye dayanÄ±klÄ± istatistikler kullanÄ±n.

## Model DeÄŸerlendirme

- Neden? EÄŸitilmiÅŸ modelin gerÃ§ek dÃ¼nyadaki (gÃ¶rÃ¼lmemiÅŸ) Ã¶rneklerdeki baÅŸarÄ±sÄ±nÄ± Ã¶ngÃ¶rmek iÃ§in.
- Test veri seti:
    - Ã–ÄŸrenmenin performansÄ±nÄ± tahmin etmek iÃ§in ayrÄ± bir Test seti kullanÄ±lÄ±r.
    - EÄŸitim setiyle aynÄ± (veya Ã§ok benzer) daÄŸÄ±lÄ±mdan gelmelidir; aksi halde daÄŸÄ±lÄ±m kaymasÄ± tahmini yanÄ±ltÄ±r.
- Veri bÃ¶lme ve dÃ¼rÃ¼st deÄŸerlendirme:
    - Train / Validation / Test ayrÄ±mÄ±; model ve hiperparametre seÃ§imi Validation Ã¼zerinde yapÄ±lÄ±r, Test yalnÄ±zca en sonda kullanÄ±lÄ±r.
    - Veri sÄ±zÄ±ntÄ±sÄ±ndan kaÃ§Ä±nÄ±n: Ã–lÃ§ekleme/normalizasyon vb. yalnÄ±zca Train Ã¼zerinde fit edilir ve Val/Testâ€™e uygulanÄ±r.
- KÄ±sa not: KÃ¼Ã§Ã¼k veri kÃ¼melerinde K-katlÄ± Ã§apraz doÄŸrulama (CV) daha gÃ¼venilir bir genelleme tahmini saÄŸlayabilir.
    
### Model BaÅŸarÄ±larÄ±nÄ± Ã–lÃ§mek iÃ§in KullanÄ±lan Teknikler

- EÄŸitimâ€“Test olarak bÃ¶lme (holdâ€‘out): Veriyi iki (veya Ã¼Ã§) parÃ§aya ayÄ±rÄ±p, modeli yalnÄ±zca eÄŸitim kÄ±smÄ±nda fit edip test kÄ±smÄ±nda deÄŸerlendiririz. Basit ve hÄ±zlÄ± bir ilk yaklaÅŸÄ±m saÄŸlar.
- Ã‡apraz DoÄŸrulama (Crossâ€‘Validation): Veriyi K katmana ayÄ±rÄ±p her katmanÄ± sÄ±rayla test seti yaparak K kez eÄŸitip test ederiz; skorlarÄ±n ortalamasÄ± daha gÃ¼venilir bir genelleme tahmini verir.

> ![EÄŸitimâ€“Test bÃ¶lmesi â€” gÃ¶rsel 131](Images/131.jpg)
>
> ![Ã‡apraz doÄŸrulama â€” kavramsal 132](Images/132.jpg)

Neden Ã§apraz doÄŸrulama?
- AÅŸÄ±rÄ± uyumdan kaÃ§Ä±nmak: YalnÄ±zca eÄŸitime Ã§ok iyi uyan, fakat yeni veride zayÄ±f kalan modelleri tespit etmeye yardÄ±mcÄ± olur.
- Daha gÃ¼venilir performans tahmini: Tek bir rastgele bÃ¶lmeye kÄ±yasla yanlÄ±lÄ±ÄŸÄ± azaltÄ±r ve varyansÄ± dÃ¼ÅŸÃ¼rÃ¼r.
- Veriyi daha etkin kullanma: KÃ¼Ã§Ã¼k veri setlerinde her gÃ¶zlem hem eÄŸitimde hem testte (farklÄ± iterasyonlarda) kullanÄ±lÄ±r.

Kâ€‘KatlÄ± Ã‡apraz DoÄŸrulama (Kâ€‘Fold)
1) Veri K eÅŸit (veya yaklaÅŸÄ±k eÅŸit) katmana bÃ¶lÃ¼nÃ¼r (genelde K=5 ya da 10).
2) 1. iterasyonda katmanâ€‘1 test, kalan Kâˆ’1 katman eÄŸitim olarak kullanÄ±lÄ±r; skor kaydedilir.
3) Katmanlar sÄ±rayla test katmanÄ± olacak ÅŸekilde K iterasyon tamamlanÄ±r.
4) Nihai skor, K skorun ortalamasÄ± (ve genellikle standart sapmasÄ±) olarak raporlanÄ±r.

> ![Kâ€‘fold â€” ÅŸema 133](Images/133.jpg)
>
> ![Kâ€‘fold â€” adÄ±mlar 134](Images/134.jpg)

KÄ±sa Python Ã¶rnekleri (scikitâ€‘learn):

 - EÄŸitimâ€“Test bÃ¶lmesi (classification):

        ```python
        from sklearn.datasets import make_classification
        from sklearn.model_selection import train_test_split
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import accuracy_score

        X, y = make_classification(n_samples=500, n_features=10, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, stratify=y, random_state=42
        )

        clf = LogisticRegression(max_iter=1000).fit(X_train, y_train)
        acc = accuracy_score(y_test, clf.predict(X_test))
        print({"accuracy": acc})
        ```

 - Kâ€‘katlÄ± Ã‡apraz DoÄŸrulama (stratified, classification):

        ```python
        from sklearn.model_selection import cross_val_score, StratifiedKFold
        from sklearn.linear_model import LogisticRegression

        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        scores = cross_val_score(LogisticRegression(max_iter=1000), X, y, cv=cv, scoring="accuracy")
        print({"cv_mean": scores.mean(), "cv_std": scores.std()})
        ```

Notlar:
- SÄ±nÄ±flandÄ±rmada katmanlÄ± bÃ¶lme (StratifiedKFold) sÄ±nÄ±f oranlarÄ±nÄ±n katmanlar arasÄ±nda korunmasÄ±nÄ± saÄŸlar.
- Hiperparametre seÃ§imi iÃ§in Ã§apraz doÄŸrulama ile GridSearchCV/RandomizedSearchCV kullanÄ±n; aynÄ± anda model seÃ§imi ve gÃ¼venilir deÄŸerlendirme saÄŸlar.

### Ä°Ã§ Ä°Ã§e (Nested) Ã‡apraz DoÄŸrulama

Neden? Hiperparametre aramasÄ±nÄ± ve genelleme deÄŸerlendirmesini ayÄ±rarak iyimserlik yanlÄ±lÄ±ÄŸÄ±nÄ± Ã¶nler. DÄ±ÅŸ dÃ¶ngÃ¼deki katmanlar gerÃ§ek â€œgÃ¶rÃ¼lmemiÅŸ veriâ€ rolÃ¼ndedir; iÃ§ dÃ¶ngÃ¼de en iyi hiperparametre bulunur.

- DÄ±ÅŸ CV (Ã¶r. StratifiedKFold, K=5): DeÄŸerlendirme katmanlarÄ±.
- Ä°Ã§ CV (Ã¶r. StratifiedKFold, K=3): Hiperparametre aramasÄ± (GridSearchCV/RandomizedSearchCV).

KÄ±sa Python Ã¶rneÄŸi (SVC iÃ§in nested CV, F1 skoru):

```python
    import numpy as np
    from sklearn.datasets import make_classification
    from sklearn.model_selection import StratifiedKFold, GridSearchCV
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import make_pipeline
    from sklearn.svm import SVC
    from sklearn.metrics import f1_score

    X, y = make_classification(n_samples=500, n_features=20, weights=[0.8, 0.2], random_state=42)

    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

    pipe = make_pipeline(StandardScaler(), SVC(probability=False))
    param_grid = {
        'svc__C': [0.1, 1, 10],
        'svc__kernel': ['linear', 'rbf'],
        'svc__gamma': ['scale', 'auto']
    }

    outer_scores = []
    for tr_idx, te_idx in outer_cv.split(X, y):
        X_tr, X_te = X[tr_idx], X[te_idx]
        y_tr, y_te = y[tr_idx], y[te_idx]

        gs = GridSearchCV(pipe, param_grid, cv=inner_cv, scoring='f1', n_jobs=-1)
        gs.fit(X_tr, y_tr)
        y_pred = gs.best_estimator_.predict(X_te)
        outer_scores.append(f1_score(y_te, y_pred))

    print({"nested_f1_mean": float(np.mean(outer_scores)), "nested_f1_std": float(np.std(outer_scores))})
```

Ä°puÃ§larÄ±:
- DÄ±ÅŸ katman skorlarÄ±nÄ±n ortalamasÄ±nÄ± ve standart sapmasÄ±nÄ± raporlayÄ±n.
- Model/hyperparametre seÃ§imi YALNIZCA iÃ§ dÃ¶ngÃ¼de yapÄ±lmalÄ±; dÄ±ÅŸ test verisine asla bakÄ±lmamalÄ±dÄ±r.

### Hiperparametre Optimizasyonu

Bir modeli yalnÄ±zca `fit()` etmek yetmez; pek Ã§ok modelin baÅŸarÄ±mÄ±, eÄŸitimden Ã–NCE ayarlanan hiperparametrelere baÄŸlÄ±dÄ±r.

- Parametre (Parameter): Modelin VERÄ°DEN Ã¶ÄŸrendiÄŸi deÄŸerlerdir. Ã–rn. Lojistik Regresyon katsayÄ±larÄ± w, bir YSAâ€™daki aÄŸÄ±rlÄ±klar.
- Hiperparametre (Hyperparameter): Ã–ÄŸrenme sÃ¼recini kontrol eden DIÅ ayarlardÄ±r; veriden Ã¶ÄŸrenilmez, biz belirleriz. Ã–rn. kâ€‘NNâ€™de komÅŸu sayÄ±sÄ± k, Decision Treeâ€™de `max_depth`, SVMâ€™de `C` ve `kernel`.

Ã–rnek hiperparametreler:
- kâ€‘NN: `n_neighbors` (k), `metric` (euclidean/manhattan...)
- Karar AÄŸacÄ±: `max_depth` (ezberi kÄ±sÄ±tlar), `criterion` (gini/entropy)
- SVM: `C` (hata cezasÄ±; marjin geniÅŸliÄŸi â†” hata toleransÄ±), `kernel` (linear/rbf), `gamma` (RBF geniÅŸliÄŸi)

Train / Validation / Test AyrÄ±mÄ±
- Training: Modeli eÄŸitiriz (~%60â€“70).
- Validation: Hiperparametre aramasÄ± iÃ§in ayrÄ±lÄ±r (~%15â€“20). En iyi kombinasyonu burada seÃ§eriz.
- Test: KÄ°LÄ°TLÄ° kutudur (~%15â€“20). En sonda sadece final raporu iÃ§in bir kez kullanÄ±lÄ±r (sÄ±zÄ±ntÄ± yapmayÄ±n).

Arama YÃ¶ntemleri
- Grid Search (Ä±zgara): BelirlediÄŸimiz tÃ¼m kombinasyonlarÄ± dener; basit ve kapsamlÄ±dÄ±r fakat kombinasyon sayÄ±sÄ± hÄ±zla patlar (boyutlanma laneti).

> ![Grid search â€” ÅŸema 141](Images/141.jpg)

 - Random Search (rastgele): Ã–n tanÄ±mlÄ± daÄŸÄ±lÄ±mlardan rastgele Ã¶rnekler dener; Ã§oÄŸu pratikte Ã§ok daha hÄ±zlÄ±dÄ±r ve kÄ±sa sÃ¼rede iyi sonuÃ§lar verir.

> ![Random search â€” sezgi 142](Images/142.jpg)
> 
> ![Random search â€” Ã¶rnek 143](Images/143.jpg)

Parametre Arama AkÄ±ÅŸÄ± (Ã¶zet)

> ![Parametre arama â€” akÄ±ÅŸ 144](Images/144.jpg)

KÄ±sa scikitâ€‘learn Ã¶rnekleri

 - SVC iÃ§in Grid Search (Stratified 5â€‘fold):

```python
    from sklearn.model_selection import GridSearchCV, StratifiedKFold
    from sklearn.pipeline import make_pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.svm import SVC

    pipe = make_pipeline(StandardScaler(), SVC())
    param_grid = {
        "svc__kernel": ["linear", "rbf"],
        "svc__C": [0.1, 1, 10, 100],
        "svc__gamma": ["scale", 0.01, 0.1, 1]
    }
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    gs = GridSearchCV(pipe, param_grid=param_grid, cv=cv, scoring="accuracy", n_jobs=-1)
    gs.fit(X, y)
    print({"best_params": gs.best_params_, "cv_best_acc": gs.best_score_})
```

 - SVC iÃ§in Randomized Search (Ã¶ncelikle geniÅŸ aralÄ±klarÄ± hÄ±zlÄ± tarama):

```python
    from sklearn.model_selection import RandomizedSearchCV
    from scipy.stats import loguniform

    pipe = make_pipeline(StandardScaler(), SVC(kernel="rbf"))
    param_dist = {
        "svc__C": loguniform(1e-3, 1e3),
        "svc__gamma": loguniform(1e-4, 1e1)
    }
    rs = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=50,
                            cv=cv, scoring="accuracy", random_state=42, n_jobs=-1)
    rs.fit(X, y)
    print({"best_params": rs.best_params_, "cv_best_acc": rs.best_score_})
```

Ä°puÃ§larÄ±:
- Skor anahtarlarÄ±nÄ± (scoring) probleme gÃ¶re seÃ§in: imbalanced veri iÃ§in `roc_auc`, `average_precision`, sÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ± iÃ§in `class_weight`.
- Sonucu raporlarken CV ortalamasÄ± ve std verin; seÃ§ilen modeli ayrÄ±ca ayrÄ± Test seti Ã¼zerinde ONAYLAYIN.

- SÄ±nÄ±flandÄ±rma ve regresyon baÄŸlamÄ±:
    - Ä°ster sÄ±nÄ±flandÄ±rma ister regresyon olsun, amaÃ§ sadece eÄŸitimi aÃ§Ä±klamak deÄŸil; gÃ¶rÃ¼lmemiÅŸ Ã¶rnekleri doÄŸru tahmin edebilen bir model (genelleme) elde etmektir.
    - Bu yÃ¼zden Test seti, eÄŸitimde hiÃ§ kullanÄ±lmayan ve eÄŸitim setiyle aynÄ±/benzer daÄŸÄ±lÄ±mdan gelen Ã¶rneklerden oluÅŸmalÄ±dÄ±r.
- Veri ayrÄ±mÄ± pratikleri:
    - Basit ayrÄ±m: EÄŸitim ve Test olarak ikiye bÃ¶lme (Ã¶zellikle temel projeler ve ilk deÄŸerlendirme iÃ§in).
    - Daha saÄŸlam yaklaÅŸÄ±m: Train/Validation/Test Ã¼Ã§lemesi; hiperparametre ayarÄ± Validation ile yapÄ±lÄ±r, Test sadece final raporlama iÃ§indir.
    - KÃ¼Ã§Ã¼k veri: Kâ€‘katlÄ± CV ile eÄŸitim/doÄŸrulama dÃ¶ngÃ¼lerini tekrar ederek daha sabit bir performans tahmini elde edilir.

- DaÄŸÄ±lÄ±m kaymasÄ± (distribution shift) ve OOD:
    - Ã–rnek senaryo: a â†’ eÄŸitim kÃ¼mesi; b ve c â†’ test kÃ¼meleri. c, a ve b'den farklÄ± daÄŸÄ±lÄ±ma sahipse, a'dan Ã¶ÄŸrenilen Ã¶zelliklerin c'de aynÄ± baÅŸarÄ±yÄ± vermesi beklenmez.
    - TÃ¼rler: Kovaryat kaymasÄ± (p(x) deÄŸiÅŸir), Ã¶ncÃ¼l kaymasÄ± (p(y) deÄŸiÅŸir), kavram kaymasÄ± (p(y|x) deÄŸiÅŸir), alan/cihaz/ortam deÄŸiÅŸimi (domain shift).
    - SonuÃ§: Test daÄŸÄ±lÄ±mÄ± eÄŸitime benzemiyorsa metrikler anlamÄ±nÄ± yitirir; genelleme bozulur.
    - Azaltma Ã¶nerileri: Veri artÄ±rma ve normalizasyon, alan uyarlama (domain adaptation), Ã¶nem aÄŸÄ±rlÄ±ÄŸÄ± (importance weighting), saÄŸlam Ã¶zellikler/ayrÄ±klaÅŸtÄ±rma, kalibrasyon ve OOD tespiti.

> \
> ![DaÄŸÄ±lÄ±m kaymasÄ± â€” a (train), b & c (test)](Images/26.jpg)
>
> AÃ§Ä±klama: a eÄŸitim; b aynÄ± daÄŸÄ±lÄ±mdan test; c farklÄ± daÄŸÄ±lÄ±mdan test. aâ†’c geÃ§iÅŸinde daÄŸÄ±lÄ±m kaymasÄ± varsa, eÄŸitilen Ã¶zelliklerin c'de iÅŸe yaramamasÄ± olaÄŸandÄ±r.

- EÄŸitime %100 uyan sÄ±nÄ±r, testte farklÄ± davranÄ±ÅŸ (overfitting):
    - Ä°ki farklÄ± karar sÄ±nÄ±rÄ± eÄŸitim kÃ¼mesinde %100 baÅŸarÄ± gÃ¶sterebilir; ancak testte genelleme farklÄ± olur.
    - AÅŸÄ±rÄ± karmaÅŸÄ±k/sallantÄ±lÄ± sÄ±nÄ±r, eÄŸitim gÃ¼rÃ¼ltÃ¼sÃ¼nÃ¼ ezberler (yÃ¼ksek varyans) â†’ testte hata artar.
    - Daha basit/ dÃ¼zgÃ¼n sÄ±nÄ±r (yÃ¼ksek bias, dÃ¼ÅŸÃ¼k varyans) genelde daha iyi geneller.
    - Ne yapmalÄ±? AyrÄ±mÄ± Validation setinde Ã¶lÃ§erek model/hiperparametre seÃ§imi, dÃ¼zenlileÅŸtirme (L2/L1, erken durdurma), veri artÄ±rma, marjini geniÅŸletme (SVM) ve Ã§apraz doÄŸrulama.

> \
> ![EÄŸitimde %100 â€” testte farklÄ± genel davranÄ±ÅŸ](Images/27.jpg)
>
> AÃ§Ä±klama: Ä°ki karar sÄ±nÄ±rÄ± eÄŸitimde kusursuz gÃ¶rÃ¼nse de, test daÄŸÄ±lÄ±mÄ±nda daha dÃ¼zgÃ¼n/az esnek sÄ±nÄ±r genellikle daha iyi performans gÃ¶sterir; bu, biasâ€‘varyans dengesinin bir yansÄ±masÄ±dÄ±r.

##### Karar SÄ±nÄ±rÄ± ve Model KarmaÅŸÄ±klÄ±ÄŸÄ±

- ParlaklÄ±k (x1) ve GeniÅŸlik (x2) daÄŸÄ±lÄ±mÄ± bir saÃ§Ä±lÄ±m grafiÄŸi (scatter) ile gÃ¶rselleÅŸtirilebilir.
- Ã–zellik uzayÄ± iki bÃ¶lgeye ayrÄ±larak bir karar sÄ±nÄ±rÄ± Ã§izilebilir; bu sÄ±nÄ±rÄ±n biÃ§imi model sÄ±nÄ±fÄ±na gÃ¶re deÄŸiÅŸir.
    - Basit: DoÄŸrusal sÄ±nÄ±r (LR, LDA, lineer SVM) â†’ yÃ¼ksek bias, dÃ¼ÅŸÃ¼k varyans.
    - Esnek: DoÄŸrusal olmayan sÄ±nÄ±r (polinom, RBF SVM, aÄŸaÃ§/orman, NN) â†’ dÃ¼ÅŸÃ¼k bias, yÃ¼ksek varyans riski.
- Uygun karmaÅŸÄ±klÄ±k seÃ§imi:
    - Validation ve/veya CV ile sÄ±nÄ±rÄ±n genellemesini Ã¶lÃ§Ã¼n; gereÄŸinde dÃ¼zenlileÅŸtirme (Î»), erken durdurma, Ã¶zellik mÃ¼hendisliÄŸi uygulayÄ±n.
    - Ã–lÃ§ekleme (x1,x2) ve sÄ±nÄ±f dengesizliÄŸi gibi veri Ã¶zellikleri sÄ±nÄ±rÄ±n ÅŸeklini ve yeri Ã¼zerinde etkilidir.

> \
> ![Karar sÄ±nÄ±rÄ± â€” x1 (parlaklÄ±k) vs x2 (geniÅŸlik)](Images/28.jpg)
>
> AÃ§Ä±klama: ParlaklÄ±kâ€‘geniÅŸlik uzayÄ±nda iki sÄ±nÄ±f ve muhtemel bir karar sÄ±nÄ±rÄ±. KarmaÅŸÄ±klÄ±k arttÄ±kÃ§a eÄŸitim uyumu artabilir; fakat genelleme iÃ§in Validation/Test performansÄ± belirleyicidir.

- Ã–zellik sayÄ±sÄ±, kalite ve boyutluluk:
    - Daha fazla Ã¶zellik her zaman daha iyi deÄŸildir; gÃ¼venilir olmayan (gÃ¼rÃ¼ltÃ¼lÃ¼, dÃ¼ÅŸÃ¼k ayÄ±rt edicilikte) Ã¶zellikler genellemeyi bozabilir.
    - Korelasyon: YÃ¼ksek korelasyonlu Ã¶zellikler gereksiz tekrar yaratÄ±r; Ã§oklu doÄŸrusal baÄŸlantÄ± (multicollinearity) bazÄ± modelleri (LR) kararsÄ±zlaÅŸtÄ±rÄ±r. Ã–zellik seÃ§imi/azaltma yapÄ±n.
    - GÃ¼rÃ¼ltÃ¼ ve Ã¶lÃ§Ã¼m hatalarÄ±: Ã–lÃ§Ã¼m sÃ¼recini iyileÅŸtirin; saÄŸlam istatistikler, dÃ¼zenlileÅŸtirme ve veri artÄ±rma ile etkisini azaltÄ±n.
    - Boyut laneti (curse of dimensionality): Boyut arttÄ±kÃ§a Ã¶rnek yoÄŸunluÄŸu seyrekleÅŸir; karar sÄ±nÄ±rlarÄ±nÄ± gÃ¼venle Ã¶ÄŸrenmek iÃ§in daha Ã§ok veri ve/veya dÃ¼zenlileÅŸtirme gerekir.
    - Ä°lke: Yeterli sayÄ±da ve iÅŸe yarar (ayÄ±rt edici, gÃ¼venilir) Ã¶zellik kullanÄ±n; mÃ¼mkÃ¼n olduÄŸunda Ã¶zellik mÃ¼hendisliÄŸi ve boyut azaltma (PCA/LDA) ile temsilinizi sadeleÅŸtirin.

- Ã–znitelik (Feature): Bir olgunun anlaÅŸÄ±lÄ±r, ayÄ±rt edici ve baÄŸÄ±msÄ±z Ã¶lÃ§Ã¼lebilir Ã¶zellikleridir. Etkili Ã¶rÃ¼ntÃ¼ tanÄ±ma, sÄ±nÄ±flandÄ±rma ve regresyon iÃ§in kritiktir. Genellikle sayÄ±saldÄ±r; ancak sentaktik Ã¶rÃ¼ntÃ¼ analizinde kelimeler, Ã§izgiler gibi yapÄ±sal Ã¶ÄŸeler de Ã¶znitelik olarak kullanÄ±labilir. Ä°yi Ã¶znitelikler Ã¶ÄŸrenmeyi kolaylaÅŸtÄ±rÄ±r, genelliÄŸi ve yorumlanabilirliÄŸi artÄ±rÄ±r. Ã–znitelik Ã§Ä±karÄ±mÄ±/seÃ§imi (Ã¶znitelik mÃ¼hendisliÄŸi) farklÄ± yÃ¶ntemlerin denenmesini ve alan uzmanÄ± sezgisini gerektirir.

- Ã–znitelik vektÃ¶rÃ¼ (Feature vector): SayÄ±sal Ã¶zniteliklerin birlikte temsil edildiÄŸi vektÃ¶rdÃ¼r (Ã¶r. x âˆˆ â„^d). Ä°kili sÄ±nÄ±flandÄ±rmada sÄ±k kullanÄ±lan basit kural: aÄŸÄ±rlÄ±klar vektÃ¶rÃ¼ w ile skaler Ã§arpÄ±m wÂ·x bir eÅŸik deÄŸeri Ï„ ile karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r; wÂ·x â‰¥ Ï„ ise sÄ±nÄ±f 1, aksi halde 0 olarak etiketlenir. Ã–znitelik vektÃ¶rÃ¼nÃ¼ kullanan yÃ¶ntemlere Ã¶rnek: k-en yakÄ±n komÅŸu (k-NN), yapay sinir aÄŸlarÄ± (YSA) ve BayesÃ§i yaklaÅŸÄ±mlar (Ã¶rn. Naive Bayes).

- HesaplamalÄ± dÃ¼ÅŸÃ¼nme (Computational thinking): Bilgisayar temelli problem Ã§Ã¶zme yaklaÅŸÄ±mÄ±dÄ±r; temel bileÅŸenleri soyutlama, ayrÄ±ÅŸtÄ±rma, modÃ¼lerlik, algoritma tasarÄ±mÄ± ve deÄŸerlendirmedir.
- Veri bilimi (Data science): Veri aÃ§Ä±sÄ±ndan zengin sorunlarÄ± Ã§Ã¶zmek iÃ§in disiplinler arasÄ± bir yaklaÅŸÄ±mdÄ±r; makine Ã¶ÄŸrenimi, bÃ¼yÃ¼k Ã¶lÃ§ekli bilgi iÅŸlem, semantik meta veriler, veri iÅŸ akÄ±ÅŸlarÄ± ve gÃ¶rselleÅŸtirmeyi kapsar.
- Kesikli (sÃ¼reksiz) deÄŸiÅŸken: TanÄ±mlÄ± olduÄŸu aralÄ±klarda yalnÄ±zca ayrÄ±k/sonlu (veya sayÄ±labilir) deÄŸerler alÄ±r. Ã–rn. sÄ±nÄ±f etiketi, olay sayÄ±sÄ±.
- SÃ¼rekli deÄŸiÅŸken: TanÄ±mlÄ± olduÄŸu aralÄ±kta sonsuz (devamlÄ±) deÄŸer kÃ¼mesini alabilir. Ã–rn. sÄ±caklÄ±k, sÃ¼re.

- Nicel (kantitatif) deÄŸiÅŸkenler: Ã–lÃ§Ã¼m sonucu deÄŸerleri saptanan sayÄ±sal Ã¶zellikleri ifade eder; sayÄ±labilir veya Ã¶lÃ§Ã¼lebilir bÃ¼yÃ¼klÃ¼klerdir. Ã–rn. yaÅŸ, gelir, sÄ±caklÄ±k.
- Nitel (kalitatif) deÄŸiÅŸkenler: Karakteristik Ã¶zellikleri/durumlarÄ± ifade eder; sayÄ±lamayan, birimlendirilemeyen ve Ã¶lÃ§Ã¼lebilir olmayan niteliklerdir. Ã–rn. renk, kategori, marka (sÄ±fat niteliÄŸinde ayÄ±rt ediciler).

- SÄ±klÄ±k (frekans) daÄŸÄ±lÄ±mÄ±: Verilerin deÄŸerlerine gÃ¶re nasÄ±l daÄŸÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶steren Ã¶zet yapÄ±dÄ±r; aynÄ± periyoda sahip bir sinyalin bÃ¼tÃ¼n iÃ§inde kaÃ§ kez tekrar ettiÄŸini ifade eder.
- SÄ±nÄ±f: EÅŸit ya da birbirine yakÄ±n deÄŸerlere sahip deneklerin/Ã¶rneklerin oluÅŸturduÄŸu her bir gruptur.
- EÅŸik (threshold): Karar verme iÃ§in kullanÄ±lan eÅŸik deÄŸeridir (Ã¶r. olasÄ±lÄ±k > 0.5 â‡’ pozitif). EÅŸik, modelin parametresi deÄŸildir; uygulama hedeflerine gÃ¶re ayarlanÄ±r.
- Sapma (bias) terimi: BirÃ§ok modelde sabit terim b (intercept) olarak geÃ§er ve Ã¶ÄŸrenilen parametrelerin bir parÃ§asÄ±dÄ±r; karar eÅŸiÄŸi ile karÄ±ÅŸtÄ±rÄ±lmamalÄ±dÄ±r.

### Bir Modelin SeÃ§ilmesi iÃ§in Gereken Kriterler

Bu bÃ¶lÃ¼m, model seÃ§imini deÄŸerlendirirken dikkate alÄ±nmasÄ± gereken temel Ã¶lÃ§Ã¼tleri Ã¶zetler. Kriterler, problem baÄŸlamÄ±na (gÃ¶rev tipi, veri yapÄ±sÄ±, kÄ±sÄ±tlar) ve iÅŸletim ortamÄ±na gÃ¶re Ã¶nceliklendirilmelidir.

#### Performans ve DoÄŸruluk Kriterleri

- DoÄŸruluk/Kesinlik (Accuracy)
    - TanÄ±m: Modelin gÃ¶rdÃ¼ÄŸÃ¼/gÃ¶rmediÄŸi Ã¶rneklerde doÄŸru etiket tahmin etme oranÄ±; genellikle Accuracy = DoÄŸruTahminler / TÃ¼mTahminler.
    - Not: Dengesiz veri setlerinde tek baÅŸÄ±na yanÄ±ltÄ±cÄ± olabilir; Kesinlik (Precision), DuyarlÄ±lÄ±k (Recall), F1, ROCâ€‘AUC/PRâ€‘AUC gibi problemâ€‘Ã¶zel metriklerle birlikte kullanÄ±lmalÄ±dÄ±r. Kritik alanlarda (Ã¶r. teÅŸhis) baÅŸarÄ± metrikleri birincil Ã¶nemdedir.

- Genelleme YeteneÄŸi (Generalization)
    - TanÄ±m: Modelin eÄŸitim verisini ezberlemeden, gÃ¶rÃ¼lmeyen (test) veride de yÃ¼ksek performans gÃ¶stermesi.
    - Ä°lgili riskler: AÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting), eksik Ã¶ÄŸrenme (underfitting). Ã‡Ã¶zÃ¼m: Ã‡apraz doÄŸrulama, dÃ¼zenlileÅŸtirme, erken durdurma, veri artÄ±rma ve titiz validasyon.

#### Operasyonel ve Verimlilik Kriterleri

- HÄ±z (Speed)
    - EÄŸitim HÄ±zÄ±: Modelin ne kadar hÄ±zlÄ± Ã¶ÄŸrenebildiÄŸi.
    - Ã‡Ä±karÄ±m (Inference) HÄ±zÄ±: CanlÄ± sistemde yanÄ±t sÃ¼resi. GerÃ§ek zamanlÄ± akÄ±ÅŸlarda (Ã¶r. sosyal medya, IoT) Ã§oÄŸu zaman Ã¶ncelikli kriterdir; gerektiÄŸinde doÄŸruluktan kÃ¼Ã§Ã¼k Ã¶dÃ¼n verilebilir.

- Ã–lÃ§eklenebilirlik (Scalability)
    - TanÄ±m: Veri boyutu/karmaÅŸÄ±klÄ±ÄŸÄ±/Ã¶zellik sayÄ±sÄ± arttÄ±kÃ§a doÄŸruluk ve hÄ±zÄ±n kabul edilebilir dÃ¼zeyde kalmasÄ±.
    - Not: BÃ¼yÃ¼k veri ve daÄŸÄ±tÄ±k ortamlarda (distributed training/serving) yatay/dikey Ã¶lÃ§eklenebilirlik Ã¶nemlidir.

- Kaynak TÃ¼ketimi (Resource Consumption)
    - TanÄ±m: EÄŸitim ve tahmin iÃ§in gereken CPU/GPU, RAM ve disk.
    - Not: Mobil/IoT gibi kÄ±sÄ±tlÄ± ortamlarda daha hafif (parametre/veri ayak izi kÃ¼Ã§Ã¼k, bellek/dÃ¶ngÃ¼ bÃ¼tÃ§esi dÃ¼ÅŸÃ¼k) modeller tercih edilir.

#### YapÄ±sal ve Uygulama Kriterleri

- SaÄŸlamlÄ±k (Robustness)
    - TanÄ±m: Eksik, gÃ¼rÃ¼ltÃ¼lÃ¼ ve aykÄ±rÄ± deÄŸerli verilerden minimum etkilenme.
    - Not: AÄŸaÃ§ tabanlÄ± yÃ¶ntemler (RF/GBM) Ã§oÄŸu senaryoda lineer modellere gÃ¶re aykÄ±rÄ±lara daha dayanÄ±klÄ± olabilir; yine de Ã¶n iÅŸleme kritiktir.

- Yorumlanabilirlik (Interpretability / Explainability)
    - TanÄ±m: Modelin kararlarÄ±nÄ±n insanlarca anlaÅŸÄ±labilir ve gerekÃ§elendirilebilir olmasÄ±.
    - Not: Finans, tÄ±p, hukuk gibi yÃ¼ksek riskli alanlarda zorunludur. DoÄŸrusal Regresyon ve Karar AÄŸaÃ§larÄ± genelde yÃ¼ksek yorumlanabilirlik sunar; derin aÄŸlar iÃ§in SHAP/LIME gibi araÃ§lar gerekebilir.

#### BaÄŸlama GÃ¶re Ã–nceliklendirme Ã–rnekleri

- GerÃ§ek ZamanlÄ±/Akan Veri: HÄ±z ve Ã–lÃ§eklenebilirlik Ã¶ncelikli; kabul edilebilir doÄŸrulukla dÃ¼ÅŸÃ¼k gecikme hedeflenir.
- Kritik Analiz (HastalÄ±k, Kredi Riski): DoÄŸruluk/Kesinlik ve Yorumlanabilirlik Ã¶ncelikli; kararlarÄ±n aÃ§Ä±klanabilir olmasÄ± esastÄ±r.
- KÄ±sÄ±tlÄ± Kaynak (Mobil/Edge): Kaynak TÃ¼ketimi ve HÄ±z Ã¶ncelikli; model boyutu ve hesap maliyeti sÄ±nÄ±rlÄ±dÄ±r.

## Makine Ã¶ÄŸrenme sorunu

- AmaÃ§: GÃ¶zlem/Ã¶lÃ§Ã¼m olarak elde ettiÄŸimiz girdiler x'e dayanarak bilinmeyen bir Ã§Ä±ktÄ±yÄ± y ile tahmin edebilecek bir model (hipotez) bulmaktÄ±r.
- Hipotez (model): h(x) ile gÃ¶sterilir; h : X â†’ Y ve tahmin yÌ‚ = h(x) biÃ§imindedir.
- Ä°fade: [Durum, x] â†’ (Olay Modeli / hipotez h) â†’ [Tahmin, yÌ‚]
    - "Durum" (context) sensÃ¶r, zaman, ortam bilgisi olabilir; x genellikle dâ€‘boyutlu bir Ã¶zellik vektÃ¶rÃ¼dÃ¼r (x âˆˆ â„^d).
    - h(x) modelin verdiÄŸi iliÅŸki; Ã¶ÄŸrenme sÃ¼recinde parametreler (Î¸) ayarlanarak h_Î¸(x) elde edilir.
- Notlar:
    - Hipotez sÄ±nÄ±fÄ± (doÄŸrusal, aÄŸaÃ§, NN vb.) problem ve veri bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne gÃ¶re seÃ§ilir.
    - Ã–ÄŸrenme: Verilen (x_i, y_i) Ã¶rnekleri kullanÄ±larak h_Î¸ en iyi uyumu saÄŸlayacak ÅŸekilde optimize edilir.

> \
> ![Makine Ã¶ÄŸrenme sorunu â€” ÅŸema](Images/33.jpg)
>
> AÃ§Ä±klama: Girdiler x â†’ Hipotez h(x) â†’ Tahmin yÌ‚. Hipotez (model) eÄŸitim verisiyle Ã¶ÄŸrenilir.

> \
> ![h(x) â€” Ã¶rnek temsili](Images/34.jpg)
>
> AÃ§Ä±klama: h(x) fonksiyonunun kavramsal gÃ¶sterimi; x bir Ã¶zellik vektÃ¶rÃ¼ olabilir ve h parametrelerle tanÄ±mlanÄ±r.

## Lineer Regresyon

- BÃ¼tÃ¼n makine Ã¶ÄŸrenme uygulamalarÄ± aynÄ± genel yÃ¶ntemi takip eder:
    - Bir olayÄ±n modeli iÃ§in ve bir baÅŸarÄ± Ã¶lÃ§Ã¼mÃ¼ iÃ§in, Ã¶nceden var olan verilerden olayÄ±n uygun modelini bulmak ve gelecekteki kararlar iÃ§in bu modeli kullanmak.
- FarklÄ± makine Ã¶ÄŸrenme yÃ¶ntemleri, farklÄ± olay modelleri kullanabilir; Ã¶rneÄŸin aÄŸaÃ§ tabanlÄ± yÃ¶ntemler, kernel yÃ¶ntemleri veya sinir aÄŸlarÄ± farklÄ± fonksiyon sÄ±nÄ±flarÄ±dÄ±r.
- Lineer regresyon modeli, bu yÃ¶ntemler iÃ§inde en basit modellerden biridir ve birÃ§ok problemde temel/baÅŸlangÄ±Ã§ yaklaÅŸÄ±mÄ± olarak kullanÄ±lÄ±r; amaÃ§, y â‰ˆ wáµ€x + b biÃ§iminde bir doÄŸrusal iliÅŸki kurup hata (Ã¶r. MSE) minimize etmektir.

- Lineer iliÅŸki varsayÄ±mÄ±:
    - Lineer regresyonda, neden (girdi x) ile sonuÃ§ (Ã§Ä±ktÄ± y) arasÄ±nda yaklaÅŸÄ±k olarak doÄŸrusal bir iliÅŸki olduÄŸu varsayÄ±lÄ±r: y â‰ˆ wáµ€x + b.
    - Neden bÃ¶yle bir varsayÄ±m?
        - Basitlik ve yorumlanabilirlik: KatsayÄ±lar w, her Ã¶zelliÄŸin y Ã¼zerindeki yÃ¶nlÃ¼ etkisini doÄŸrudan gÃ¶sterir.
        - Ä°lk yaklaÅŸÄ±m (baseline): Ã‡ok karmaÅŸÄ±k modellerden Ã¶nce doÄŸrusal bir model denemek hÄ±zlÄ± ve bilgilendiricidir.
        - Yerel doÄŸrusal yaklaÅŸÄ±m: BirÃ§ok sistemin kÃ¼Ã§Ã¼k bÃ¶lge/lineerizasyon dÃ¼zeyinde doÄŸrusal davranÄ±ÅŸÄ± vardÄ±r; doÄŸrusal model, daha karmaÅŸÄ±k fonksiyonlarÄ±n yerel bir yaklaÅŸÄ±k Ã§Ã¶zÃ¼mÃ¼ olabilir.
        - Hesaplama ve optimizasyon: OLS (en kÃ¼Ã§Ã¼k kareler) gibi kapalÄ±-form Ã§Ã¶zÃ¼mler ve verimli gradyan yÃ¶ntemleri sayesinde hÄ±zlÄ±ca Ã¶ÄŸrenilebilir.
    - UyarÄ±: EÄŸer gerÃ§ek iliÅŸki gÃ¼Ã§lÃ¼ biÃ§imde doÄŸrusal deÄŸilse, polinom terimler, Ã¶zellik dÃ¶nÃ¼ÅŸÃ¼mleri veya daha karmaÅŸÄ±k modeller kullanmak gerekir.

### Notasyon (sÃ¼rekli kullanÄ±lacaktÄ±r)

- m: Ã–nceden var olan Ã¶rneklerin sayÄ±sÄ± (eÄŸitim kÃ¼mesindeki Ã¶rnek sayÄ±sÄ±).
- EÄŸitim kÃ¼mesi: Elimizdeki geÃ§miÅŸ verilerin tÃ¼mÃ¼; her Ã¶rnek bir (x, y) Ã§ifti iÃ§erir.
- x: Girdi deÄŸiÅŸkeni / baÄŸÄ±msÄ±z deÄŸiÅŸken / aÃ§Ä±klayÄ±cÄ± deÄŸiÅŸken / "neden" faktÃ¶rÃ¼ (Ã¶r. reklam harcamalarÄ±).
- y: Ã‡Ä±ktÄ± deÄŸiÅŸkeni / baÄŸÄ±mlÄ± deÄŸiÅŸken / sonuÃ§ (Ã¶r. gelen Ã¶ÄŸrenci sayÄ±sÄ±).
- (x, y): Tek bir Ã¶rneÄŸi gÃ¶sterir â€” bir girdi ve ona karÅŸÄ±lÄ±k gelen Ã§Ä±ktÄ±.
- (x_i, y_i): EÄŸitim kÃ¼mesindeki i numaralÄ± Ã¶rnek; i = 1, 2, ..., m.
- h(x) â€” Ä°liÅŸki fonksiyonu

- h(x) modelin iliÅŸki fonksiyonudur; girdi x iÃ§in modelin Ã¼rettiÄŸi tahmini Ã§Ä±ktÄ± Å· = h(x) olarak gÃ¶sterilir.
- Lineer regresyonda Ã¶zel olarak: h(x) = wáµ€x + b (tek deÄŸiÅŸkenli iÃ§in h(x) = w x + b). Burada w ve b model parametreleridir.
- Hedef: EÄŸitim verisinden (x_i, y_i) Ã¶rneklerini kullanarak parametreleri Ã¶ÄŸrenmek ve yeni x'ler iÃ§in gÃ¼venilir Å· tahminleri Ã¼retmektir.

- Hipotez (h): "h(x)" fonksiyonuna hipotez denir.
    - Yani, olayÄ±n modeli veya x ile y arasÄ±ndaki olasÄ± iliÅŸki fonksiyonu iÃ§in belli bir ÅŸekli (fonksiyon sÄ±nÄ±fÄ±nÄ±) Ã¶nceden varsayÄ±yoruz; buna hipotez sÄ±nÄ±fÄ± denir (Ã¶r. doÄŸrusal, polinom, aÄŸaca dayalÄ±, NN vb.).
    - Ã–ÄŸrenme sÃ¼recinde bu hipotez sÄ±nÄ±fÄ± iÃ§inden veriyle en iyi uyumu saÄŸlayan (parametreleri) seÃ§eriz; bu yÃ¼zden iyi bir hipotez sÄ±nÄ±fÄ± seÃ§imi Ã¶nemlidir.

- Basit lineer hipotez Ã¶rneÄŸi:
    - En basit model/hipotez tek deÄŸiÅŸkenli iÃ§in ÅŸÃ¶yle yazÄ±lÄ±r:
        - y = h_Î¸(x) = Î¸0 + Î¸1 Â· x
    - Bu modele "lineer model/hipotezi" denir.
    - Burada y baÄŸÄ±mlÄ± deÄŸiÅŸken, x baÄŸÄ±msÄ±z deÄŸiÅŸkendir; Î¸0 ve Î¸1 parametre/ aÄŸÄ±rlÄ±k deÄŸerleridir.
    - Î¸0 terimi bias (sapma) olarak adlandÄ±rÄ±lÄ±r; pratikte Î¸0'in Ã§arpanÄ± x0 = 1 olarak alÄ±nÄ±r, bu yÃ¼zden formÃ¼lde ayrÄ± yazÄ±lÄ±r.
    - Genel Ã§ok deÄŸiÅŸkenli formda: y = h_Î¸(x) = Î¸0 + Î¸1 x1 + Î¸2 x2 + ... + Î¸d xd = Î¸áµ€ xÌƒ, burada xÌƒ = [1, x1, ..., xd]áµ€ ve Î¸ = [Î¸0, Î¸1, ..., Î¸d]áµ€.


    - Ã–ÄŸrenme sÃ¼reci (parametre seÃ§imi):
        - Ã–ÄŸrenme, uygun modeli (hipotez) seÃ§tikten sonra eÄŸitim kÃ¼mesindeki Ã¶rnekleri kullanarak hipotez fonksiyonunun parametrelerini (Î¸) belirlemektir.
        - Tek deÄŸiÅŸkenli lineer Ã¶rnek iÃ§in
            - y = h_Î¸(x) = Î¸0 + Î¸1 Â· x
            - Burada (Î¸0, Î¸1) modelin parametreleridir ve eÄŸitim verisiyle (Ã¶r. OLS veya optimizasyon) tahmin edilir.
        - Ã–ÄŸrenme sonrasÄ±nda elde edilen Î¸ ile yeni x iÃ§in tahmin Å· = h_Î¸(x) yapÄ±lÄ±r.

     - Ã–rnek uygulama â€” Reklam harcamalarÄ± â†’ Gelecek Ã¶ÄŸrenci sayÄ±sÄ±:
    - Problem: Reklam harcamalarÄ±na (Ã¶r. aylÄ±k TL) baÄŸlÄ± olarak gelecek Ã¶ÄŸrenci sayÄ±sÄ±nÄ± tahmin etmek istiyoruz.
    - Basit tek deÄŸiÅŸkenli model: y â‰ˆ wÂ·x + b (x = reklam harcamasÄ±, y = kayÄ±t/baÅŸvuru sayÄ±sÄ±). OLS ile w ve b Ã¶ÄŸrenilir.
    - Veri gereksinimi: GeÃ§miÅŸ reklam bÃ¼tÃ§eleri ve o dÃ¶nem gelen Ã¶ÄŸrenci sayÄ±larÄ±; eÄŸitim/validation/test split ile model doÄŸrulanmalÄ±dÄ±r.
    - Performans Ã¶lÃ§Ã¼mleri: RMSE, MAE ve RÂ² kullanÄ±lÄ±r; modelin geÃ§erliliÄŸi iÃ§in residual ( artÄ±k ) analizi yapÄ±n.
    - UyarÄ±lar: DoÄŸrusal iliÅŸki olmadÄ±ÄŸÄ± durumlarda log-transform, polinom terimler veya daha esnek modeller (Ã¶r. aÄŸaÃ§lar, SVR) deneyin; reklamÄ±n etkisi gecikmeli olabilir â€” zaman serisi yaklaÅŸÄ±mlarÄ± dÃ¼ÅŸÃ¼nÃ¼n.

> \
> ![Reklam harcamasÄ± vs Ã¶ÄŸrenci sayÄ±sÄ±](Images/29.jpg)
>
> AÃ§Ä±klama: Reklam harcamasÄ± (x) ile gelen Ã¶ÄŸrenci sayÄ±sÄ± (y) arasÄ±ndaki saÃ§Ä±lÄ±m ve doÄŸrusal fit Ã¶rneÄŸi; bu tÃ¼r uygulamalar lineer regresyonla hÄ±zlÄ± prototipleme iÃ§in uygundur.

> \
> ![Reklam harcamasÄ± â€” alternatif gÃ¶rsel (rezidÃ¼ller/fit)](Images/30.jpg)
>
> AÃ§Ä±klama: AynÄ± verinin alternatif gÃ¶sterimi â€” Ã¶rneÄŸin residual analizi veya farklÄ± bir fit gÃ¶rselleÅŸtirmesi. Model deÄŸerlendirmesinde Ã§eÅŸitli gÃ¶rselleÅŸtirmeler yararlÄ±dÄ±r.

- Model amaÃ§larÄ± (bu lineer regresyon iÃ§in Ã¶rnekler):
    - Gelecek Ã¶ÄŸrenci sayÄ±sÄ±nÄ± tahmin etmek (yeni dÃ¶nem iÃ§in baÅŸvuru/kayÄ±t tahmini).
    - Belirli bir hedef Ã¶ÄŸrenci sayÄ±sÄ±na ulaÅŸmak iÃ§in gereken reklam harcamasÄ±nÄ± hesaplamak (ters problem: x* = (y_target âˆ’ b)/w).

> \
> ![Regresyon amaÃ§larÄ± â€” tahmin ve ters problem](Images/31.jpg)
>
> AÃ§Ä±klama: Lineer regresyonun iki yaygÄ±n kullanÄ±m amacÄ±: doÄŸrudan tahmin (yÌ‚) ve ters problem (hedefe ulaÅŸmak iÃ§in gereken x miktarÄ±nÄ± hesaplama).

> \
> ![Lineer regresyon â€” Ã¶rnek gÃ¶rsel 36](Images/36.jpg)
>
> AÃ§Ä±klama: Lineer regresyon uygulamasÄ±na ait ek bir gÃ¶rsel; eÄŸitim verisi ve doÄŸrusal fitin farklÄ± bir gÃ¶sterimi.
>
> \

> ![Lineer regresyon â€” Ã¶rnek gÃ¶rsel 38](Images/38.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 38 â€” eÄŸitim verisi ile elde edilen fitin detaylÄ± gÃ¶sterimi; parametre tahmini ve veri-saÃ§Ä±lÄ±mÄ± iliÅŸkisini gÃ¶sterir.
>
> ![Lineer regresyon â€” Ã¶rnek gÃ¶rsel 39](Images/39.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 39 â€” modelin hata/rezidÃ¼ daÄŸÄ±lÄ±mÄ± veya deÄŸerlendirme gÃ¶stergelerinin (Ã¶r. residual plot) gÃ¶rselleÅŸtirmesi.

## Ã‡ok deÄŸiÅŸkenli (Ã§ok boyutlu) Lineer Regresyon

"Gelecek Ã¶ÄŸrenci sayÄ±sÄ±nÄ± tahmin etmek" â€” Ã¶rnek Ã¶zellikler (faktÃ¶rler):
- Reklam harcamalarÄ±
- Okuldan mezun olan Ã¶ÄŸrenci sayÄ±sÄ±
- Ã–ÄŸrencilerin ortalama notu
- KayÄ±t gÃ¼nÃ¼ndeki hava sÄ±caklÄ±ÄŸÄ±
- Bizim programÄ±mÄ±zÄ±n Ã¼creti
- DiÄŸer Ã¼niversitelerin Ã¼cretleri
- ...

Bu bÃ¼tÃ¼n faktÃ¶rler sonucumuzu etkileyebilirler. BÃ¶yle problemlere "Ã§ok boyutlu lineer regresyon" diyoruz: sonuÃ§ birÃ§ok faktÃ¶re baÄŸlÄ±dÄ±r. Bu faktÃ¶rlere makine Ã¶ÄŸrenmesi uygulamalarÄ±nda "Ã¶zellik" (feature) denir.

BirÃ§ok boyutlu lineer regresyon modeli Ã¶rneÄŸi:
- Ã–nce (tek deÄŸiÅŸkenli): y = h_Î¸(x) = Î¸0 + Î¸1 x
- Åimdi (Ã§ok deÄŸiÅŸkenli): y = h_Î¸(x) = Î¸0 + Î¸1 x1 + Î¸2 x2 + ... + Î¸n x_n

Ã–rnek Ã¶zellik haritasÄ± (x_i):
- x1: Reklam harcamalarÄ±
- x2: Okuldan mezun olan Ã¶ÄŸrenci sayÄ±sÄ±
- x3: Lisans programÄ±mÄ±zÄ±n Ã¼creti
- x4: DiÄŸer Ã¼niversitelerin Ã¼cretleri
- x5: Ã–ÄŸrencilerin ortalama notu
- x6: KayÄ±t gÃ¼nÃ¼ndeki hava sÄ±caklÄ±ÄŸÄ±
- ...

- Ã–nce: bir Ã¶zellik (x), iki parametre (Î¸0, Î¸1)
- Åimdi: n Ã¶zellik (x_i), n+1 parametre (Î¸0, Î¸1, Î¸2, ..., Î¸n)


### Maliyet fonksiyonu

- Hipotez parametreleri iyi ÅŸekilde nasÄ±l seÃ§ilebilir?
    - "Ä°yi" parametre seÃ§imi: EÄŸitim verisi Ã¼zerinde modelin tahminleri gerÃ§ek Ã§Ä±ktÄ±lara yakÄ±n (yani hata kÃ¼Ã§Ã¼k) olmalÄ±dÄ±r.

- Bunun iÃ§in bir maliyet (cost) fonksiyonu kullanÄ±lÄ±r:

- FarklÄ± modellerin uygunluÄŸunu kesinleÅŸtirmek iÃ§in, maliyet fonksiyonu kullanÄ±lmaktadÄ±r.
    - Maliyet fonksiyonu, bir modelin var olan verilere uygunluÄŸu ya da iyiliÄŸini sayÄ±sal olarak belirtir; daha kÃ¼Ã§Ã¼k J(Î¸) deÄŸeri genellikle daha iyi uyum demektir.

- Bunun iÃ§in bir maliyet (cost) fonksiyonu kullanÄ±lÄ±r:
    - Ã–rnek baÅŸÄ±na kayÄ±p (loss) olarak kare hatayÄ± kullanÄ±rÄ±z: L(h_Î¸(x^{(i)}), y^{(i)}) = (h_Î¸(x^{(i)}) - y^{(i)})^2.
    - Toplam/ortalama maliyet fonksiyonu (Mean Squared Error tabanlÄ±):

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^{2}
$$

        - Buradaki 1/(2m) sabiti, tÃ¼rev alÄ±ndÄ±ÄŸÄ±nda ortaya Ã§Ä±kan 2 faktÃ¶rÃ¼nÃ¼ dengeler ve gradyan ifadesini daha sade yapar.

- Hipotez parametrelerini seÃ§me (Ã¶ÄŸrenme):
    - AmaÃ§: J(Î¸)'yi minimize eden Î¸ deÄŸerlerini bulmak.
    - YaklaÅŸÄ±mlar:
    - Analitik Ã§Ã¶zÃ¼m (Normal denklem) â€” lineer OLS iÃ§in: Î¸ = (X^T X)^{-1} X^T y (kÃ¼Ã§Ã¼k/orta boy veri, X^T X terslenebiliyorsa; terslenemiyorsa yalancÄ± ters (pseudo-inverse)/SVD kullanÄ±lÄ±r).
        - Ä°teratif optimizasyon â€” Gradyan Ä°niÅŸi (Gradient Descent):
            - GÃ¼ncelleme: Î¸ := Î¸ - Î± âˆ‡_Î¸ J(Î¸), Ã¶rn. tek adÄ±mda her j iÃ§in
$$
	heta_j := \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)}) x^{(i)}_j
$$
        - Normal fonksiyonun convex olmasÄ± sayesinde lineer regresyonda global minimum garanti edilir.

- Bizim sorunumuza tekrar bakalÄ±m:
    - Problem ÅŸu: EÄŸitim kÃ¼mesi \{(x^{(i)}, y^{(i)})\}_{i=1}^m verildiÄŸinde, uygun bir maliyet fonksiyonu (Ã¶r. J(Î¸) yukarÄ±daki gibi) tanÄ±mlayÄ±p, bu maliyeti minimize eden Î¸'yÄ± bulmalÄ±yÄ±z; elde edilen Î¸ ile yeni girdiler iÃ§in tahmin \hat{y} = h_{\theta}(x) yapÄ±lÄ±r.

- Ä°yi bir hipotez iÃ§in, tahmin edilen y'lerin eÄŸitim kÃ¼mesindeki y'lere yakÄ±n olmasÄ±nÄ± isteriz.
- Bu Ã¶lÃ§Ã¼de, (Î¸0, Î¸1) model parametreleri, tahmin edilen y'lerinin gerÃ§ek var olan verilere en yakÄ±n olmasÄ±nÄ± saÄŸlamak zorundadÄ±r.
- Tahmin edilen y'lerin gerÃ§ek verilerden uzaklÄ±ÄŸÄ±nÄ± Ã¶lÃ§mek iÃ§in ÅŸu maliyet fonksiyonu kullanÄ±labilir:

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \bigl(h_{\theta}(x^{(i)}) - y^{(i)}\bigr)^{2}
$$

        - Bu fonksiyon, her eÄŸitim Ã¶rneÄŸi iÃ§in tahmin ile gerÃ§ek deÄŸer arasÄ±ndaki kare farklarÄ± toplar ve ortalamasÄ±nÄ± alÄ±r (1/(2m) sabiti tÃ¼revleri sadeleÅŸtirmek iÃ§indir).

        - Bu durumda, J fonksiyonuna â€œmaliyet fonksiyonuâ€ denir.
        - Maliyet fonksiyonu, farklÄ± modellerin gerÃ§ek verilere ne kadar yakÄ±n olduÄŸunu tanÄ±mlamaktadÄ±r.
        - BÃ¼yÃ¼k J deÄŸerleri, h_{\theta}(x) deÄŸerlerinin gerÃ§ek verilerden Ã§ok uzak olduÄŸunu gÃ¶stermektedir.

    - Bir Ã¶rnek iÃ§in aralÄ±k (rezidÃ¼):

$$
r^{(i)} = h_{\theta}(x^{(i)}) - y^{(i)}
$$

    - EÄŸer bu aralÄ±klar (rezidÃ¼ler) bÃ¼yÃ¼k ise, model veriyi iyi yakalayamamÄ±ÅŸ demektir; bu durumda tek tek aralÄ±klar bÃ¼yÃ¼k olacak ve toplam/ortalama maliyet J(Î¸) de bÃ¼yÃ¼k olacaktÄ±r.

    > ![RezidÃ¼ Ã¶rnekleri â€” gÃ¶rsel 41](Images/41.jpg)
    >
    > AÃ§Ä±klama: GÃ¶rsel 41 â€” her Ã¶rnek iÃ§in tahmin ile gerÃ§ek deÄŸer arasÄ±ndaki farklarÄ±n (rezidÃ¼lerin) gÃ¶sterimi; bÃ¼yÃ¼k sapmalar modelin kÃ¶tÃ¼ uyduÄŸunu gÃ¶sterir.
    >
    > En iyi model, gerÃ§ek verilere en yakÄ±n modeldir; dolayÄ±sÄ±yla en kÃ¼Ã§Ã¼k J(Î¸) deÄŸerine sahip olandÄ±r.
    >
    > ![En iyi model â€” gÃ¶rsel 42](Images/42.jpg)
    >
    > AÃ§Ä±klama: GÃ¶rsel 42 â€” en iyi modelin seÃ§imi iÃ§in maliyet deÄŸerlerinin karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±; kÃ¼Ã§Ã¼k J en iyi uyuma iÅŸaret eder.
    >
    > Model tahminlerinin gerÃ§ek verilere en yakÄ±n olmasÄ±nÄ± saÄŸlamak, (Î¸0, Î¸1) iÃ§in bu optimizasyon problemidir.
    >
    > ![Optimizasyon â€” gÃ¶rsel 43](Images/43.jpg)
    >
    > AÃ§Ä±klama: GÃ¶rsel 43 â€” parametrelerin (Î¸0, Î¸1) optimizasyonu ve maliyet yÃ¼zeyi/araÅŸtÄ±rÄ±lmasÄ± gÃ¶rselleÅŸtirmesi.
    >
    > Tek deÄŸiÅŸkenli lineer regresyon probleminde maliyet, iki parametreye (Î¸0, Î¸1) baÄŸlÄ±dÄ±r; yani J(Î¸) iki boyutlu bir fonksiyondur.
    >
    > ![Maliyet yÃ¼zeyi â€” gÃ¶rsel 44](Images/44.jpg)
    >
    > AÃ§Ä±klama: GÃ¶rsel 44 â€” (Î¸0, Î¸1) uzayÄ±nda maliyet yÃ¼zeyi / kontur plot; optimizasyonun niÃ§in 2D bir problem olduÄŸu gÃ¶sterilir.

## Dereceli azaltma metodu

- En iyi olay modeli, en iyi model parametreleri, en dÃ¼ÅŸÃ¼k maliyet deÄŸeri demektir.
- Maliyetin minimumunu nasÄ±l buluyoruz?

- Dereceli azaltma (gradient descent) metodu, Ã§ok gÃ¼Ã§lÃ¼ ve Ã§ok genel optimizasyon metodudur.
- Bir (Î¸0, Î¸1) noktasÄ±nda baÅŸlÄ±yoruz.
- DevamlÄ± olarak, J'nin deÄŸerlerini azaltmak iÃ§in (Î¸0, Î¸1) uzayÄ±nda kÃ¼Ã§Ã¼k adÄ±mlar yapÄ±yoruz.
- J'nin deÄŸerleri her adÄ±mda dÃ¼ÅŸmek zorunda deÄŸildir; uygun Ã¶ÄŸrenme oranÄ± (Î±) seÃ§imi ve konveks J iÃ§in genel eÄŸilim azalÄ±r ve yakÄ±nsama beklenir. Aksi hÃ¢lde salÄ±nÄ±m/daÄŸÄ±lma gÃ¶rÃ¼lebilir.

> ![Dereceli azaltma â€” gÃ¶rsel 45](Images/45.jpg)

- Bir noktada baÅŸladÄ±k ...
- ... J deÄŸerini azaltan kÃ¼Ã§Ã¼k adÄ±mlarÄ± yapÄ±yoruz ...
- ... minimum noktasÄ±na gelmek zorundayÄ±z

> ![Gradient descent steps â€” gÃ¶rsel 46](Images/46.jpg)

- Sadece lokal olarak bir minimumdur: baÅŸka bir noktadan baÅŸlayÄ±nca baÅŸka noktalara gelmek mÃ¼mkÃ¼ndÃ¼r.
- Genellikle, bu metot birkaÃ§ rastgele baÅŸlangÄ±Ã§ noktasÄ± ile Ã§alÄ±ÅŸtÄ±rÄ±lmalÄ± ve en iyi minimum seÃ§ilmelidir.

> ![Lokal minimum Ã¶rnekleri â€” gÃ¶rsel 47](Images/47.jpg)

- Ortadaki adÄ±mlarÄ±n, J deÄŸerini en Ã§ok azaltmasÄ±nÄ± isteriz.
- Bunun iÃ§in, adÄ±mlarÄ± "gradient" (yani eÄŸim) yÃ¶nÃ¼nde yaparÄ±z.
- Dereceli azaltma algoritmasÄ± (Ã¶zeti):

    - YakÄ±nsamaya kadar tekrarlama {
        - (her iterasyonda tÃ¼revler hesaplanÄ±r)
        - Î¸_j := Î¸_j - Î± * (1/m) Î£_{i=1}^m (h_Î¸(x^{(i)}) - y^{(i)}) x^{(i)}_j
    - }

> ![Gradient descent formula â€” gÃ¶rsel 48](Images/48.jpg)

- Bu formÃ¼lde tÃ¼retilen ifade, hatanÄ±n (rezidÃ¼lerin) x ile aÄŸÄ±rlÄ±klÄ± ortalamasÄ±dÄ±r ve Î± Ã¶ÄŸrenme oranÄ±dÄ±r.

> ![TÃ¼rev gÃ¶sterimi â€” gÃ¶rsel 49](Images/49.jpg)

- Ã–nemli Not:
    - TÃ¼revler, ÅŸu andaki (Î¸0, Î¸1) noktasÄ± iÃ§in hepsi dÃ¶ngÃ¼den Ã¶nce hesaplanmalÄ±. Sonra, Î¸0 ve Î¸1 deÄŸerleri gÃ¼ncelleÅŸtirilmelidir.
    - Î¸0, Î¸1'in gÃ¼ncelleÅŸtirilmesi tÃ¼revlerin hesaplanmasÄ±yla aynÄ± anda yapÄ±lmaz; yani (Î¸0, Î¸1) parÃ§a-parÃ§a ÅŸekilde gÃ¼ncelleÅŸtirilmez.

- YakÄ±nsamaya kadar tekrarlayÄ±n {
    - Î¸0 := Î¸0 - Î± * (1/m) Î£ (h_Î¸(x^{(i)}) - y^{(i)})
    - Î¸1 := Î¸1 - Î± * (1/m) Î£ (h_Î¸(x^{(i)}) - y^{(i)}) x^{(i)}
    - }

- Alpha (Î±) seÃ§imi â€” biraz dikkat edilmelidir:
    - KÃ¼Ã§Ã¼k Î± â†’ yavaÅŸ yakÄ±nsama.
    - BÃ¼yÃ¼k Î± â†’ ileri-geri yakÄ±nsama (divergens veya osilasyon riski).

> ![Alpha seÃ§im ilÃ¼strasyonu â€” gÃ¶rsel 50](Images/50.jpg)

- Alpha seÃ§me Ã¶rneÄŸi: tipik denenebilecek Î± deÄŸerleri: 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10 ...

> ![Alpha etkisi â€” gÃ¶rsel 51](Images/51.jpg)
> ![Alpha etkisi â€” gÃ¶rsel 52](Images/52.jpg)

- Ä°yi bir alpha seÃ§mek iÃ§in, birkaÃ§ alpha deÄŸeri denenir; her birinde dereceli azaltma Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r ve yakÄ±nsama/performance izlenir.


- AynÄ± eÄŸitim kÃ¼mesi iÃ§in birkaÃ§ hipotez denenebilir:
    > ![Hipotez karÅŸÄ±laÅŸtÄ±rmasÄ± â€” gÃ¶rsel 40](Images/40.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 40 â€” aynÄ± eÄŸitim verisi Ã¼zerinde farklÄ± hipotezlerin (Ã¶r. farklÄ± derece polinomlar, farklÄ± Ã¶zellik seÃ§imi vs.) karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±; maliyet fonksiyonu deÄŸerleri kullanÄ±larak hangi hipotezin daha iyi olduÄŸu sayÄ±sal olarak belirlenir.

> ![Lineer regresyon â€” Ã¶rnek gÃ¶rsel 37](Images/37.jpg)
>
> AÃ§Ä±klama: Model deÄŸerlendirmesi veya residual/diagnostic gÃ¶sterimi iÃ§in alternatif bir gÃ¶rselleÅŸtirme.

- Veri ve denetimli Ã¶ÄŸrenme:
    - GeÃ§miÅŸ senelerden veya benzer kurumlardan elimizde reklam harcamalarÄ± (girdi x) ve gelen Ã¶ÄŸrenci sayÄ±larÄ± (Ã§Ä±ktÄ± y) varsa, bu model denetimli Ã¶ÄŸrenme (supervised learning) ile Ã¶ÄŸrenilebilir.
    - Etiketli veri (x, y) kullanÄ±larak model parametreleri (w, b) OLS veya optimizasyon yÃ¶ntemiyle tahmin edilir; model seÃ§imi ve deÄŸerlendirme Train/Val/Test Ã¼zerine yapÄ±lÄ±r.

> \
> ![Denetimli Ã¶ÄŸrenme â€” reklam verisi Ã¶rneÄŸi](Images/32.jpg)
>
> AÃ§Ä±klama: GeÃ§miÅŸ reklam ve sonuÃ§ verileriyle denetimli Ã¶ÄŸrenme uygulanarak model elde edilir; bu model gelecekteki tahminler ve ters problemler iÃ§in kullanÄ±labilir.


## SÄ±nÄ±flandÄ±rma (Classification) â€” GiriÅŸ

- TanÄ±m: SÄ±nÄ±flandÄ±rma, girdileri ayrÄ±k etiketlere (sÄ±nÄ±flara) ayÄ±rma problemidir.
    - Ä°kili (binary) sÄ±nÄ±flandÄ±rma: {0, 1} veya {negatif, pozitif}
    - Ã‡ok sÄ±nÄ±flÄ± (multi-class) sÄ±nÄ±flandÄ±rma: {1, 2, ..., K}

- AmaÃ§: Bir karar sÄ±nÄ±rÄ±/karar kuralÄ± Ã¶ÄŸrenerek yeni bir x iÃ§in sÄ±nÄ±f etiketi yÌ‚ âˆˆ {sÄ±nÄ±flar} tahmin etmek.
- Tipik Ã¶rnekler: e-posta spam tespiti, hastalÄ±k var/yok, gÃ¶rÃ¼ntÃ¼de kedi/kÃ¶pek, kredi geri Ã¶deme riski, vb.
- DeÄŸerlendirme Ã¶lÃ§Ã¼tleri: DoÄŸruluk (accuracy), kesinlik (precision), duyarlÄ±lÄ±k (recall), F1, ROC-AUC; veri dengesiz ise sadece doÄŸruluk yanÄ±ltÄ±cÄ± olabilir.
- KayÄ±p/amaÃ§ Ã¶rneÄŸi: Lojistik kayÄ±p (log loss) ve Ã§apraz entropi; karar eÅŸiÄŸi (threshold) seÃ§imi uygulamaya baÄŸlÄ±dÄ±r.

> ![SÄ±nÄ±flandÄ±rma â€” gÃ¶rsel 53](Images/53.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 53 â€” sÄ±nÄ±flandÄ±rma problemine gÃ¶rsel bir giriÅŸ; karar sÄ±nÄ±rlarÄ± ve sÄ±nÄ±f bÃ¶lgeleri fikrini sezgisel olarak gÃ¶sterir.

## SÄ±nÄ±flandÄ±rma BaÅŸarÄ± Metrikleri

SÄ±nÄ±flandÄ±rma modellerini deÄŸerlendirirken kullanÄ±lan metrikler, sorunun baÄŸlamÄ±na gÃ¶re farklÄ± maliyetleri ve Ã¶ncelikleri yansÄ±tÄ±r. AyrÄ±ntÄ±lÄ± referans: scikitâ€‘learn â€” Classification metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics

> ![SÄ±nÄ±flandÄ±rma metrikleri â€” genel bakÄ±ÅŸ 125](Images/125.jpg)

### KarmaÅŸÄ±klÄ±k Matrisi (Confusion Matrix)

KarmaÅŸÄ±klÄ±k Matrisi, model tahminlerini gerÃ§ek etiketlerle karÅŸÄ±laÅŸtÄ±rarak hangi sÄ±nÄ±flarda doÄŸru/yanlÄ±ÅŸ yaptÄ±ÄŸÄ±nÄ± ayrÄ±ntÄ±lÄ± gÃ¶sterir. Ä°kili sÄ±nÄ±flamada genelde 2Ã—2, Ã§ok sÄ±nÄ±flÄ±da NÃ—N boyutundadÄ±r.

> ![KarmaÅŸÄ±klÄ±k matrisi â€” ÅŸema 126](Images/126.jpg)
> 
> ![KarmaÅŸÄ±klÄ±k matrisi â€” Ã¶rnek 127](Images/127.jpg)

TanÄ±mlar (ikili sÄ±nÄ±flama):

- GerÃ§ek Pozitif (TP): GerÃ§ekte Pozitif olanÄ± Pozitif tahmin etmek.
- GerÃ§ek Negatif (TN): GerÃ§ekte Negatif olanÄ± Negatif tahmin etmek.
- YanlÄ±ÅŸ Pozitif (FP, Tipâ€‘I Hata): Negatif Ã¶rneÄŸi yanlÄ±ÅŸlÄ±kla Pozitif tahmin etmek (yanlÄ±ÅŸ alarm).
- YanlÄ±ÅŸ Negatif (FN, Tipâ€‘II Hata): Pozitif Ã¶rneÄŸi yanlÄ±ÅŸlÄ±kla Negatif tahmin etmek (gÃ¶zden kaÃ§Ä±rma).

Neden FP ve FN ayrÄ± incelenmeli? Ã‡Ã¼nkÃ¼ hata maliyetleri baÄŸlama gÃ¶re deÄŸiÅŸir. Ã–rn. spam filtresi: FP (geÃ§erli eâ€‘postayÄ± spam sanmak) can sÄ±kÄ±cÄ±dÄ±r; tÄ±pta FN (hastayÄ± saÄŸlÄ±klÄ± saymak) kritik risk doÄŸurur.

Ã–rnek kÃ¼Ã§Ã¼k veri kÃ¼mesi ve tahminleri:

| BaÅŸvuru | EÄÄ°TÄ°M | YAÅ  | CÄ°NSÄ°YET | KABUL | TAHMÄ°N |
|:-------:|:------:|:----:|:--------:|:-----:|:------:|
| 1 | ORTA  | YAÅLI | ERKEK | EVET  | EVET  |
| 2 | Ä°LK   | GENÃ‡  | ERKEK | HAYIR | HAYIR |
| 3 | YÃœKSEK| ORTA  | KADIN | HAYIR | EVET  |
| 4 | ORTA  | ORTA  | ERKEK | EVET  | EVET  |
| 5 | Ä°LK   | ORTA  | ERKEK | EVET  | EVET  |
| 6 | YÃœKSEK| YAÅLI | KADIN | EVET  | EVET  |
| 7 | Ä°LK   | GENÃ‡  | KADIN | HAYIR | HAYIR |
| 8 | ORTA  | ORTA  | KADIN | EVET  | EVET  |

Bu tablo iÃ§in Ã¶zet (KABUL pozitif sÄ±nÄ±fÄ± â€œEVETâ€ kabul edilmiÅŸtir): TP=5, TN=2, FP=1, FN=0; Toplam=8.

Python ile karmaÅŸÄ±klÄ±k matrisi:

```python
from sklearn.metrics import confusion_matrix

y_true = [1, 0, 1, 0, 1, 1, 0, 0, 0, 0]
y_pred = [0, 1, 1, 0, 1, 1, 1, 0, 0, 0]

cm = confusion_matrix(y_true, y_pred)
print(cm)  # scikit-learn Ã§Ä±ktÄ±sÄ± [[TN, FP], [FN, TP]] dÃ¼zenindedir.
```

> ![Confusion matrix â€” Ã§Ä±ktÄ± Ã¶rneÄŸi 128](Images/128.jpg)

### Temel Ã–lÃ§Ã¼tler

- DoÄŸruluk (Accuracy):

$$
    	ext{ACC} = \frac{TP + TN}{TP + TN + FP + FN}.
$$

- Hata OranÄ± (Error Rate):

$$
    	ext{ER} = 1 - \text{ACC} = \frac{FP + FN}{TP + TN + FP + FN}.
$$

- DuyarlÄ±lÄ±k (Recall, Sensitivity, TPR):

$$
    	ext{TPR} = \frac{TP}{TP + FN}.
$$

- Kesinlik (Precision, PPV):

$$
    	ext{PPV} = \frac{TP}{TP + FP}.
$$

- F1â€‘Skoru (Harmonik ortalama):

$$
    	ext{F1} = 2\,\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}.
$$

Ä°puÃ§larÄ±:
- Dengesiz veri setlerinde Accuracy yanÄ±ltÄ±cÄ± olabilir; Recall/Precision/F1 ve sÄ±nÄ±fâ€‘bazlÄ± raporlarÄ± kullanÄ±n.
- Uygulamaya gÃ¶re hatanÄ±n maliyetine gÃ¶re eÅŸik ayarÄ± yapÄ±lmalÄ±dÄ±r (Ã¶r. FN maliyeti yÃ¼ksekse Recall Ã¶nceliklenir).

> ![Ã–lÃ§Ã¼tler â€” gÃ¶rsel Ã¶zet 129](Images/129.jpg)

### ROC EÄŸrisi ve AUC

Accuracy neden her zaman yeterli deÄŸil? â€” Dengesiz veri Ã¶rneÄŸi
- Ã–rnek: 100 eâ€‘postanÄ±n yalnÄ±z 1â€™i spam olsun (%1). TÃ¼m eâ€‘postalara â€œspam deÄŸilâ€ diyen tembel bir modelin doÄŸruluÄŸu %99 gÃ¶rÃ¼nÃ¼r; fakat spam yakalama baÅŸarÄ±sÄ± %0â€™dÄ±r. Bu nedenle dengesiz veri setlerinde daha zengin metriklere (Recall/Precision, ROCâ€‘AUC, PRâ€‘AUC) ihtiyaÃ§ vardÄ±r.

ROC eksenleri ve tanÄ±mlar
- True Positive Rate (TPR, Recall): \(\displaystyle \frac{TP}{TP+FN}\). â€œPozitiflerin yÃ¼zde kaÃ§Ä±nÄ± yakaladÄ±k?â€
- False Positive Rate (FPR): \(\displaystyle \frac{FP}{FP+TN}\). â€œNegatiflerin yÃ¼zde kaÃ§Ä±na yanlÄ±ÅŸlÄ±kla pozitif dedik?â€ Not: FPR = 1 âˆ’ Ã–zgÃ¼llÃ¼k (Specificity), Ã–zgÃ¼llÃ¼k = \(\tfrac{TN}{TN+FP}\).

ROC (Receiver Operating Characteristic) eÄŸrisi nedir?
- TÃ¼m eÅŸik (threshold) deÄŸerleri iÃ§in model performansÄ±nÄ± gÃ¶sterir: Yâ€‘ekseni TPR, Xâ€‘ekseni FPRâ€™dir.
- EÅŸik 1.0â€™dan 0.0â€™a dÃ¼ÅŸÃ¼rÃ¼lÃ¼rken her noktada (FPR, TPR) hesaplanÄ±r; (0,0)â€™dan baÅŸlayÄ±p (1,1)â€™de biter. NoktalarÄ±n birleÅŸimi ROC eÄŸrisidir.

> ![ROC eÄŸrisi â€” nasÄ±l oluÅŸturulur 135](Images/135.jpg)

EÅŸik deÄŸiÅŸtikÃ§e TPR ve FPR deÄŸerleri deÄŸiÅŸir; ROC eÄŸrisi bu deÄŸiÅŸimi gÃ¶rselleÅŸtirir. EÄŸri altÄ±ndaki alan (AUC) 1â€™e yaklaÅŸtÄ±kÃ§a model ayrÄ±ÅŸtÄ±rma gÃ¼cÃ¼ artar.

```python
import numpy as np
from sklearn import metrics

y = np.array([1, 1, 0, 1])
scores = np.array([0.1, 0.4, 0.35, 0.8])  # Pozitif olasÄ±lÄ±k/skorlar
fpr, tpr, thresholds = metrics.roc_curve(y, scores)
auc_val = metrics.auc(fpr, tpr)
print({"auc": auc_val, "thresholds": thresholds.tolist()})
```

> ![ROC ve AUC â€” gÃ¶rsel 130](Images/130.jpg)

ROC alanÄ±nÄ±n yorumu (AUC)
- MÃ¼kemmel model solâ€‘Ã¼st kÃ¶ÅŸeye ((0,1)) yakÄ±n seyreder: TPR=1, FPR=0.
- Rastgele model diyagonal (y=x) Ã¼zerindedir: ayÄ±rt etme gÃ¼cÃ¼ yoktur.
- EÄŸri diyagonalin altÄ±na dÃ¼ÅŸerse rastgeleden kÃ¶tÃ¼dÃ¼r (etiketleri ters Ã§evirmek fayda saÄŸlayabilir).

> ![ROC eÄŸrisi â€” yorum ve AUC 136](Images/136.jpg)

Pratik: EÅŸik seÃ§imi (threshold tuning)
- Youdenâ€™s J istatistiÄŸi: \( J = \max (\,TPR - FPR\,) \). ROCâ€™ta diyagonale en uzak (dikey) noktayÄ± bulur; dengeli bir seÃ§imdir.
- (0,1) noktasÄ±na en yakÄ±n: \( d = \min \sqrt{(1-TPR)^2 + (FPR)^2} \). Hem yÃ¼ksek TPR hem dÃ¼ÅŸÃ¼k FPR isteyen uygulamalar iÃ§in uygundur.

KÄ±sa Python Ã¶rneÄŸi â€” en iyi eÅŸiÄŸi seÃ§mek:

```python
import numpy as np
from sklearn import metrics

# y: gerÃ§ek etiketler (0/1), scores: pozitif sÄ±nÄ±f skoru/olasÄ±lÄ±ÄŸÄ±
fpr, tpr, thresholds = metrics.roc_curve(y, scores)
auc_val = metrics.auc(fpr, tpr)

# Youden J (TPR - FPR) maksimize
j_vals = tpr - fpr
thr_j = thresholds[np.argmax(j_vals)]

# (0,1)'e en yakÄ±n nokta minimize
dist = np.sqrt((1 - tpr)**2 + (fpr)**2)
thr_01 = thresholds[np.argmin(dist)]

print({"auc": auc_val, "thr_j": float(thr_j), "thr_01": float(thr_01)})
```

Notlar:
- EÅŸik ayarÄ± iÅŸ hedeflerine gÃ¶re yapÄ±lmalÄ±; Precisionâ€‘Recall eÄŸrileri dengesiz sÄ±nÄ±flarda daha bilgilendirici olabilir.
- Ã‡ok sÄ±nÄ±flÄ± senaryoda macro/micro/weighted ortalamalarla metrikleri raporlayÄ±n; scikitâ€‘learn `classification_report` kullanÄ±ÅŸlÄ±dÄ±r.

## UzaklÄ±k Ã–lÃ§Ã¼leri

- BirÃ§ok sÄ±nÄ±flandÄ±rma ve kÃ¼meleme yÃ¶nteminde (Ã¶r. k-NN, k-means) Ã¶rnekler arasÄ± benzerlik/ayrÄ±ÅŸma, bir uzaklÄ±k Ã¶lÃ§Ã¼sÃ¼ ile deÄŸerlendirilir.
- GÃ¶sterimler:
    - p: deÄŸiÅŸken (Ã¶zellik) sayÄ±sÄ±
    - d_{ij}: iâ€™nci ve jâ€™inci nesneler arasÄ±ndaki uzaklÄ±k
    - x_{ik}: iâ€™nci nesnenin kâ€™Ä±ncÄ± deÄŸiÅŸkendeki deÄŸeri
    - x_{jk}: jâ€™inci nesnenin kâ€™Ä±ncÄ± deÄŸiÅŸkendeki deÄŸeri

> ![UzaklÄ±k Ã¶lÃ§Ã¼leri â€” gÃ¶rsel 54](Images/54.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 54 â€” uzaklÄ±k terimleri ve notasyonun kÄ±sa Ã¶zeti.

### Ã–klid UzaklÄ±ÄŸÄ± (Euclidean Distance)

- TanÄ±m:

$$
d_{ij}^{\text{Ã–klid}} = \sqrt{\sum_{k=1}^{p} (x_{ik} - x_{jk})^{2}}
$$

- Ã–zellikler: Ã–lÃ§ek duyarlÄ±dÄ±r; bÃ¼yÃ¼k farklarÄ± vurgular; metrik Ã¶zellikleri (pozitiflik, simetri, Ã¼Ã§gen eÅŸitsizliÄŸi) saÄŸlar.

> ![Ã–klid uzaklÄ±ÄŸÄ± â€” gÃ¶rsel 55](Images/55.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 55 â€” Ã–klid uzaklÄ±ÄŸÄ± formÃ¼lÃ¼ ve Ã¶rnek gÃ¶sterimi.

### Karesel Ã–klid UzaklÄ±ÄŸÄ± (Squared Euclidean)

- TanÄ±m (karekÃ¶k alÄ±nmadan):

$$
d_{ij}^{2} = \sum_{k=1}^{p} (x_{ik} - x_{jk})^{2}
$$

- Notlar:
    - SÄ±ralama aÃ§Ä±sÄ±ndan Ã§oÄŸu durumda Ã–klid ile aynÄ± karÅŸÄ±laÅŸtÄ±rmayÄ± verir (monoton iliÅŸki: sqrt).
    - KarekÃ¶k olmadÄ±ÄŸÄ± iÃ§in hesaplama daha hÄ±zlÄ±dÄ±r; optimizasyonda sÄ±k tercih edilir.
    - BÃ¼yÃ¼k sapmalarÄ± daha da fazla cezalandÄ±rÄ±r (kare terim).

> ![Karesel Ã–klid uzaklÄ±ÄŸÄ± â€” gÃ¶rsel 56](Images/56.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 56 â€” karesel Ã–klid uzaklÄ±ÄŸÄ± formÃ¼lÃ¼ ve kullanÄ±m notlarÄ±.

#### Ã–rnek: SÄ±caklÄ±k ve Nem OranÄ± ile Ã–klid UzaklÄ±ÄŸÄ±

AÅŸaÄŸÄ±daki tabloda Ã¼Ã§ gÃ¼n iÃ§in iki Ã¶zellik (SÄ±caklÄ±k, Nem OranÄ±) verilmiÅŸtir. Bu noktalara gÃ¶re Ã–klid uzaklÄ±klarÄ±nÄ± hesaplayalÄ±m.

| GÃ¼n | SÄ±caklÄ±k | Nem OranÄ± |
|-----|----------|-----------|
| 1   | 25       | 80        |
| 2   | 22       | 50        |
| 3   | 18       | 80        |

UzaklÄ±k hesaplarÄ± (p = 2):

- \( d_{12} = \sqrt{(25-22)^2 + (80-50)^2} = \sqrt{9 + 900} = \sqrt{909} \approx 30.14 \)
- \( d_{13} = \sqrt{(25-18)^2 + (80-80)^2} = \sqrt{49 + 0} = 7 \)
- \( d_{23} = \sqrt{(22-18)^2 + (50-80)^2} = \sqrt{4 + 900} = \sqrt{904} \approx 30.06 \)

GÃ¶zlem: GÃ¼n 1 ile GÃ¼n 3 arasÄ±ndaki uzaklÄ±k 7 ile en kÃ¼Ã§Ã¼ktÃ¼r; bu iki gÃ¼n (bu iki Ã¶zellik uzayÄ±nda) birbirine daha benzerdir.

### Karl Pearson UzaklÄ±ÄŸÄ± (StandartlaÅŸtÄ±rÄ±lmÄ±ÅŸ Ã–klid)

- Ã–klid uzaklÄ±ÄŸÄ±nÄ±n deÄŸiÅŸken varyansÄ±na oranlanmasÄ±yla elde edilir; farklÄ± Ã¶lÃ§Ã¼ birimlerine sahip deÄŸiÅŸkenler iÃ§in tercih edilir.
- FormÃ¼l (varyanslarla standardizasyon):

$$
d_{ij}^{\text{KP}} = \sqrt{\sum_{k=1}^{p} \frac{\bigl(x_{ik} - x_{jk}\bigr)^{2}}{s_k^{2}} }
$$

    - Burada s_k^2, kâ€™Ä±ncÄ± deÄŸiÅŸkenin varyans deÄŸeridir; pratikte bazen standart sapma (s_k) ile Ã¶lÃ§eklenmiÅŸ farklar kullanÄ±lÄ±r: \(\Delta_k / s_k\).
    - AmaÃ§: Her Ã¶zelliÄŸin Ã¶lÃ§Ã¼ birimi/Ã¶lÃ§eÄŸi farklÄ± olsa bile, katkÄ±sÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±labilir hÃ¢le getirmek (Ã¶lÃ§ek duyarlÄ±lÄ±ÄŸÄ±nÄ± azaltmak).
    - UyarÄ±: s_k^2 â‰ˆ 0 ise bÃ¶lme hatasÄ±na yol aÃ§mamak iÃ§in sabit/sÄ±fÄ±ra yakÄ±n varyanslÄ± Ã¶zellikleri elemek veya kÃ¼Ã§Ã¼k bir Îµ ile dÃ¼zenlemek gerekir.

> ![Karl Pearson uzaklÄ±ÄŸÄ± â€” gÃ¶rsel 57](Images/57.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 57 â€” standartlaÅŸtÄ±rÄ±lmÄ±ÅŸ (varyansla Ã¶lÃ§eklenen) Ã–klid uzaklÄ±ÄŸÄ± notasyonu; farklÄ± birim/Ã¶lÃ§ekli deÄŸiÅŸkenlerde tercih edilir.

#### Ã–rnek: SÄ±caklÄ±k ve Nem OranÄ± ile Karl Pearson UzaklÄ±ÄŸÄ±

Veri tablosu (iki Ã¶zellik: SÄ±caklÄ±k, Nem OranÄ±):

| GÃ¼n | SÄ±caklÄ±k | Nem OranÄ± |
|-----|----------|-----------|
| 1   | 25       | 80        |
| 2   | 22       | 50        |
| 3   | 18       | 80        |

Verilen standart sapmalar (Ïƒ): SÄ±caklÄ±k Ïƒ = 8.22, Nem OranÄ± Ïƒ = 200.

Bu derste kullanÄ±lan konvansiyona gÃ¶re (sunum notasyonu):

$$
d_{ij}^{\text{KP}} = \sqrt{ \frac{(\Delta\text{SÄ±caklÄ±k})^{2}}{(8.22)^{2}} + \frac{(\Delta\text{Nem})^{2}}{(200)^{2}} }
$$

Hesaplar:

- \( d_{12} = \sqrt{ \tfrac{(25-22)^2}{(8.22)^2} + \tfrac{(80-50)^2}{(200)^2} } = \sqrt{0.133 + 0.0225} \approx 0.39 \)
- \( d_{13} = \sqrt{ \tfrac{(25-18)^2}{(8.22)^2} + \tfrac{(80-80)^2}{(200)^2} } = \sqrt{0.725 + 0} \approx 0.85 \)
- \( d_{23} = \sqrt{ \tfrac{(22-18)^2}{(8.22)^2} + \tfrac{(50-80)^2}{(200)^2} } = \sqrt{0.237 + 0.0225} \approx 0.50 \)

Not â€” yaygÄ±n alternatif tanÄ±m: BirÃ§ok kaynakta standartlaÅŸtÄ±rÄ±lmÄ±ÅŸ Ã–klid, \( d^2 = \sum_k \tfrac{(x_{ik} - x_{jk})^2}{\operatorname{Var}(x_k)} \) olarak verilir (paydada varyans, karekÃ¶k dÄ±ÅŸarÄ±da). Bu tanÄ±mda sayÄ±sal bÃ¼yÃ¼klÃ¼k farklÄ± olur fakat karÅŸÄ±laÅŸtÄ±rmalÄ± sÄ±ralama Ã§oÄŸu durumda deÄŸiÅŸmez.

### Manhattan (City-Block) UzaklÄ±ÄŸÄ±

- TanÄ±m: Birimler arasÄ±ndaki farklarÄ±n mutlak deÄŸerlerinin toplamÄ±dÄ±r (L1 uzaklÄ±ÄŸÄ±).

$$
d_{ij}^{\text{Manhattan}} = \sum_{k=1}^{p} \bigl| x_{ik} - x_{jk} \bigr|
$$

- Ne zaman tercih edilir?
    - DeÄŸiÅŸkenler arasÄ±nda iliÅŸki (korelasyon) yoksa veya zayÄ±fsa,
    - AykÄ±rÄ± deÄŸerlere (outlier) Ã–klid'e gÃ¶re daha dayanÄ±klÄ± bir Ã¶lÃ§Ã¼ isteniyorsa,
    - Åehir bloklarÄ±/Ä±zgara hareket maliyetleri gibi L1 uygun maliyetler sÃ¶z konusuysa.

> ![Manhattan uzaklÄ±ÄŸÄ± â€” gÃ¶rsel 58](Images/58.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 58 â€” Manhattan (city-block) uzaklÄ±ÄŸÄ±nÄ±n tanÄ±mÄ± ve gÃ¶rsel sezgisi.

#### Ã–rnek: SÄ±caklÄ±k ve Nem OranÄ± ile Manhattan UzaklÄ±ÄŸÄ±

AynÄ± tablo (SÄ±caklÄ±k, Nem OranÄ±):

| GÃ¼n | SÄ±caklÄ±k | Nem OranÄ± |
|-----|----------|-----------|
| 1   | 25       | 80        |
| 2   | 22       | 50        |
| 3   | 18       | 80        |

Hesaplar (p = 2):

- \( d_{12} = |25-22| + |80-50| = 3 + 30 = 33 \)
- \( d_{13} = |25-18| + |80-80| = 7 + 0 = 7 \)
- \( d_{23} = |22-18| + |50-80| = 4 + 30 = 34 \)

GÃ¶zlem: Ã–klid Ã¶rneÄŸinde olduÄŸu gibi, GÃ¼n 1 ve GÃ¼n 3 birbirine en yakÄ±ndÄ±r (d = 7).

### Minkowski (Lp) UzaklÄ±ÄŸÄ±

- Genel tanÄ±m: Ã–klid (L2) ve Manhattan (L1) uzaklÄ±klarÄ±nÄ±n genellemesidir.

$$
d_{ij}^{(p)} = \Biggl( \sum_{k=1}^{d} \bigl| x_{ik} - x_{jk} \bigr|^{\,p} \Biggr)^{\!1/p}
$$

    - Burada d Ã¶zellik (boyut) sayÄ±sÄ±, p ise norm derecesidir.
    - Ã–zel durumlar:
        - p = 1 â†’ Manhattan (L1)
        - p = 2 â†’ Ã–klid (L2)
        - p â†’ âˆ â†’ Chebyshev: \( d_{ij}^{(\infty)} = \max_k |x_{ik} - x_{jk}| \)

- Pratik ipuÃ§larÄ±:
    - p arttÄ±kÃ§a bÃ¼yÃ¼k farklar daha da baskÄ±n hÃ¢le gelir (outlier etkisi artar).
    - d yÃ¼ksek olduÄŸunda tÃ¼m Lp uzaklÄ±klarÄ± birbirine daha Ã§ok benzemeye meyleder; uygun Ã¶lÃ§ekleme/Ã¶zellik seÃ§imi kritik olur.

> ![Minkowski uzaklÄ±ÄŸÄ± â€” gÃ¶rsel 59](Images/59.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 59 â€” Minkowski (Lp) uzaklÄ±ÄŸÄ± ve p/Î» parametresi; Î» deÄŸiÅŸtikÃ§e farklÄ± metrikler elde edilir.

- Notasyon:
    - BazÄ± kaynaklarda p yerine Î» kullanÄ±lÄ±r (bu notlarda p â‰¡ Î» kabul edilir).
- SonuÃ§lar (Î» seÃ§imine gÃ¶re):
    - Î» = 1 â‡’ Manhattan UzaklÄ±ÄŸÄ± (L1)
    - Î» = 2 â‡’ Ã–klid UzaklÄ±ÄŸÄ± (L2)

### Ã–zellik Ã–lÃ§ekleme (Feature Scaling) â€” Mesafe TabanlÄ± YÃ¶ntemler

Mesafe Ã¶lÃ§Ã¼lerinde farklÄ± Ã¶lÃ§eklerdeki Ã¶zellikler sonucu domine eder. Bu nedenle k-NN, k-means gibi yÃ¶ntemlerde Ã¶lÃ§ekleme ÅŸarttÄ±r.

- Neden?
    - SÄ±caklÄ±k (Â°C) ve Nem (%) gibi farklÄ± birimlerde/Ã¶lÃ§eklerde Ã¶zellikler, ham Ã–klid/Manhattan ile karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda bÃ¼yÃ¼k Ã¶lÃ§ekli olanÄ± Ã¶ne Ã§Ä±karÄ±r.
    - Standardize Ã–klid (Karl Pearson) bu problemi deÄŸiÅŸken varyansÄ±na gÃ¶re aÄŸÄ±rlÄ±klandÄ±rarak azaltÄ±r.

- YÃ¶ntemler:
    - Standardizasyon (z-skor): \( z = \tfrac{x - \mu}{\sigma} \)
        - Ortalama 0, standart sapma 1 olacak ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r; birim etkisini kaldÄ±rÄ±r.
        - Outlierâ€™lar varsa robuslaÅŸtÄ±rÄ±lmÄ±ÅŸ seÃ§enekler: Medyan ve IQR ile Ã¶lÃ§ekleme.
    - Minâ€“Max Ã¶lÃ§ekleme: \( x' = \tfrac{x - x_{\min}}{x_{\max} - x_{\min}} \in [0,1] \)
        - SÄ±nÄ±rlarÄ± sabitler; aykÄ±rÄ± deÄŸerlere duyarlÄ± olabilir.
    - VektÃ¶r normlama (birim-norm): \( x' = \tfrac{x}{\lVert x \rVert_2} \) vb.
        - Ã–zellikle kosinÃ¼s benzerliÄŸi/uzaklÄ±ÄŸÄ± veya metin vektÃ¶rlerinde faydalÄ±dÄ±r.

- Uygulama notlarÄ±:
    - EÄŸitimâ€“doÄŸrulamaâ€“test sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nlemek iÃ§in Ã¶lÃ§ekleme parametrelerini (\(\mu,\sigma, x_{\min}, x_{\max}\)) yalnÄ±zca eÄŸitim verisinden Ã¶ÄŸrenin ve diÄŸer bÃ¶lÃ¼mlere uygulayÄ±n.
    - k-means/k-NN + Ã–klid kullanÄ±rken standardizasyon Ã§oÄŸunlukla varsayÄ±lan iyi baÅŸlangÄ±Ã§tÄ±r.


#### NormalleÅŸtirme (Minâ€“Max)

- TanÄ±m: SayÄ±sal deÄŸerleri belirli ve kÃ¼Ã§Ã¼k bir aralÄ±ÄŸa (genellikle [0,1]) yerleÅŸtirmek iÃ§in yapÄ±lan Ã¶lÃ§eklendirme iÅŸlemidir.
- En BÃ¼yÃ¼kâ€“En KÃ¼Ã§Ã¼k (Minâ€“Max) NormalleÅŸtirme: Orijinal veri Ã¼zerinde doÄŸrusal bir dÃ¶nÃ¼ÅŸÃ¼m yapar; veri iÃ§indeki en kÃ¼Ã§Ã¼k ve en bÃ¼yÃ¼k deÄŸerler referans alÄ±nÄ±r.

$$
x_i' = \frac{x_i - \min(x)}{\max(x) - \min(x)} \in [0,1]
$$

- Terimler:
    - \(x_i'\): DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ deÄŸer
    - \(x_i\): GÃ¶zlem deÄŸeri
    - \(\max(x)\): Verideki en bÃ¼yÃ¼k x deÄŸeri
    - \(\min(x)\): Verideki en kÃ¼Ã§Ã¼k x deÄŸeri

Not: â€œStandartlaÅŸtÄ±rmaâ€ (z-skor) farklÄ± bir dÃ¶nÃ¼ÅŸÃ¼mdÃ¼r ve \( z = \tfrac{x - \mu}{\sigma} \) ile yapÄ±lÄ±r (yukarÄ±daki YÃ¶ntemler bÃ¶lÃ¼mÃ¼ne bakÄ±nÄ±z).


##### Ã–rnek: Minâ€“Max NormalleÅŸtirme (min=2, max=90)

AÅŸaÄŸÄ±daki X deÄŸerleri, min=2 ve max=90 kullanÄ±larak \( x' = \tfrac{x - 2}{90 - 2} \) formÃ¼lÃ¼yle [0,1] aralÄ±ÄŸÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r (sonuÃ§lar 4 ondalÄ±ÄŸa yuvarlanmÄ±ÅŸtÄ±r):

| X  | min(x) | max(x) | Ä°ÅŸlem                 | SonuÃ§   |
|----|--------|--------|-----------------------|---------|
| 15 | 2      | 90     | (15 âˆ’ 2) / (90 âˆ’ 2)  | 0.1477  |
| 5  | 2      | 90     | (5 âˆ’ 2) / (90 âˆ’ 2)   | 0.0341  |
| 22 | 2      | 90     | (22 âˆ’ 2) / (90 âˆ’ 2)  | 0.2273  |
| 59 | 2      | 90     | (59 âˆ’ 2) / (90 âˆ’ 2)  | 0.6477  |
| 90 | 2      | 90     | (90 âˆ’ 2) / (90 âˆ’ 2)  | 1.0000  |
| 30 | 2      | 90     | (30 âˆ’ 2) / (90 âˆ’ 2)  | 0.3182  |
| 2  | 2      | 90     | (2 âˆ’ 2) / (90 âˆ’ 2)   | 0.0000  |
| 80 | 2      | 90     | (80 âˆ’ 2) / (90 âˆ’ 2)  | 0.8864  |
| 20 | 2      | 90     | (20 âˆ’ 2) / (90 âˆ’ 2)  | 0.2045  |


## K-en YakÄ±n KomÅŸu (K-NN) AlgoritmasÄ±

- K-en yakÄ±n komÅŸu algoritmasÄ± (K-nearest neighbor, K-NN) mesafeye dayalÄ± bir sÄ±nÄ±flandÄ±rma yÃ¶ntemidir.
- AmaÃ§: SÄ±nÄ±flarÄ± bilinen (etiketli) bir Ã¶rnek kÃ¼mesinden yararlanarak, yeni gelen bir gÃ¶zlemin hangi sÄ±nÄ±fa ait olduÄŸunu belirlemek.
- Esas: Yeni gÃ¶zlem ile eÄŸitim kÃ¼mesindeki her bir gÃ¶zlem arasÄ±ndaki uzaklÄ±klar hesaplanÄ±r; en kÃ¼Ã§Ã¼k uzaklÄ±ÄŸa sahip k adet komÅŸu seÃ§ilir ve Ã§oÄŸunluk oyu ile sÄ±nÄ±f atanÄ±r.
- UzaklÄ±k Ã¶lÃ§Ã¼tleri: Ã–klid, Standardize Ã–klid (Karl Pearson), Manhattan vb. (bkz. UzaklÄ±k Ã–lÃ§Ã¼leri).

### AdÄ±mlar

a) K parametresi belirlenir (verilen noktaya bakÄ±lacak komÅŸu sayÄ±sÄ±).

b) Yeni gÃ¶zlem ile eÄŸitim kÃ¼mesindeki tÃ¼m noktalar arasÄ±ndaki uzaklÄ±klar tek tek hesaplanÄ±r.

c) UzaklÄ±klara gÃ¶re noktalar sÄ±ralanÄ±r ve en kÃ¼Ã§Ã¼k olan k tanesi seÃ§ilir.

d) Bu k komÅŸunun sÄ±nÄ±f etiketleri sayÄ±lÄ±r; en Ã§ok tekrarlanan sÄ±nÄ±f belirlenir (Ã§oÄŸunluk oyu).

e) SeÃ§ilen sÄ±nÄ±f, yeni gÃ¶zlemin tahmini kategorisi olarak kabul edilir.

### Pratik Notlar

- K seÃ§imi:
    - Ã‡ok kÃ¼Ã§Ã¼k K (Ã¶r. 1) â†’ gÃ¼rÃ¼ltÃ¼ye duyarlÄ±; Ã§ok bÃ¼yÃ¼k K â†’ karar sÄ±nÄ±rlarÄ± aÅŸÄ±rÄ± dÃ¼zleÅŸir (yÃ¼ksek bias).
    - Ä°kili sÄ±nÄ±flandÄ±rmada baÄŸ durumlarÄ±nÄ± azaltmak iÃ§in K'nin tek sayÄ± seÃ§ilmesi pratik bir tercihtir.
- BaÄŸ (tie) durumlarÄ±: EÅŸit oy Ã§Ä±karsa en yakÄ±n komÅŸunun sÄ±nÄ±fÄ± veya mesafe-aÄŸÄ±rlÄ±klÄ± oylama kullanÄ±labilir.
- AÄŸÄ±rlÄ±klÄ± K-NN (opsiyonel): Daha yakÄ±n komÅŸulara daha yÃ¼ksek aÄŸÄ±rlÄ±k verilir, Ã¶r. aÄŸÄ±rlÄ±k = 1/(d+Îµ) veya 1/d^2.
- Ã–lÃ§ekleme Ã¶nemli: Mesafe tabanlÄ± olduÄŸu iÃ§in Ã¶zellik Ã¶lÃ§eklemesi (standardizasyon) genellikle gereklidir (bkz. Ã–zellik Ã–lÃ§ekleme).
- KarmaÅŸÄ±klÄ±k: Naif uygulamada her tahmin iÃ§in O(nÂ·d) mesafe hesabÄ± gerekir; bÃ¼yÃ¼k n iÃ§in indeksleme/yakÄ±n komÅŸu yapÄ±larÄ± dÃ¼ÅŸÃ¼nÃ¼lebilir.


### Ã–rnek 1: (8,4) Ä°Ã§in K-NN SÄ±nÄ±flandÄ±rma (k = 4)

Verilen: X1 ve X2 nitelikleri ile Y sÄ±nÄ±fÄ±ndan oluÅŸan bir gÃ¶zlem tablosu; yeni gÃ¶zlem (8,4).

> ![Ã–rnek gÃ¶zlem tablosu â€” gÃ¶rsel 60](Images/60.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 60 â€” X1, X2 ve Y (sÄ±nÄ±f) deÄŸerlerini iÃ§eren gÃ¶zlem tablosu.

a) Kâ€™nÄ±n belirlenmesi: k = 4 kabul edilir.

b) UzaklÄ±klarÄ±n hesaplanmasÄ±: (8,4) ile her bir gÃ¶zlem arasÄ±ndaki uzaklÄ±klar Ã–klid uzaklÄ±ÄŸÄ±na gÃ¶re hesaplanÄ±r.

> ![Ã–klid uzaklÄ±ÄŸÄ± â€” gÃ¶rsel 61](Images/61.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 61 â€” Ã–klid uzaklÄ±ÄŸÄ± formÃ¼lÃ¼ ve hesap adÄ±mÄ±.

Ã–rnek hesap (birinci gÃ¶zlem (2,4) iÃ§in):

$$
d\bigl((8,4),(2,4)\bigr) = \sqrt{(8-2)^2 + (4-4)^2} = 6.
$$

Benzer ÅŸekilde tÃ¼m gÃ¶zlemler iÃ§in uzaklÄ±klar hesaplanÄ±r ve aÅŸaÄŸÄ±daki tablo elde edilir:

> ![UzaklÄ±klar tablosu â€” gÃ¶rsel 62](Images/62.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 62 â€” (8,4) noktasÄ±nÄ±n tÃ¼m gÃ¶zlemlere olan uzaklÄ±klarÄ±.

c) En kÃ¼Ã§Ã¼k uzaklÄ±klarÄ±n belirlenmesi: SatÄ±rlar uzaklÄ±ÄŸa gÃ¶re sÄ±ralanÄ±r, en kÃ¼Ã§Ã¼k k = 4 tanesi seÃ§ilir (en yakÄ±n 4 komÅŸu).

> ![En yakÄ±n 4 komÅŸu â€” gÃ¶rsel 63](Images/63.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 63 â€” UzaklÄ±ÄŸa gÃ¶re sÄ±ralama ve en kÃ¼Ã§Ã¼k 4 uzaklÄ±ÄŸÄ±n seÃ§imi.

d) SeÃ§ilen komÅŸularÄ±n sÄ±nÄ±flarÄ±: Bu dÃ¶rt komÅŸunun Y sÄ±nÄ±flarÄ± incelenir; 1 adet Ä°YÄ° ve 3 adet KÃ–TÃœ vardÄ±r.

> ![KomÅŸu sÄ±nÄ±flarÄ± â€” gÃ¶rsel 64](Images/64.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 64 â€” SeÃ§ilen komÅŸularÄ±n sÄ±nÄ±f daÄŸÄ±lÄ±mÄ± ve baskÄ±n sÄ±nÄ±f.

e) SonuÃ§: KÃ–TÃœ sÄ±nÄ±fÄ±, Ä°YÄ° sÄ±nÄ±fÄ±ndan daha fazla olduÄŸu iÃ§in (8,4) noktasÄ±nÄ±n sÄ±nÄ±fÄ± KÃ–TÃœ olarak belirlenir.


### Ã–rnek 2: (7,8,5) Ä°Ã§in K-NN â€” Minâ€“Max Normalizasyon ile (k = 3)

Verilen: Y sÄ±nÄ±f niteliÄŸini iÃ§eren bir gÃ¶zlem tablosu. Yeni gÃ¶zlem noktasÄ±: (7, 8, 5).

> ![Ã–rnek 2 â€” ham gÃ¶zlem tablosu â€” gÃ¶rsel 65](Images/65.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 65 â€” X Ã¶zellikleri ve Y sÄ±nÄ±fÄ±nÄ± iÃ§eren ham veri tablosu.

Ã–nce tÃ¼m Ã¶zellikler minâ€“max ile (0,1) aralÄ±ÄŸÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r. Aday noktanÄ±n normalizasyon sonucu: (0.27, 0.43, 0.07).

> ![Ã–rnek 2 â€” minâ€“max sonrasÄ± tablo â€” gÃ¶rsel 66](Images/66.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 66 â€” Minâ€“max normalleÅŸtirmesi sonrasÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ deÄŸerler.

a) Kâ€™nÄ±n belirlenmesi: k = 3.

b) UzaklÄ±klarÄ±n hesaplanmasÄ±: Aday nokta (0.27, 0.43, 0.07) ile her bir normalleÅŸtirilmiÅŸ gÃ¶zlem arasÄ±ndaki Ã–klid uzaklÄ±klarÄ± hesaplanÄ±r.

> ![Ã–rnek 2 â€” uzaklÄ±k hesaplarÄ± â€” gÃ¶rsel 67](Images/67.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 67 â€” NormalleÅŸtirilmiÅŸ uzayda Ã–klid uzaklÄ±ÄŸÄ± hesaplarÄ±.

c) En kÃ¼Ã§Ã¼k uzaklÄ±klarÄ±n belirlenmesi: SatÄ±rlar uzaklÄ±ÄŸa gÃ¶re sÄ±ralanÄ±r, en kÃ¼Ã§Ã¼k k = 3 tanesi seÃ§ilir.

> ![Ã–rnek 2 â€” en yakÄ±n 3 komÅŸu â€” gÃ¶rsel 68](Images/68.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 68 â€” UzaklÄ±ÄŸa gÃ¶re sÄ±ralama ve en kÃ¼Ã§Ã¼k 3 uzaklÄ±ÄŸÄ±n seÃ§imi.

d) KomÅŸularÄ±n sÄ±nÄ±flarÄ±: SeÃ§ilen 3 komÅŸunun Y sÄ±nÄ±flarÄ± incelenir; 1 adet HAYIR ve 2 adet EVET vardÄ±r.

> ![Ã–rnek 2 â€” komÅŸu sÄ±nÄ±flarÄ± â€” gÃ¶rsel 69](Images/69.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 69 â€” SeÃ§ilen komÅŸularÄ±n sÄ±nÄ±f daÄŸÄ±lÄ±mÄ± ve baskÄ±n sÄ±nÄ±f.

e) SonuÃ§: EVET sayÄ±sÄ± HAYIR sayÄ±sÄ±ndan fazla olduÄŸundan (7,8,5) gÃ¶zleminin sÄ±nÄ±fÄ± EVET olarak kabul edilir.


### AÄŸÄ±rlÄ±klÄ± Oylama (Weighted Voting)

Basit Ã§oÄŸunluk oylamasÄ±nda her komÅŸu aynÄ± aÄŸÄ±rlÄ±ÄŸa sahiptir. Ancak bazÄ± durumlarda, daha yakÄ±n komÅŸularÄ±n daha etkili olmasÄ± istenir. Bu amaÃ§la aÄŸÄ±rlÄ±klÄ± oylama kullanÄ±lÄ±r.

- Fikir: KomÅŸu katkÄ±sÄ± mesafe ile ters orantÄ±lÄ±dÄ±r; daha yakÄ±n komÅŸu daha bÃ¼yÃ¼k aÄŸÄ±rlÄ±k alÄ±r.
- SÄ±k kullanÄ±lan aÄŸÄ±rlÄ±klar: \( w_j = \tfrac{1}{d(i,j)+\varepsilon} \) veya \( w_j = \tfrac{1}{d(i,j)^2+\varepsilon} \)
- SÄ±nÄ±f skoru: \( S_c = \sum_{j \in \mathcal{N}_k,\; y_j=c} w_j \). Tahmin edilen sÄ±nÄ±f: \( \arg\max_c S_c \).

> ![AÄŸÄ±rlÄ±klÄ± oylama formÃ¼lÃ¼ â€” gÃ¶rsel 70](Images/70.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 70 â€” d(i,j) yeni nokta i ile komÅŸu j arasÄ±ndaki Ã–klid uzaklÄ±ÄŸÄ±dÄ±r; her sÄ±nÄ±f iÃ§in aÄŸÄ±rlÄ±klandÄ±rÄ±lmÄ±ÅŸ katkÄ±lar toplanÄ±r.

Ã–rnek 2 iÃ§in aÄŸÄ±rlÄ±klÄ± oylama:

- EVET (Toplam) = 2,52 + 1,32 = 3,84
- HAYIR = 5,17

> ![AÄŸÄ±rlÄ±klÄ± oylama â€” sonuÃ§ â€” gÃ¶rsel 71](Images/71.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 71 â€” AÄŸÄ±rlÄ±klÄ± oylama sÄ±nÄ±f skorlarÄ±; en bÃ¼yÃ¼k skor HAYIR.

SonuÃ§: AÄŸÄ±rlÄ±klÄ± oylama ile (7,8,5) gÃ¶zleminin sÄ±nÄ±fÄ± HAYIR olur. Bu, aÄŸÄ±rlÄ±klandÄ±rmanÄ±n Ã§oÄŸunluk oyundan farklÄ± bir sonuÃ§ verebileceÄŸini gÃ¶sterir (daha yakÄ±n HAYIR komÅŸularÄ±nÄ±n etkisi daha bÃ¼yÃ¼ktÃ¼r).

> ![AÄŸÄ±rlÄ±klÄ± oylama â€” uygulama â€” gÃ¶rsel 72](Images/72.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 72 â€” AÄŸÄ±rlÄ±klÄ± oylamanÄ±n uygulama akÄ±ÅŸÄ±/Ã¶rnek adÄ±mlarÄ±.

## Destek VektÃ¶r Makineleri (Support Vector Machines â€” SVM)

SVM, denetimli Ã¶ÄŸrenmede Ã¶zellikle sÄ±nÄ±flandÄ±rmada yaygÄ±n olup, uygun Ã§ekirdek (kernel) fonksiyonlarÄ±yla doÄŸrusal ve doÄŸrusal olmayan karar sÄ±nÄ±rlarÄ±nÄ± Ã¶ÄŸrenebilir. Veri tamamen ayrÄ±labiliyorsa tek bir hiperâ€‘dÃ¼zlemle ayrÄ±m yapÄ±lÄ±r; ayrÄ±lmÄ±yorsa yumuÅŸak kenar (soft margin) ve/veya Ã§ekirdek hilesi (kernel trick) kullanÄ±lÄ±r.

> ![SVM â€” temel fikir 137](Images/137.jpg)

### Karar DÃ¼zlemi, Marjin ve Destek VektÃ¶rleri

- Ä°kili sÄ±nÄ±flandÄ±rmada etiketler genellikle y âˆˆ {âˆ’1, +1} alÄ±nÄ±r. DoÄŸrusal SVMâ€™de karar hiperâ€‘dÃ¼zlemi:

$$
f(\mathbf{x}) = \mathbf{w}^{\top}\mathbf{x} + b = 0.
$$

- SÄ±nÄ±r dÃ¼zlemleri: \( \mathbf{w}^{\top}\mathbf{x} + b = +1 \) ve \( \mathbf{w}^{\top}\mathbf{x} + b = -1 \). Bu iki sÄ±nÄ±r arasÄ±ndaki bant â€œmarjinâ€dir ve geniÅŸliÄŸi \( 2/\lVert\mathbf{w}\rVert_2 \) olur. Marjini maksimize etmek genel ayÄ±rt ediciliÄŸi artÄ±rÄ±r.
- Destek vektÃ¶rleri, marjine en yakÄ±n (veya Ã¼zerinde) bulunan eÄŸitim Ã¶rnekleridir; Ã§Ã¶zÃ¼m yalnÄ±zca bu noktalara dayanÄ±r.

> ![SVM â€” karar doÄŸrusu ve marjin 138](Images/138.jpg)

### Sert ve YumuÅŸak Kenar FormÃ¼lasyonu

- Sert kenar (tam ayrÄ±labilir):

$$
\min_{\mathbf{w}, b} \ \tfrac{1}{2}\,\lVert\mathbf{w}\rVert^2 \quad \text{bÃ¶yle ki}\quad y_i\big(\mathbf{w}^{\top}\mathbf{x}_i + b\big) \ge 1,\ \forall i.
$$

- YumuÅŸak kenar (ayrÄ±lamayan veri; esneme deÄŸiÅŸkenleri \(\xi_i\)):

$$
\min_{\mathbf{w}, b,\,\boldsymbol{\xi}} \ \tfrac{1}{2}\,\lVert\mathbf{w}\rVert^2 + C\sum_i \xi_i \quad \text{bÃ¶yle ki}\quad y_i\big(\mathbf{w}^{\top}\mathbf{x}_i + b\big) \ge 1 - \xi_i,\ \xi_i \ge 0.
$$

- EÅŸdeÄŸer kayÄ±p perspektifi (hinge loss): \( L_{\text{hinge}}(y, f(x)) = \max\{0, 1 - y\,f(x)\} \); amaÃ§, dÃ¼zenlileÅŸtirme ile birlikte ortalama hinge kaybÄ±nÄ± minimize etmektir.

> ![SVM â€” destek vektÃ¶rleri ve marjin 139](Images/139.jpg)

### Kernel Hilesi (Kernel Trick)

DoÄŸrusal olmayan ayrÄ±mlar iÃ§in veriyi aÃ§Ä±kÃ§a daha yÃ¼ksek boyuta taÅŸÄ±mak yerine, Ã§ekirdek fonksiyonlarÄ± ile iÃ§ Ã§arpÄ±mlar \( \phi(\mathbf{x}_i)^{\top}\phi(\mathbf{x}_j) \) hesaplanÄ±r:

- DoÄŸrusal: \( K(\mathbf{x},\mathbf{x}') = \mathbf{x}^{\top}\mathbf{x}' \)
- Polinom: \( K(\mathbf{x},\mathbf{x}') = (\gamma\,\mathbf{x}^{\top}\mathbf{x}' + r)^d \)
- RBF/Gauss: \( K(\mathbf{x},\mathbf{x}') = \exp(-\gamma\,\lVert\mathbf{x}-\mathbf{x}'\rVert^2) \)
- Sigmoid (tanh): \( K = \tanh(\gamma\,\mathbf{x}^{\top}\mathbf{x}' + r) \)

> ![SVM â€” Ã§ekirdek fonksiyonlarÄ± 140](Images/140.jpg)

Pratik ipuÃ§larÄ±:
- Ã–zellik Ã¶lÃ§ekleme (standartlaÅŸtÄ±rma) SVM iÃ§in Ã§ok Ã¶nemlidir, Ã¶zellikle RBF/Polinom Ã§ekirdeÄŸinde.
- C (dÃ¼zenlileÅŸtirme): BÃ¼yÃ¼k C â†’ hataya dÃ¼ÅŸÃ¼k tolerans (daha dar marjin), kÃ¼Ã§Ã¼k C â†’ daha geniÅŸ marjin (daha fazla tolerans).
- RBFâ€™de Î³ (gamma): BÃ¼yÃ¼k Î³ â†’ daha â€œdarâ€ etkiler (aÅŸÄ±rÄ± uyum riski), kÃ¼Ã§Ã¼k Î³ â†’ daha â€œgenelâ€ karar sÄ±nÄ±rÄ±.

KÄ±sa scikitâ€‘learn Ã¶rnekleri:

```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# DoÄŸrusal SVM (Ã§izgisel ayÄ±rÄ±cÄ±)
clf_lin = make_pipeline(StandardScaler(), SVC(kernel="linear", C=1.0))
clf_lin.fit(X_train, y_train)
acc_lin = clf_lin.score(X_test, y_test)

# RBF Ã§ekirdekli SVM (doÄŸrusal olmayan)
clf_rbf = make_pipeline(StandardScaler(), SVC(kernel="rbf", C=1.0, gamma="scale"))
clf_rbf.fit(X_train, y_train)
acc_rbf = clf_rbf.score(X_test, y_test)
print({"acc_linear": acc_lin, "acc_rbf": acc_rbf})
```

Notlar:
- OlasÄ±lÄ±k Ã§Ä±ktÄ±sÄ± gerekirse `SVC(probability=True)` ile kalibrasyon yapÄ±labilir (maliyetlidir). Alternatif: `CalibratedClassifierCV`.
- Regresyon iÃ§in SVR/SVR(kernel=...) varyantlarÄ± mevcuttur (Îµâ€‘insensitive loss).

### Avantajlar (Ã¶zet)

- Ã‡Ã¶zÃ¼m yalnÄ±zca destek vektÃ¶rlerine dayanÄ±r; seyrek ve genellemesi kuvvetlidir.
- KÃ¼Ã§Ã¼k/orta boy veri kÃ¼melerinde gÃ¼Ã§lÃ¼ karar sÄ±nÄ±rlarÄ± Ã¶ÄŸrenir; yÃ¼ksek boyutlarda da etkilidir.
- Uygun Ã§ekirdeklerle doÄŸrusal olmayan karmaÅŸÄ±k sÄ±nÄ±rlar oluÅŸturabilir.

## Karar AÄŸaÃ§larÄ±

- Karar aÄŸaÃ§larÄ±, aÄŸaÃ§ yapÄ±sÄ± ÅŸeklindeki bir dizi karar kuralÄ±yla veri kÃ¼mesini bÃ¶lerek hedef deÄŸiÅŸkenin tahmin edilmesini saÄŸlar.
- AkÄ±ÅŸ ÅŸemasÄ±na benzeyen yapÄ±lardÄ±r: Her bir Ã¶znitelik bir dÃ¼ÄŸÃ¼mle temsil edilir. Dallar ve yapraklar aÄŸaÃ§ yapÄ±sÄ±nÄ±n elemanlarÄ±dÄ±r. En Ã¼st yapÄ± â€œkÃ¶kâ€, aradaki yapÄ±lar â€œdalâ€, en son sÄ±nÄ±flandÄ±rÄ±cÄ±/Ã§Ä±ktÄ± dÃ¼ÄŸÃ¼mleri â€œyaprakâ€tÄ±r.
- Parametrik olmayan bir denetimli Ã¶ÄŸrenme algoritmasÄ±dÄ±r; hem sÄ±nÄ±flandÄ±rma (Decision Tree Classifier) hem de regresyon (Decision Tree Regressor) gÃ¶revlerinde kullanÄ±lÄ±r.

> ![Karar aÄŸaÃ§larÄ± â€” gÃ¶rsel 73](Images/73.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 73 â€” Karar aÄŸacÄ± yapÄ±sÄ±: kÃ¶k, iÃ§ dÃ¼ÄŸÃ¼mler (kurallar) ve yapraklar (tahmin).

KÄ±sa notlar:

- BÃ¶lme Ã¶lÃ§Ã¼tleri (sÄ±nÄ±flandÄ±rma): Gini safsÄ±zlÄ±ÄŸÄ±, Entropi (Bilgi KazancÄ±)
- BÃ¶lme Ã¶lÃ§Ã¼tÃ¼ (regresyon): Ortalama kare hatasÄ± (MSE), Ortalama mutlak hata (MAE)
- AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) Ã¶nlemek iÃ§in: maksimum derinlik, yaprak baÅŸÄ±na minimum Ã¶rnek sayÄ±sÄ±, budama (cost-complexity pruning) gibi dÃ¼zenleme kontrolleri ayarlanÄ±r.


### Dallanma Kriterleri

- Karar aÄŸaÃ§larÄ±nda en kritik konulardan biri, herhangi bir dÃ¼ÄŸÃ¼mden (Ã¶zellikle kÃ¶kten) itibaren hangi kritere gÃ¶re dallanÄ±lacaÄŸÄ±dÄ±r.
- KullanÄ±lan algoritmaya (CART, ID3/C4.5, CHAID vb.) gÃ¶re aÄŸacÄ±n ÅŸekli deÄŸiÅŸebilir. KÃ¶k dÃ¼ÄŸÃ¼mÃ¼n farklÄ± seÃ§ilmesi, yapraklara giden yolu ve dolayÄ±sÄ±yla sÄ±nÄ±flandÄ±rma sonucunu deÄŸiÅŸtirebilir.
- Uygulama baÄŸlamÄ±nda sÄ±kÃ§a aranan hedef: O noktada dallara ayrÄ±ldÄ±ÄŸÄ±nda kayÄ±tlarÄ±n olabildiÄŸince â€œdengeliâ€ ve bilgi aÃ§Ä±sÄ±ndan ayÄ±rt edici parÃ§alara bÃ¶lÃ¼nmesidir; bÃ¶ylece istenen sÄ±nÄ±fa/yanÄ±ta en kÄ±sa yoldan ulaÅŸmak amaÃ§lanÄ±r.
- Pratikte yaygÄ±n Ã¶lÃ§Ã¼tler (hatÄ±rlatma):
    - SÄ±nÄ±flandÄ±rma: Gini safsÄ±zlÄ±ÄŸÄ±, Entropi (Bilgi KazancÄ±)
    - Regresyon: MSE/MAE bazlÄ± bÃ¶lmeler


### Ã–rnek: KÃ¶k DÃ¼ÄŸÃ¼m SeÃ§imi ve Dallanma Ãœzerine

AÅŸaÄŸÄ±daki veri kÃ¼mesinde sÄ±nÄ±f deÄŸiÅŸkeni Bedenâ€™dir (baÄŸÄ±mlÄ± deÄŸiÅŸken). AmaÃ§, uygun kÃ¶k dÃ¼ÄŸÃ¼mÃ¼ ve dallanma mantÄ±ÄŸÄ±nÄ± tartÄ±ÅŸmaktÄ±r.

| Cinsiyet | Kilo | Boy | Beden |
|----------|------|-----|-------|
| K        | 48   | 170 | Orta  |
| K        | 49   | 151 | KÃ¼Ã§Ã¼k |
| K        | 52   | 158 | Orta  |
| K        | 56   | 165 | Orta  |
| E        | 59   | 160 | KÃ¼Ã§Ã¼k |
| K        | 61   | 159 | Orta  |
| E        | 62   | 162 | KÃ¼Ã§Ã¼k |
| E        | 63   | 174 | Orta  |
| K        | 68   | 168 | Orta  |
| K        | 69   | 177 | BÃ¼yÃ¼k |
| E        | 72   | 170 | Orta  |
| E        | 74   | 165 | KÃ¼Ã§Ã¼k |
| E        | 85   | 175 | Orta  |
| E        | 85   | 190 | BÃ¼yÃ¼k |
| E        | 98   | 190 | BÃ¼yÃ¼k |

> ![Karar aÄŸacÄ± Ã¶rneÄŸi â€” gÃ¶rsel 74](Images/74.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 74 â€” Ã–rnek veri kÃ¼mesi Ã¼zerinde farklÄ± kÃ¶k seÃ§imlerinin aÄŸaÃ§ yapÄ±sÄ±na etkisi.

TartÄ±ÅŸma notlarÄ±:

- Beden deÄŸiÅŸkeni sÄ±nÄ±f deÄŸiÅŸkenidir (baÄŸÄ±mlÄ± deÄŸiÅŸken). Hangi deÄŸiÅŸken kÃ¶k dÃ¼ÄŸÃ¼m olmalÄ±?
- FarklÄ± algoritmalar (CART, ID3/C4.5, CHAID) farklÄ± aÄŸaÃ§lar Ã¼retebilir; kÃ¶k seÃ§imi deÄŸiÅŸince sÄ±nÄ±flandÄ±rma yolu ve sonucu da deÄŸiÅŸebilir.
- FarklÄ± aÄŸaÃ§ yapÄ±larÄ± aynÄ± sonucu verebilir; bu durum Ã¶zellikle deÄŸiÅŸken sayÄ±sÄ± fazla olduÄŸunda gÃ¶rÃ¼lebilir.
- Bu Ã¶rnekte â€œkÃ¶k dÃ¼ÄŸÃ¼m olarak Cinsiyetâ€™in seÃ§ilmesiâ€ daha mantÄ±klÄ± bir baÅŸlangÄ±Ã§ olabilir (ayrÄ±ÅŸtÄ±rma gÃ¼cÃ¼ne ve basitliÄŸe gÃ¶re).

> ![Karar aÄŸacÄ± Ã¶rneÄŸi â€” gÃ¶rsel 75](Images/75.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 75 â€” Boy ve Kilo iÃ§in dallanma seÃ§enekleri: kaÃ§ dal kullanÄ±lmalÄ±? AralÄ±klara bÃ¶lme ve cinsiyete gÃ¶re farklÄ± aralÄ±klar mantÄ±klÄ± mÄ±?

Ä°leri dÃ¼ÅŸÃ¼nme sorularÄ±:

- Boy ve Kilo dÃ¼ÄŸÃ¼mleri iÃ§in kaÃ§ dal olmalÄ±? SÃ¼rekli deÄŸiÅŸkenler iÃ§in tipik yaklaÅŸÄ±m ikili bÃ¶lme (eÅŸik t: Boy < t / Boy â‰¥ t) veya aralÄ±klara bÃ¶lme (binning) ÅŸeklindedir.
- Boy ve Kilo deÄŸerlerini aralÄ±klara bÃ¶lmek (discretization) mantÄ±klÄ± mÄ±? Veri miktarÄ±, model sadeliÄŸi ve performans arasÄ±nda denge kurmak gerekir.
- Boy ve Kiloâ€™yu Cinsiyete gÃ¶re farklÄ± aralÄ±klara bÃ¶lmek mantÄ±klÄ± mÄ±? EtkileÅŸim (Cinsiyet Ã— Boy/Kilo) anlamlÄ± ise evet; aksi halde gereksiz karmaÅŸÄ±klÄ±k yaratabilir. AÄŸaÃ§ algoritmalarÄ± bu etkileÅŸimi kendiliÄŸinden yakalayabilir (Ã¶nce Cinsiyetâ€™e, sonra Boy/Kiloâ€™ya bÃ¶lerek).

### Karar aÄŸacÄ± oluÅŸturma adÄ±mlarÄ±

- AÄŸaÃ§ top-down recursive divide-and-conquer bir yaklaÅŸÄ±mla kurulur.
- BaÅŸlangÄ±Ã§: AÄŸaÃ§, tÃ¼m veriyi iÃ§eren tek bir kÃ¶k dÃ¼ÄŸÃ¼mle baÅŸlar.
- Nitelikler: Kategorik kabul edilir; sÃ¼rekli nitelikler iÃ§in eÅŸik/arÄ±klara bÃ¶lme (discretization) ya da doÄŸrudan eÅŸik tabanlÄ± bÃ¶lme uygulanÄ±r.
- Durdurma koÅŸullarÄ±:
    - Ã–rneklerin hepsi (veya bÃ¼yÃ¼k Ã§oÄŸunluÄŸu) aynÄ± sÄ±nÄ±fa aitse: dÃ¼ÄŸÃ¼m yapraktÄ±r ve sÄ±nÄ±f etiketini alÄ±r.
    - BÃ¶lmeyi yapacak uygun nitelik kalmamÄ±ÅŸsa.
    - Kalan nitelik deÄŸerlerine uyan Ã¶rnek kalmamÄ±ÅŸsa.
- Aksi durumda: Ã–rnekleri sÄ±nÄ±flara en iyi bÃ¶lecek nitelik ve/veya eÅŸik seÃ§ilir, veri alt kÃ¼melere ayrÄ±lÄ±r ve aynÄ± sÃ¼reÃ§ her alt dÃ¼ÄŸÃ¼mde tekrarlanÄ±r.

### En iyi bÃ¶len niteliÄŸi belirleme (Ä°yilik Fonksiyonu)

- FarklÄ± algoritmalar farklÄ± iyilik fonksiyonlarÄ± kullanÄ±r; bu seÃ§im aÄŸacÄ±n ÅŸeklini ve performansÄ±nÄ± etkiler.
- Bilgi kazancÄ± (Information Gain): ID3, C4.5
    - Klasik hÃ¢li ayrÄ±k niteliklere yÃ¶neliktir; sÃ¼rekli nitelikler iÃ§in aday eÅŸikler taranarak en iyi bÃ¶lme seÃ§ilir.
- Gini indeksi (CART)
    - Her nitelik iÃ§in ikili bÃ¶lmeler (eÅŸik tabanlÄ±) denenir; olasÄ± tÃ¼m ikili bÃ¶lmeler sÄ±nanarak en iyi ayrÄ±m seÃ§ilir.

### Entropi

- Entropi, bir sistemdeki belirsizliÄŸin Ã¶lÃ§Ã¼sÃ¼dÃ¼r. Karar aÄŸaÃ§larÄ±nda (Ã¶zellikle ID3/C4.5) kÃ¶k ve sonraki dÃ¼ÄŸÃ¼mlerde hangi nitelikten bÃ¶lÃ¼neceÄŸini belirlemede kullanÄ±lÄ±r.

$$
H(S) = - \sum_{c} p(c) \, \log_2 p(c)
$$

- Ã–zellikler (ikili sÄ±nÄ±flandÄ±rma baÄŸlamÄ±nda):
    - Ã–rneklerin hepsi aynÄ± sÄ±nÄ±fa aitse: entropi = 0
    - Ã–rnekler sÄ±nÄ±flar arasÄ±nda eÅŸit daÄŸÄ±lmÄ±ÅŸsa: entropi = 1
    - Rastgele/ara daÄŸÄ±lÄ±mlarda: 0 < entropi < 1

> ![Entropi denklemi â€” gÃ¶rsel 76](Images/76.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 76 â€” Shannon entropi denklemi (Claude Shannon, 1948).

> ![Entropi â€” gÃ¶rsel 77](Images/77.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 77 â€” Entropi deÄŸerlerinin sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±na gÃ¶re deÄŸiÅŸimi.

#### Ã–rnek: Entropi HesabÄ±

Veri kÃ¼mesi: \( S = \{\text{evet},\,\text{evet},\,\text{hayÄ±r},\,\text{hayÄ±r},\,\text{hayÄ±r},\,\text{hayÄ±r},\,\text{hayÄ±r},\,\text{hayÄ±r}\} \)

OlasÄ±lÄ±klar: \( p(\text{evet}) = 2/8 = 0.25 \), \( p(\text{hayÄ±r}) = 6/8 = 0.75 \).

$$
H(S) = -\bigl( 0.25\,\log_2 0.25 + 0.75\,\log_2 0.75 \bigr) \approx 0.81128.
$$

> ![Entropi â€” Ã¶rnek hesap â€” gÃ¶rsel 78](Images/78.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 78 â€” Ã–rnek sÄ±nÄ±f daÄŸÄ±lÄ±mÄ± iÃ§in entropi hesap adÄ±mlarÄ±.


#### Ã–rnek: MEMNUN Entropisi

Veri kÃ¼mesi (8 gÃ¶zlem): MODEL, CÄ°NSÄ°YET, YAÅ, MEMNUN.

SÄ±nÄ±f olasÄ±lÄ±klarÄ±: \( p(\text{HAYIR}) = 5/8 \), \( p(\text{EVET}) = 3/8 \).

$$
H(\text{MEMNUN}) = - \Bigl( \tfrac{5}{8}\,\log_2 \tfrac{5}{8} + \tfrac{3}{8}\,\log_2 \tfrac{3}{8} \Bigr) \approx 0{,}954.
$$


### Bilgi KazancÄ± (ID3 / C4.5)

- Bilgi kuramÄ± kavramlarÄ± kullanÄ±larak karar aÄŸacÄ± oluÅŸturulur; amaÃ§, sÄ±nÄ±flandÄ±rma iÃ§in gerekli karÅŸÄ±laÅŸtÄ±rma sayÄ±sÄ±nÄ± azaltmaktÄ±r.
- Soru: Bir nitelik Ã¼zerinde dallandÄ±ÄŸÄ±mÄ±zda entropi ne kadar dÃ¼ÅŸer? Bu dÃ¼ÅŸÃ¼ÅŸ â€œbilgi kazancÄ±â€dÄ±r.
- TanÄ±m (ikili/Ã§ok sÄ±nÄ±flÄ± genel hÃ¢l):

$$
\operatorname{Gain}(T, X) = H(T) - H(T\mid X), \quad H(T\mid X) = \sum_{x \in \mathcal{X}} p(x)\, H(T\mid X{=}x).
$$

    Notasyon notu: BazÄ± slayt/kitaplarda \( H(X,T) \) yazÄ±mÄ± gÃ¶rÃ¼lÃ¼r; burada kastedilen ÅŸartlÄ± entropinin beklenen deÄŸeridir (\( H(T\mid X) \)).

#### Ä°ÅŸlem AdÄ±mlarÄ±

1) SÄ±nÄ±f niteliÄŸinin entropisini hesapla: \( H(T) \)

> ![SÄ±nÄ±f entropisi â€” gÃ¶rsel 79](Images/79.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 79 â€” SÄ±nÄ±f (hedef) deÄŸiÅŸkeninin entropisinin hesaplanmasÄ±.

2) Her Ã¶zellik iÃ§in sÄ±nÄ±fa baÄŸÄ±mlÄ± (ÅŸartlÄ±) entropiyi hesapla: \( H(T\mid X) \)

> ![ÅartlÄ± entropiler â€” gÃ¶rsel 80](Images/80.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 80 â€” Ã–zelliklere gÃ¶re alt kÃ¼melerde entropi ve aÄŸÄ±rlÄ±klÄ± ortalama.

3) KazanÃ§larÄ± bul ve en bÃ¼yÃ¼k kazanca sahip Ã¶zelliÄŸi dÃ¼ÄŸÃ¼m olarak seÃ§: \( \operatorname{Gain}(T, X) = H(T) - H(T\mid X) \)


#### Ã–rnek: Bilgi KazancÄ± HesabÄ± (BORÃ‡ Ã¶zelliÄŸi)

Veri kÃ¼mesi (hedef: RÄ°SK):

| ID | BORÃ‡   | GELÄ°R | STATÃœ    | RÄ°SK |
|----|--------|-------|----------|------|
| 1  | YÃ¼ksek | YÃ¼ksek| Ä°ÅŸveren  | KÃ¶tÃ¼ |
| 2  | YÃ¼ksek | YÃ¼ksek| Ãœcretli  | KÃ¶tÃ¼ |
| 3  | YÃ¼ksek | DÃ¼ÅŸÃ¼k | Ãœcretli  | KÃ¶tÃ¼ |
| 4  | DÃ¼ÅŸÃ¼k  | DÃ¼ÅŸÃ¼k | Ãœcretli  | Ä°yi  |
| 5  | DÃ¼ÅŸÃ¼k  | DÃ¼ÅŸÃ¼k | Ä°ÅŸveren  | KÃ¶tÃ¼ |
| 6  | DÃ¼ÅŸÃ¼k  | YÃ¼ksek| Ä°ÅŸveren  | Ä°yi  |
| 7  | DÃ¼ÅŸÃ¼k  | YÃ¼ksek| Ãœcretli  | Ä°yi  |
| 8  | DÃ¼ÅŸÃ¼k  | DÃ¼ÅŸÃ¼k | Ãœcretli  | Ä°yi  |
| 9  | DÃ¼ÅŸÃ¼k  | DÃ¼ÅŸÃ¼k | Ä°ÅŸveren  | KÃ¶tÃ¼ |
| 10 | DÃ¼ÅŸÃ¼k  | YÃ¼ksek| Ä°ÅŸveren  | Ä°yi  |

1) SÄ±nÄ±f entropisi: RÄ°SK = {kÃ¶tÃ¼,kÃ¶tÃ¼,kÃ¶tÃ¼,iyi,kÃ¶tÃ¼,iyi,iyi,iyi,kÃ¶tÃ¼,iyi} â‡’ p(Ä°yi)=0.5, p(KÃ¶tÃ¼)=0.5

$$
H(\text{RÄ°SK}) = -\bigl(0.5\log_2 0.5 + 0.5\log_2 0.5\bigr) = 1.
$$

2) BORÃ‡â€™a gÃ¶re ÅŸartlÄ± entropiler ve aÄŸÄ±rlÄ±klar:

- BORÃ‡=YÃ¼ksek: 3 Ã¶rnek, sÄ±nÄ±f daÄŸÄ±lÄ±mÄ± (3 KÃ¶tÃ¼, 0 Ä°yi) â‡’ \( H(\text{RÄ°SK}\mid \text{BORÃ‡}{=}\text{YÃ¼ksek}) = 0 \)
- BORÃ‡=DÃ¼ÅŸÃ¼k: 7 Ã¶rnek, sÄ±nÄ±f daÄŸÄ±lÄ±mÄ± (2 KÃ¶tÃ¼, 5 Ä°yi) â‡’ \( H(\text{RÄ°SK}\mid \text{BORÃ‡}{=}\text{DÃ¼ÅŸÃ¼k}) \approx 0{,}863 \)

AÄŸÄ±rlÄ±klÄ± ÅŸartlÄ± entropi (slayttaki deÄŸerlerle):

$$
H(\text{RÄ°SK}\mid \text{BORÃ‡}) = \tfrac{3}{10}\cdot 0 + \tfrac{7}{10}\cdot 0{,}863 \approx 0{,}64.
$$

3) Bilgi kazancÄ±:

$$
\operatorname{Gain}(\text{RÄ°SK}, \text{BORÃ‡}) = H(\text{RÄ°SK}) - H(\text{RÄ°SK}\mid \text{BORÃ‡}) \approx 1 - 0{,}64 = 0{,}36.
$$

Not: Benzer ÅŸekilde \( \operatorname{Gain}(\text{RÄ°SK},\text{GELÄ°R}) \) ve \( \operatorname{Gain}(\text{RÄ°SK},\text{STATÃœ}) \) hesaplanÄ±r; en bÃ¼yÃ¼k kazanca sahip nitelik kÃ¶k dÃ¼ÄŸÃ¼m seÃ§ilir.


#### Ã–rnek: Bilgi KazancÄ± â€” MEMNUN Veri KÃ¼mesi

Veri kÃ¼mesi (8 gÃ¶zlem):

| MODEL | CÄ°NSÄ°YET | YAÅ | MEMNUN |
|-------|----------|-----|--------|
| X5    | ERKEK    | 21  | HAYIR  |
| X3    | KADIN    | 19  | EVET   |
| X5    | ERKEK    | 22  | HAYIR  |
| X3    | ERKEK    | 21  | EVET   |
| X3    | ERKEK    | 30  | EVET   |
| X3    | KADIN    | 60  | HAYIR  |
| X3    | KADIN    | 45  | HAYIR  |
| X3    | ERKEK    | 55  | HAYIR  |

SÄ±nÄ±f entropisi:

$$
H(\text{MEMNUN}) = -\Bigl( \tfrac{5}{8}\log_2 \tfrac{5}{8} + \tfrac{3}{8}\log_2 \tfrac{3}{8} \Bigr) \approx 0{,}954.
$$

1) MODEL iÃ§in kazanÃ§:

- Alt kÃ¼meler:
    - MODEL = X5: (2 HAYIR, 0 EVET) â‡’ \( H = 0 \)
    - MODEL = X3: (3 EVET, 3 HAYIR) â‡’ \( H = 1 \)
- AÄŸÄ±rlÄ±klÄ± ÅŸartlÄ± entropi: \( H(\text{MEMNUN}\mid \text{MODEL}) = \tfrac{2}{8}\cdot 0 + \tfrac{6}{8}\cdot 1 = 0{,}75 \)
- KazanÃ§: \( 0{,}954 - 0{,}75 = 0{,}204 \)

2) CÄ°NSÄ°YET iÃ§in kazanÃ§:

- Alt kÃ¼meler:
    - ERKEK: (2 HAYIR, 3 EVET) â‡’ \( H \approx 0{,}971 \)
    - KADIN: (2 HAYIR, 1 EVET) â‡’ \( H \approx 0{,}918 \)
- AÄŸÄ±rlÄ±klÄ± ÅŸartlÄ± entropi: \( H(\text{MEMNUN}\mid \text{CÄ°NSÄ°YET}) = \tfrac{5}{8}\cdot 0{,}971 + \tfrac{3}{8}\cdot 0{,}918 \approx 0{,}951 \)
- KazanÃ§: \( 0{,}954 - 0{,}951 = 0{,}003 \)

3) YAÅ iÃ§in kazanÃ§ (eÅŸikler taranarak):

- \( \le 19/ >19 \): \( H\_{\le 19} = 0,\ H\_{>19} \approx 0{,}863 \) â‡’ \( H = \tfrac{1}{8}\cdot 0 + \tfrac{7}{8}\cdot 0{,}863 = 0{,}754 \), KazanÃ§ â‰ˆ 0.200
- \( \le 21/ >21 \): \( H\_{\le 21} \approx 0{,}918,\ H\_{>21} \approx 0{,}722 \) â‡’ \( H \approx 0{,}796 \), KazanÃ§ â‰ˆ 0.158
- \( \le 22/ >22 \): \( H\_{\le 22} = 1,\ H\_{>22} \approx 0{,}811 \) â‡’ \( H \approx 0{,}905 \), KazanÃ§ â‰ˆ 0.049
- \( \le 30/ >30 \): \( H\_{\le 30} \approx 0{,}971,\ H\_{>30} = 0 \) â‡’ \( H = \tfrac{5}{8}\cdot 0{,}971 + \tfrac{3}{8}\cdot 0 = 0{,}607 \), KazanÃ§ â‰ˆ 0.347
- \( \le 45/ >45 \): \( H\_{\le 45} = 1,\ H\_{>45} = 0 \) â‡’ \( H = 0{,}75 \), KazanÃ§ â‰ˆ 0.204
- \( \le 55/ >55 \): \( H\_{\le 55} \approx 0{,}985,\ H\_{>55} = 0 \) â‡’ \( H \approx 0{,}862 \), KazanÃ§ â‰ˆ 0.092
- \( \le 60/ >60 \): \( H\_{\le 60} \approx 0{,}954 \) (tÃ¼m veri) â‡’ KazanÃ§ = 0

Ã–zet: En yÃ¼ksek kazanÃ§ YAÅ â‰¤ 30 eÅŸiÄŸi ile \( \approx 0{,}347 \).

> ![Bilgi kazancÄ± Ã¶zeti â€” gÃ¶rsel 100](Images/100.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 100 â€” MODEL, CÄ°NSÄ°YET ve YAÅ iÃ§in bilgi kazancÄ± karÅŸÄ±laÅŸtÄ±rmasÄ±.

Alt aÄŸaÃ§ (YAÅ â‰¤ 30) iÃ§in yeniden kazanÃ§lar:

- Filtre: YAÅ â‰¤ 30 kayÄ±tlarÄ± tutulur (5 gÃ¶zlem). \( H(\text{MEMNUN}) = - (\tfrac{2}{5}\log_2 \tfrac{2}{5} + \tfrac{3}{5}\log_2 \tfrac{3}{5}) \approx 0{,}971 \).
- MODEL iÃ§in: X5 alt kÃ¼mesi (2 HAYIR) â‡’ H=0; X3 alt kÃ¼mesi (3 EVET) â‡’ H=0 â†’ AÄŸÄ±rlÄ±klÄ± H = 0, KazanÃ§ = 0.971
- CÄ°NSÄ°YET iÃ§in: ERKEK (2 HAYIR, 2 EVET) â‡’ H = 1; KADIN (1 EVET) â‡’ H = 0 â†’ AÄŸÄ±rlÄ±klÄ± H = \( \tfrac{4}{5}\cdot 1 + \tfrac{1}{5}\cdot 0 = 0{,}8 \), KazanÃ§ = 0.171

> ![Alt aÄŸaÃ§ kazanÃ§ Ã¶zeti â€” gÃ¶rsel 101](Images/101.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 101 â€” YAÅ â‰¤ 30 alt kÃ¼mesinde bilgi kazanÃ§larÄ± (MODEL ve CÄ°NSÄ°YET).

Kurallar (Ã¶zet):

- EÄŸer YAÅ â‰¤ 30 ve MODEL = X3 ise â‡’ MEMNUN = EVET
- EÄŸer YAÅ â‰¤ 30 ve MODEL = X5 ise â‡’ MEMNUN = HAYIR
- EÄŸer YAÅ > 30 ise â‡’ MEMNUN = HAYIR


## Uygulama: Hava Problemi (ID3 / C4.5)

BaÅŸlangÄ±Ã§ kÃ¼mesi ve sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:

- OYUN = {hayÄ±r, hayÄ±r, hayÄ±r, hayÄ±r, hayÄ±r, evet, evet, evet, evet, evet, evet, evet, evet, evet}
- C1 = â€œhayÄ±râ€, C2 = â€œevetâ€
- p(C1) = 5/14, p(C2) = 9/14

> ![Hava problemi â€” veri ve daÄŸÄ±lÄ±m â€” gÃ¶rsel 81](Images/81.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 81 â€” Hava problemi verisi ve sÄ±nÄ±f daÄŸÄ±lÄ±mÄ± (OYUN).

### AdÄ±m 1: Birinci Dallanma

> ![AdÄ±m 1 â€” gÃ¶rsel 83](Images/83.jpg)
>
> ![AdÄ±m 1 â€” gÃ¶rsel 84](Images/84.jpg)
>
> ![AdÄ±m 1 â€” gÃ¶rsel 85](Images/85.jpg)
>
> ![AdÄ±m 1 â€” gÃ¶rsel 86](Images/86.jpg)

Birinci dallanma sonrasÄ± oluÅŸan karar aÄŸacÄ±:

> ![AdÄ±m 1 â€” karar aÄŸacÄ± â€” gÃ¶rsel 87](Images/87.jpg)

### AdÄ±m 2: HAVA = â€œgÃ¼neÅŸliâ€ dalÄ± iÃ§in dallanma

> ![AdÄ±m 2 â€” gÃ¼neÅŸli â€” gÃ¶rsel 88](Images/88.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 88 â€” â€œgÃ¼neÅŸliâ€ alt kÃ¼mesi Ã¼zerinde olasÄ± bÃ¶lmeler.

> ![AdÄ±m 2 â€” gÃ¼neÅŸli â€” OYUN entropisi â€” gÃ¶rsel 89](Images/89.jpg)
>
> ![AdÄ±m 2 â€” gÃ¼neÅŸli â€” gÃ¶rsel 90](Images/90.jpg)
>
> ![AdÄ±m 2 â€” gÃ¼neÅŸli â€” gÃ¶rsel 91](Images/91.jpg)
>
> ![AdÄ±m 2 â€” gÃ¼neÅŸli â€” gÃ¶rsel 92](Images/92.jpg)
>
> ![AdÄ±m 2 â€” gÃ¼neÅŸli â€” gÃ¶rsel 93](Images/93.jpg)

### AdÄ±m 3: HAVA = â€œbulutluâ€ dalÄ± iÃ§in dallanma

> ![AdÄ±m 3 â€” bulutlu â€” gÃ¶rsel 94](Images/94.jpg)
>
> ![AdÄ±m 3 â€” bulutlu â€” gÃ¶rsel 95](Images/95.jpg)
>
> ![AdÄ±m 3 â€” bulutlu â€” gÃ¶rsel 96](Images/96.jpg)
>
> ![AdÄ±m 3 â€” bulutlu â€” gÃ¶rsel 97](Images/97.jpg)
>
> ![AdÄ±m 3 â€” bulutlu â€” gÃ¶rsel 98](Images/98.jpg)

### SonuÃ§: OluÅŸturulan Karar AÄŸacÄ±

> ![Hava problemi â€” final aÄŸaÃ§ â€” gÃ¶rsel 99](Images/99.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 99 â€” Nihai karar aÄŸacÄ± (ID3/C4.5 adÄ±mlarÄ± ile elde edilen).


### C4.5 AlgoritmasÄ±

- C4.5, Quinlan tarafÄ±ndan ID3â€™Ã¼n devamÄ± olarak geliÅŸtirilmiÅŸtir; sayÄ±sal (nÃ¼merik) niteliklerle karar aÄŸacÄ± kurmayÄ± destekler.
- ID3â€™e gÃ¶re temel farklarÄ±:
    - NÃ¼merik deÄŸerleri kategorik kararlara dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in bir eÅŸik (threshold) belirler ve \( x < t \) / \( x \ge t \) ÅŸeklinde dallandÄ±rÄ±r.
    - Bilinmeyen/eksik deÄŸerler iÃ§in bir iÅŸleme yÃ¶ntemi sunar (Ã¶r. olasÄ±lÄ±ksal daÄŸÄ±tÄ±m veya Ã¶rnek aÄŸÄ±rlÄ±klarÄ±yla paylaÅŸtÄ±rma).
    - (Not) Uygulamada budama (pruning) yaparak aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi azaltÄ±r.
- EÅŸik seÃ§imi: Aday eÅŸikler Ã¼zerinde bilgi kazancÄ± deÄŸerlendirilir; en bÃ¼yÃ¼k bilgi kazancÄ±nÄ± veren eÅŸik seÃ§ilerek dallanma yapÄ±lÄ±r.

Not â€” yaygÄ±n pratik: C4.5, bilgi kazancÄ±nÄ±n yÃ¼ksek kardinaliteli nitelikleri kayÄ±rma eÄŸilimine karÅŸÄ± â€œkazanÃ§ oranÄ±â€ (gain ratio) da kullanÄ±r. Bu notlarda slayt iÃ§eriÄŸine uygun olarak eÅŸik seÃ§iminde bilgi kazancÄ± vurgulanmÄ±ÅŸtÄ±r.


### SÄ±nÄ±flandÄ±rma ve Regresyon AÄŸaÃ§larÄ± (CART)

- TarihÃ§e: 1984â€™te Breiman, Friedman, Olshen ve Stone tarafÄ±ndan ortaya konmuÅŸtur. CART, her iÃ§ dÃ¼ÄŸÃ¼mde veriyi Ä°KÄ°YE bÃ¶len ikili (binary) karar aÄŸaÃ§larÄ± Ã¼retir.
- Temel ilke: Her karar dÃ¼ÄŸÃ¼mÃ¼nde bir nitelik ve olasÄ± bir eÅŸik seÃ§ilir; dÃ¼ÄŸÃ¼m, â€œsolâ€ ve â€œsaÄŸâ€ olmak Ã¼zere iki dala ayrÄ±lÄ±r. Bu iÅŸlem Ã¶zyinelemeli ÅŸekilde devam eder.

#### BÃ¶lme (Split) NasÄ±l SeÃ§ilir?

1) TÃ¼m nitelikler taranÄ±r. SayÄ±sal (nÃ¼merik) nitelikler iÃ§in aday eÅŸikler genelde sÄ±ralÄ± deÄŸerler arasÄ±ndaki orta noktalardÄ±r. Kategorik nitelikler iÃ§in ikili bÃ¶lmeler (alt kÃ¼meâ€“tamamlayanÄ±) denenir.
2) Her aday bÃ¶lme iÃ§in bir iyilik Ã¶lÃ§Ã¼tÃ¼ hesaplanÄ±r; en bÃ¼yÃ¼k safsÄ±zlÄ±k azalÄ±mÄ± (impurity decrease) saÄŸlayan bÃ¶lme seÃ§ilir.
3) Durdurma: Minimum yaprak Ã¶rneÄŸi, maksimum derinlik, min impurity decrease gibi koÅŸullar; aksi halde yaprakta sÄ±nÄ±f Ã§oÄŸunluÄŸu (sÄ±nÄ±flandÄ±rma) veya hedefin ortalamasÄ±/medyanÄ± (regresyon) atanÄ±r.

#### SÄ±nÄ±flandÄ±rma iÃ§in Ã–lÃ§Ã¼tler

- Gini SafsÄ±zlÄ±ÄŸÄ± (Gini Index): CARTâ€™ta varsayÄ±lan Ã¶lÃ§Ã¼ttÃ¼r.
    - Bir dÃ¼ÄŸÃ¼mde sÄ±nÄ±f oranlarÄ± p(c) ise: Gini(t) = 1 âˆ’ âˆ‘_c p(c)^2
    - Aday bÃ¶lme s = (t â†’ L,R) iÃ§in aÄŸÄ±rlÄ±klÄ± safsÄ±zlÄ±k: Gini_s = (|L|/|t|)Â·Gini(L) + (|R|/|t|)Â·Gini(R)
    - KazanÃ§: Î” = Gini(t) âˆ’ Gini_s; en bÃ¼yÃ¼k Î” seÃ§ilir.

- Twoing Kriteri: Ã‡ok sÄ±nÄ±flÄ± sorunlarda sÄ±nÄ±flarÄ± iki â€œsÃ¼per-sÄ±nÄ±faâ€ ayÄ±racak bÃ¶lmeleri teÅŸvik eder.
    - Sezgi: Sol ve saÄŸ dallardaki sÄ±nÄ±f oranlarÄ± farklarÄ±nÄ± bÃ¼yÃ¼tmeyi hedefler.
    - Tipik form: Score(s) = (|L|Â·|R| / |t|^2) Â· [Â½ Â· âˆ‘_c |p_L(c) âˆ’ p_R(c)|]^2. DeÄŸer ne kadar bÃ¼yÃ¼kse bÃ¶lme o kadar iyidir.

#### Twoing AlgoritmasÄ± (detay)

Bir t dÃ¼ÄŸÃ¼mÃ¼ iÃ§in aday bir bÃ¶lme, veriyi iki alt kÃ¼meye ayÄ±rÄ±r: sol (L) ve saÄŸ (R). AÅŸaÄŸÄ±daki bÃ¼yÃ¼klÃ¼kler tanÄ±mlanÄ±r:

- Sol ve saÄŸ dal oranlarÄ±: $P_L = \tfrac{|L|}{|t|}$, $P_R = \tfrac{|R|}{|t|}$.
- SÄ±nÄ±f koÅŸullu oranlarÄ±: $p_L(j) = \tfrac{n_j(L)}{|L|}$, $p_R(j) = \tfrac{n_j(R)}{|R|}$; burada $n_j(\cdot)$ ilgili alt kÃ¼medeki j sÄ±nÄ±fÄ± adedidir.

Slayt notasyonunda (sol/saÄŸ iÃ§in $P_{sol}, P_{saÄŸ}$; koÅŸullu olasÄ±lÄ±klar $P(j\mid t_{sol}), P(j\mid t_{saÄŸ})$) Twoing uygunluk Ã¶lÃ§Ã¼sÃ¼ ÅŸu ÅŸekilde yazÄ±lÄ±r:

$$
\Phi_s(t) = 2\,P_{sol}\,P_{saÄŸ} \sum_{j=1}^{K} \bigl\lvert P(j\mid t_{sol}) - P(j\mid t_{saÄŸ}) \bigr\rvert.
$$

Not â€” kaynak farkÄ±: BazÄ± referanslarda aÅŸaÄŸÄ±daki eÅŸdeÄŸere yakÄ±n bir form kullanÄ±lÄ±r ve karesel terim iÃ§erir:

$$
        	ext{Score}(s) = \frac{|L|\,|R|}{|t|^2} \left[\tfrac{1}{2}\sum_{j=1}^{K} \bigl\lvert p_L(j) - p_R(j) \bigr\rvert\right]^2.
$$

Ä°ki form da pratikte benzer sÄ±ralama yapar; amaÃ§, sol ve saÄŸdaki sÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±nÄ± mÃ¼mkÃ¼n olduÄŸunca birbirinden farklÄ± kÄ±lmaktÄ±r. Ä°ÅŸlem adÄ±mlarÄ±:

1) Aday bÃ¶lmeyi (Ã¶zellik + eÅŸik/alt kÃ¼me) oluÅŸtur.
2) $P_L, P_R$ ve her sÄ±nÄ±f iÃ§in $p_L(j), p_R(j)$ deÄŸerlerini hesapla.
3) $\Phi_s(t)$ (veya eÅŸdeÄŸeri skor) deÄŸerini hesapla.
4) TÃ¼m adaylar iÃ§inde en bÃ¼yÃ¼k skoru veren bÃ¶lmeyi seÃ§.

##### Ã–rnek: Twoing ile Karar AÄŸacÄ± (MEMNUN â€” 11 gÃ¶zlem)

Tabloda Ã§alÄ±ÅŸanlarÄ±n maaÅŸ, deneyim, gÃ¶rev niteliklerine gÃ¶re hedef niteliÄŸi MEMNUN olan 11 gÃ¶zlem verilmiÅŸtir. Twoing algoritmasÄ±yla karar aÄŸacÄ± oluÅŸturalÄ±m.

> ![Ã–rnek veri â€” gÃ¶rsel 106](Images/106.jpg)
>
> AÃ§Ä±klama: 11 gÃ¶zlem ve nitelikler (MAAÅ, DENEYÄ°M, GÃ–REV) ile MEMNUN hedefi.

Ã–nce tÃ¼m aday bÃ¶lÃ¼nmeleri Ã¼retelim.

> ![Aday bÃ¶lÃ¼nmeler â€” gÃ¶rsel 107](Images/107.jpg)
>
> AÃ§Ä±klama: Ã–zellikâ€“eÅŸik/alt kÃ¼me temelli tÃ¼m aday ikili bÃ¶lmeler.

Her aday bÃ¶lme iÃ§in sol/saÄŸ taraf sÄ±nÄ±f olasÄ±lÄ±klarÄ± hesaplanÄ±r.

> ![SÄ±nÄ±f olasÄ±lÄ±klarÄ± (I) â€” gÃ¶rsel 108](Images/108.jpg)
>
> ![SÄ±nÄ±f olasÄ±lÄ±klarÄ± (II) â€” gÃ¶rsel 109](Images/109.jpg)

Twoing uygunluk Ã¶lÃ§Ã¼sÃ¼ (slayt notasyonu):

$$
\Phi_s(t) = 2\,P_{sol}\,P_{saÄŸ} \sum_{j\in\{EVET, HAYIR\}} \bigl\lvert P(j\mid t_{sol}) - P(j\mid t_{saÄŸ}) \bigr\rvert.
$$

Ã–rnek bir aday iÃ§in (BÃ¶lÃ¼nme 1):

$$
\Phi_s(t) = 2\cdot 0.09\cdot 0.91\,\Bigl(|1.00 - 0.60| + |0.00 - 0.40|\Bigr) = 0.131.
$$

AdaylarÄ±n Ã¶zeti:

| BÃ–LÃœNME | Psol | PsaÄŸ | P(Evet|tsol) | P(Evet|tsaÄŸ) | P(HayÄ±r|tsaÄŸ) | P(HayÄ±r|tsol) | Uygunluk Ã–lÃ§Ã¼sÃ¼ |
|---------:|:----:|:----:|:------------:|:------------:|:-------------:|:-------------:|:----------------:|
| 1 | 0.09 | 0.91 | 1.00 | 0.60 | 0.40 | 0.00 | 0.131 |
| 2 | 0.45 | 0.55 | 0.60 | 0.67 | 0.33 | 0.40 | 0.069 |
| 3 | 0.45 | 0.55 | 0.60 | 0.67 | 0.33 | 0.40 | 0.069 |
| 4 | 0.18 | 0.82 | 1.00 | 0.56 | 0.44 | 0.00 | 0.260 |
| 5 | 0.45 | 0.55 | 0.60 | 0.67 | 0.33 | 0.40 | 0.069 |
| 6 | 0.36 | 0.64 | 0.50 | 0.71 | 0.29 | 0.50 | 0.194 |
| 7 | 0.55 | 0.45 | 0.33 | 1.00 | 0.00 | 0.67 | 0.663 |
| 8 | 0.45 | 0.55 | 1.00 | 0.33 | 0.67 | 0.00 | 0.663 |

En bÃ¼yÃ¼k uygunluk Ã¶lÃ§Ã¼sÃ¼nÃ¼ veren bÃ¶lÃ¼nme seÃ§ilir (Ã¶rnekte 7 veya 8). AÄŸaÃ§ inÅŸasÄ±na bu bÃ¶lÃ¼nmeden baÅŸlanÄ±r ve alt dÃ¼ÄŸÃ¼mlerde aynÄ± iÅŸlem yinelemeli sÃ¼rdÃ¼rÃ¼lÃ¼r.

> ![SeÃ§ilen bÃ¶lÃ¼nme â€” gÃ¶rsel 110](Images/110.jpg)
>
> ![Ara aÄŸaÃ§ â€” gÃ¶rsel 111](Images/111.jpg)

Bu Ã¶rnekten elde edilen kurallar:

1) EÄER (GÃ–REV = YÃ–NETÄ°CÄ°) Ä°SE (MEMNUN = EVET)
2) EÄER (GÃ–REV = UZMAN) VE (MAAÅ = NORMAL) Ä°SE (MEMNUN = EVET)
3) EÄER (GÃ–REV = UZMAN) VE (MAAÅ = DÃœÅÃœK VEYA MAAÅ = YÃœKSEK) VE (DENEYÄ°M = YOK) Ä°SE (MEMNUN = EVET)
4) EÄER (GÃ–REV = UZMAN) VE (MAAÅ = DÃœÅÃœK VEYA MAAÅ = YÃœKSEK) VE (DENEYÄ°M = ORTA VEYA DENEYÄ°M = Ä°YÄ°) Ä°SE (MEMNUN = HAYIR)

#### Gini AlgoritmasÄ± (detay)

Gini algoritmasÄ±nda her aday bÃ¶lÃ¼nmede dÃ¼ÄŸÃ¼m iki parÃ§aya ayrÄ±lÄ±r ve sol/saÄŸ alt dÃ¼ÄŸÃ¼mlerin safsÄ±zlÄ±klarÄ± hesaplanÄ±r.

- TanÄ±mlar:
    - k: sÄ±nÄ±f sayÄ±sÄ±, T: mevcut dÃ¼ÄŸÃ¼mdeki Ã¶rnekler
    - |T_sol|, |T_saÄŸ|: sol/saÄŸ alt dÃ¼ÄŸÃ¼mlerdeki Ã¶rnek sayÄ±larÄ±
    - L_i: sol taraftaki iâ€™nci sÄ±nÄ±f adedi, R_i: saÄŸ taraftaki iâ€™nci sÄ±nÄ±f adedi

- Sol ve saÄŸ iÃ§in Gini safsÄ±zlÄ±ÄŸÄ±:

$$
\operatorname{Gini}_{sol} = 1 - \sum_{i=1}^{k} \left( \frac{L_i}{|T_{sol}|} \right)^2, \quad
\operatorname{Gini}_{saÄŸ} = 1 - \sum_{i=1}^{k} \left( \frac{R_i}{|T_{saÄŸ}|} \right)^2.
$$

- Aday bÃ¶lmenin aÄŸÄ±rlÄ±klÄ± Gini deÄŸeri (Ã¶zellik j iÃ§in): n eÄŸitim kaydÄ± varsa

$$
\operatorname{Gini}_j = \frac{1}{n}\Bigl( |T_{sol}|\,\operatorname{Gini}_{sol} + |T_{saÄŸ}|\,\operatorname{Gini}_{saÄŸ} \Bigr)
= \frac{|T_{sol}|}{n}\,\operatorname{Gini}_{sol} + \frac{|T_{saÄŸ}|}{n}\,\operatorname{Gini}_{saÄŸ}.
$$

- SeÃ§im: Her j niteliÄŸi iÃ§in hesaplanan \(\operatorname{Gini}_j\) deÄŸerleri arasÄ±ndan en kÃ¼Ã§Ã¼k olanÄ± seÃ§ilir ve bÃ¶lÃ¼nme bu nitelik/eÅŸik Ã¼zerinden yapÄ±lÄ±r. Sonra aynÄ± iÅŸlem alt dÃ¼ÄŸÃ¼mlerde tekrarlanÄ±r.

##### Ã–rnek: Gini ile Karar AÄŸacÄ±

> ![Gini Ã¶rnek veri â€” gÃ¶rsel 112](Images/112.jpg)
>
> ![Aday bÃ¶lÃ¼nmeler â€” gÃ¶rsel 113](Images/113.jpg)
>
> ![Sol/saÄŸ daÄŸÄ±lÄ±mlar â€” gÃ¶rsel 114](Images/114.jpg)
>
> ![Gini hesap adÄ±mlarÄ± â€” gÃ¶rsel 115](Images/115.jpg)
>
> ![En iyi bÃ¶lÃ¼nme â€” gÃ¶rsel 116](Images/116.jpg)
>
> ![Ara aÄŸaÃ§ â€” gÃ¶rsel 117](Images/117.jpg)
>
> ![Nihai aÄŸaÃ§ â€” gÃ¶rsel 118](Images/118.jpg)

Bu Ã¶rnekten elde edilen kurallar:

1) EÄER (YAÅ = GENÃ‡) Ä°SE (SONUÃ‡ = HAYIR)
2) EÄER (YAÅ = ORTA VEYA YAÅ = YAÅLI) VE (CÄ°NSÄ°YET = ERKEK) Ä°SE (SONUÃ‡ = EVET)
3) EÄER (YAÅ = ORTA VEYA YAÅ = YAÅLI) VE (CÄ°NSÄ°YET = KADIN) VE (YAÅ = YAÅLI) Ä°SE (SONUÃ‡ = EVET)
4) EÄER (YAÅ = ORTA VEYA YAÅ = YAÅLI) VE (CÄ°NSÄ°YET = KADIN) VE (YAÅ = ORTA) Ä°SE (SONUÃ‡ = HAYIR)

##### SayÄ±sal DeÄŸerler iÃ§in Gini AlgoritmasÄ±

SayÄ±sal nitelikler iÃ§in (Ã¶rn. YAÅ) C4.5â€™teki eÅŸik tarama yaklaÅŸÄ±mÄ±na benzer ÅŸekilde, her aday eÅŸik iÃ§in Gini Ã¶lÃ§Ã¼tÃ¼ hesaplanÄ±r ve en kÃ¼Ã§Ã¼k deÄŸeri veren eÅŸik seÃ§ilir.

- AdÄ±mlar:
    1) NÃ¼merik Ã¶zelliÄŸin benzersiz deÄŸerlerini sÄ±rala; ardÄ±ÅŸÄ±k deÄŸerlerin ortalarÄ±nÄ± aday eÅŸik olarak belirle.
    2) Her eÅŸik iÃ§in iki alt kÃ¼me oluÅŸtur (â‰¤ t, > t) ve sol/saÄŸ Gini deÄŸerlerini hesapla.
    3) AÄŸÄ±rlÄ±klÄ± Giniâ€™yi hesapla ve en kÃ¼Ã§Ã¼k deÄŸere sahip eÅŸiÄŸi seÃ§.

MEMNUN veri kÃ¼mesi (8 gÃ¶zlem) Ã¶rneÄŸi:

Ã–znitelik bazÄ±nda Gini (kategorikler):

- MODEL: \( Gini_{X5} = 1 - (0/2)^2 - (2/2)^2 = 0 \), \( Gini_{X3} = 1 - (3/6)^2 - (3/6)^2 = 0.5 \)
    - AÄŸÄ±rlÄ±klÄ±: \( Gini_{MODEL} = \tfrac{2\cdot 0 + 6\cdot 0.5}{8} = 0.375 \)
- CÄ°NSÄ°YET:
    - ERKEK: \( Gini = 1 - (2/5)^2 - (3/5)^2 = 0.48 \)
    - KADIN: \( Gini = 1 - (1/3)^2 - (2/3)^2 \approx 0.45 \)
    - AÄŸÄ±rlÄ±klÄ±: \( Gini_{C\Ä°NS\Ä°YET} = \tfrac{5\cdot 0.48 + 3\cdot 0.45}{8} \approx 0.469 \)

YAÅ iÃ§in aday eÅŸikler ve Gini deÄŸerleri:

| EÅŸik t | Sol (â‰¤ t) | SaÄŸ (> t) | Gini_sol | Gini_saÄŸ | AÄŸÄ±rlÄ±klÄ± Gini |
|:------:|:----------:|:---------:|:--------:|:--------:|:--------------:|
| 19 | (1E, 0H) | (2E, 5H) | 0.000 | 0.408 | 0.357 |
| 21 | (2E, 1H) | (1E, 4H) | 0.445 | 0.320 | 0.369 |
| 22 | (2E, 2H) | (1E, 3H) | 0.500 | 0.375 | 0.438 |
| 30 | (3E, 2H) | (0E, 3H) | 0.480 | 0.000 | 0.300 |
| 45 | ... | ... | â€” | â€” | 0.375 |
| 55 | ... | ... | â€” | â€” | 0.429 |
| 60 | ... | ... | â€” | â€” | 0.469 |

Ã–zet (en iyi eÅŸik ve en iyi Ã¶znitelik):

- En kÃ¼Ã§Ã¼k Gini deÄŸeri YAÅ iÃ§in t = 30â€™da: 0.300
- Ã–znitelik karÅŸÄ±laÅŸtÄ±rmasÄ±: MODEL = 0.375, CÄ°NSÄ°YET â‰ˆ 0.469, YAÅ = 0.300 â‡’ En iyi: YAÅ (t = 30)

> ![SayÄ±sal deÄŸerler iÃ§in Gini â€” gÃ¶rsel 119](Images/119.jpg)
>
> AÃ§Ä±klama: YAÅ iÃ§in aday eÅŸiklerde Gini Ã¶lÃ§Ã¼tleri ve Ã¶znitelik bazÄ±nda Ã¶zet; en kÃ¼Ã§Ã¼k deÄŸer t = 30 eÅŸiklidir.

#### Regresyon iÃ§in Ã–lÃ§Ã¼tler

- Varyans (MSE) AzalÄ±mÄ±: DÃ¼ÄŸÃ¼mdeki hedef deÄŸiÅŸken iÃ§in kare hatayÄ± azaltacak bÃ¶lme seÃ§ilir.
    - MSE(t) = (1/|t|) âˆ‘_{iâˆˆt} (y_i âˆ’ È³_t)^2
    - BÃ¶lme sonrasÄ± aÄŸÄ±rlÄ±klÄ± hata: MSE_s = (|L|/|t|)Â·MSE(L) + (|R|/|t|)Â·MSE(R)
    - KazanÃ§: Î” = MSE(t) âˆ’ MSE_s; en bÃ¼yÃ¼k Î” seÃ§ilir.

##### Regresyon AÄŸaÃ§larÄ± (detay)

Temel fikir: Her dÃ¼ÄŸÃ¼mde veri iki dala ayrÄ±lÄ±r ve her dal iÃ§in ortalama kare hata (MSE) hesaplanÄ±r. En kÃ¼Ã§Ã¼k toplam hata veren bÃ¶lme seÃ§ilir.

- Bir t dÃ¼ÄŸÃ¼mÃ¼nde n(t) Ã¶rnek iÃ§in:

$$
R(t) = \frac{1}{n(t)} \sum_{x_i\in t} \bigl(y_i - \bar y_t\bigr)^2, \quad \bar y_t = \frac{1}{n(t)} \sum_{x_i\in t} y_i.
$$

- Ä°kili bÃ¶lme ile t â†’ (t_{sol}, t_{saÄŸ}). Aday bÃ¶lme iÃ§in hesaplanÄ±r:

$$
R(t_{sol}) = \frac{1}{n(t_{sol})} \sum_{x_i\in t_{sol}} \bigl(y_i - \bar y_{t_{sol}}\bigr)^2,\quad
R(t_{saÄŸ}) = \frac{1}{n(t_{saÄŸ})} \sum_{x_i\in t_{saÄŸ}} \bigl(y_i - \bar y_{t_{saÄŸ}}\bigr)^2.
$$

SeÃ§im Ã¶lÃ§Ã¼tÃ¼: BÃ¶lme, \( R(t_{sol}) + R(t_{saÄŸ}) \) toplamÄ±nÄ± minimize edecek ÅŸekilde seÃ§ilir (eÅŸdeÄŸer olarak MSE_s minimize edilir).

###### Ã–rnek (kÃ¼Ã§Ã¼k veri seti)

Veri (Y hedefi):

| Ã–rnek | X1 | X2 | Y |
|:-----:|:--:|:--:|:--:|
| 1 | 12 | 34 | 44 |
| 2 | 18 | 25 | 35 |
| 3 | 11 | 18 | 30 |
| 4 | 9  | 23 | 45 |
| 5 | 15 | 31 | 43 |

Ã–rnek bÃ¶lme: X1 â‰¤ 12 (1. eleman referans alÄ±narak)

- t_{sol} = {44, 30, 45}, t_{saÄŸ} = {35, 43}
- \( \bar y_{t_{sol}} = (44+30+45)/3 = 39.67 \), \( \bar y_{t_{saÄŸ}} = (35+43)/2 = 39 \)
- \( R(t_{sol}) = \tfrac{(44-39.67)^2 + (30-39.67)^2 + (45-39.67)^2}{3} = 46.89 \)
- \( R(t_{saÄŸ}) = \tfrac{(35-39)^2 + (43-39)^2}{2} = 16 \)
- Toplam: \( R(t_{sol}) + R(t_{saÄŸ}) = 62.89 \)

TÃ¼m olasÄ± eÅŸiklerde benzer hesap yapÄ±larak aÅŸaÄŸÄ±daki tablo elde edilir:

| KoÅŸul | R(t_{sol}) + R(t_{saÄŸ}) |
|:------|:------------------------:|
| X1 â‰¤ 12 | 62.89 |
| X1 â‰¤ 18 | 34.64 |
| X1 â‰¤ 11 | 72.47 |
| X1 â‰¤ 9  | 33.50 |
| X1 â‰¤ 15 | 37.25 |
| X2 â‰¤ 34 | 34.64 |
| X2 â‰¤ 25 | 39.14 |
| X2 â‰¤ 18 | 15.69 |
| X2 â‰¤ 23 | 72.47 |
| X2 â‰¤ 31 | 36.69 |

En kÃ¼Ã§Ã¼k toplam hata X2 â‰¤ 18 koÅŸulundadÄ±r. BÃ¶lme noktasÄ±, bu deÄŸerin bir altÄ±ndaki sonraki deÄŸerle ortalamasÄ± alÄ±narak belirlenir:

$$
    	ext{BÃ¶lme NoktasÄ±} = \frac{18 + 23}{2} = 20.5.
$$

> ![Regresyon aÄŸacÄ± â€” Ã¶rnek gÃ¶rsel 120](Images/120.jpg)
>
> AÃ§Ä±klama: En iyi bÃ¶lme X2 â‰ˆ 20.5 eÅŸiÄŸindedir; dÃ¼ÄŸÃ¼mdeki Ã¶rnekler iki dala ayrÄ±larak toplam MSE minimize edilir.

#### Regresyon BaÅŸarÄ± Metrikleri

Regresyon modellerini deÄŸerlendirirken kullanÄ±lan baÅŸlÄ±ca metrikler MSE, RMSE ve RÂ²â€™dir. AyrÄ±ntÄ±lÄ± referans iÃ§in: scikitâ€‘learn â€” Regression metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics

> ![Regresyon metrikleri â€” genel bakÄ±ÅŸ 121](Images/121.jpg)

##### Ortalama Kare Hata (MSE) ve KÃ¶k Ortalama Kare Hata (RMSE)

- TanÄ±m:

$$
\operatorname{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat y_i)^2, \quad \operatorname{RMSE} = \sqrt{\operatorname{MSE}}.
$$

- Yorum:
    - MSE kare hatalarÄ± ortalar; bÃ¼yÃ¼k hatalarÄ± daha fazla cezalandÄ±rÄ±r (aykÄ±rÄ± deÄŸerlere duyarlÄ±dÄ±r).
    - RMSE, hedef deÄŸiÅŸkenin birimiyle aynÄ± birimdedir; yorumlamasÄ± daha sezgiseldir.

KÄ±sa Python Ã¶rneÄŸi (scikitâ€‘learn):

```python
from sklearn.metrics import mean_squared_error

y_true = [3.0, 2.0, 4.0, 5.0]
y_pred = [2.5, 2.1, 3.9, 4.8]

mse  = mean_squared_error(y_true, y_pred, squared=True)   # MSE
rmse = mean_squared_error(y_true, y_pred, squared=False)  # RMSE

print({"mse": mse, "rmse": rmse})
```

> ![MSE ve RMSE â€” sezgisel karÅŸÄ±laÅŸtÄ±rma 122](Images/122.jpg)

##### Belirleme KatsayÄ±sÄ± (RÂ²)

- TanÄ±m (toplam sapma SS_tot ve artÄ±k sapma SS_res ile):

$$
R^2 = 1 - \frac{\sum_i (y_i - \hat y_i)^2}{\sum_i (y_i - \bar y)^2} = 1 - \frac{\operatorname{SS}_{res}}{\operatorname{SS}_{tot}}, \quad \bar y = \frac{1}{n}\sum_i y_i.
$$

- Yorum:
    - RÂ² â‰ˆ 1: Model, varyansÄ±n bÃ¼yÃ¼k kÄ±smÄ±nÄ± aÃ§Ä±klar (iyi uyum). 0: Sadece ortalama tahmini kadar. < 0: Ortalama tahmininden daha kÃ¶tÃ¼.
    - Ã‡oklu doÄŸrusal regresyonda RÂ², Ã¶zellik sayÄ±sÄ±yla artma eÄŸilimindedir; bu yÃ¼zden karÅŸÄ±laÅŸtÄ±rmalarda dÃ¼zeltilmiÅŸ RÂ² (Adjusted RÂ²) tercih edilebilir.

KÄ±sa Python Ã¶rneÄŸi (scikitâ€‘learn):

```python
from sklearn.metrics import r2_score

y_true = [3.0, 2.0, 4.0, 5.0]
y_pred = [2.5, 2.1, 3.9, 4.8]

r2 = r2_score(y_true, y_pred)
print({"r2": r2})
```

> ![RÂ² metrik aÃ§Ä±klamasÄ± 123](Images/123.jpg)

Notlar:
- FarklÄ± metrikler farklÄ± hedefleri optimize eder; Ã¶rneÄŸin MAE aykÄ±rÄ± deÄŸerlere daha dayanÄ±klÄ±dÄ±r, MSE/RMSE bÃ¼yÃ¼k hatalarÄ± daha Ã§ok cezalandÄ±rÄ±r.
- Model seÃ§imi/karÅŸÄ±laÅŸtÄ±rmasÄ± iÃ§in aynÄ± veri bÃ¶lmeleri (Ã¶rn. kâ€‘katlÄ± Ã‡apraz DoÄŸrulama) Ã¼zerinde aynÄ± metrikleri kullanÄ±n; veri sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nleyin.

> ![Regresyon metrikleri â€” Ã¶rnek gÃ¶rsel 124](Images/124.jpg)


#### Uygulama NotlarÄ±

- NÃ¼merik nitelikler: DeÄŸerleri sÄ±rala; ardÄ±ÅŸÄ±k benzersiz deÄŸerlerin ortalarÄ±nÄ± eÅŸik adayÄ± olarak dene.
- Kategorik nitelikler: TÃ¼m alt kÃ¼me bÃ¶lmeleri Ã¼stel sayÄ±da olabilir; pratikte sÄ±nÄ±f-odaklÄ± sezgisel aramalar veya hedef sayÄ±sÄ± kÃ¼Ã§Ã¼kse tam tarama yapÄ±lÄ±r.
- Budama: CART tipik olarak cost-complexity pruning (Î±-uyumlu budama) uygular; doÄŸrulama seti ile Î± seÃ§ilebilir.

> ![CART â€” Ã¶rnek gÃ¶rsel 105](Images/105.jpg)
>
> AÃ§Ä±klama: CARTâ€™ta her dÃ¼ÄŸÃ¼m ikili bÃ¶lÃ¼nÃ¼r; Ã¶rnekte, seÃ§ilen nitelik ve eÅŸik ile veri iki dala ayrÄ±lmÄ±ÅŸ, Gini/Twoing veya MSE azalÄ±ma gÃ¶re en iyi bÃ¶lme tercih edilmiÅŸtir.

### Kural Ã‡Ä±kartma (Rule Extraction)

> ![Kural Ã§Ä±kartma â€” gÃ¶rsel 102](Images/102.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 102 â€” Karar aÄŸacÄ±ndan tÃ¼retilen kurallarÄ±n Ã¶rnek gÃ¶sterimi.

- Karar aÄŸacÄ±ndan elde edilen kurallar (Ã¶rnek):
    1. EÄER (NÄ°TELÄ°K1 = a) VE (NÄ°TELÄ°K2 = EÅŸit veya KÃ¼Ã§Ã¼k) Ä°SE (SINIF = SÄ±nÄ±f1)
    2. EÄER (NÄ°TELÄ°K1 = a) VE (NÄ°TELÄ°K2 = BÃ¼yÃ¼k) Ä°SE (SINIF = SÄ±nÄ±f2)
    3. EÄER (NÄ°TELÄ°K1 = b) Ä°SE (SINIF = SÄ±nÄ±f1)
    4. EÄER (NÄ°TELÄ°K1 = c) VE (NÄ°TELÄ°K3 = yanlÄ±ÅŸ) Ä°SE (SINIF = SÄ±nÄ±f1)
    5. EÄER (NÄ°TELÄ°K1 = c) VE (NÄ°TELÄ°K3 = doÄŸru) Ä°SE (SINIF = SÄ±nÄ±f2)

### Bilinmeyen Ã–znitelik DeÄŸerleri (Missing Values)

Eksik/kayÄ±p deÄŸer iÃ§eren nitelikler iÃ§in bilgi kazancÄ± hesaplanÄ±rken aÅŸaÄŸÄ±daki yaklaÅŸÄ±m kullanÄ±labilir:

- KayÄ±plÄ± Ã¶rnekler hariÃ§ tutularak \( H(T) \) ve \( H(T\mid X) \) normal ÅŸekilde hesaplanÄ±r (sadece bilinen X deÄŸerleri ile).
- SonuÃ§, kayÄ±psÄ±z Ã¶rnek oranÄ± \( F \) ile Ã¶lÃ§eklenir:

$$
F = \frac{\text{X deÄŸeri bilinen Ã¶rnek sayÄ±sÄ±}}{\text{toplam Ã¶rnek sayÄ±sÄ±}}, \quad \operatorname{Gain}_X = F\,\bigl( H(T) - H(T\mid X) \bigr).
$$

Ã–rnek veri kÃ¼mesi (NÄ°TELÄ°K1â€™de bir eksik â€œ?â€ deÄŸer var, toplam 14 kayÄ±t):

| NÄ°TELÄ°K1 | NÄ°TELÄ°K2 | NÄ°TELÄ°K3 | SINIF  |
|----------|----------|----------|--------|
| a        | 70       | DoÄŸru    | SÄ±nÄ±f1 |
| a        | 90       | DoÄŸru    | SÄ±nÄ±f2 |
| a        | 85       | YanlÄ±ÅŸ   | SÄ±nÄ±f2 |
| a        | 95       | YanlÄ±ÅŸ   | SÄ±nÄ±f2 |
| a        | 70       | YanlÄ±ÅŸ   | SÄ±nÄ±f1 |
| ?        | 90       | DoÄŸru    | SÄ±nÄ±f1 |
| b        | 78       | YanlÄ±ÅŸ   | SÄ±nÄ±f1 |
| b        | 65       | DoÄŸru    | SÄ±nÄ±f1 |
| b        | 75       | YanlÄ±ÅŸ   | SÄ±nÄ±f1 |
| c        | 80       | DoÄŸru    | SÄ±nÄ±f2 |
| c        | 70       | DoÄŸru    | SÄ±nÄ±f2 |
| c        | 80       | YanlÄ±ÅŸ   | SÄ±nÄ±f1 |
| c        | 80       | YanlÄ±ÅŸ   | SÄ±nÄ±f1 |
| c        | 96       | YanlÄ±ÅŸ   | SÄ±nÄ±f1 |

 Hedef entropisi (tÃ¼m 14 kayÄ±t, SINIF daÄŸÄ±lÄ±mÄ±: 9 SÄ±nÄ±f1, 5 SÄ±nÄ±f2):

$$
H(\text{SINIF}) = -\Bigl( \tfrac{9}{14}\log_2 \tfrac{9}{14} + \tfrac{5}{14}\log_2 \tfrac{5}{14} \Bigr) \approx 0{,}94.
$$

Not: Hedef entropisi H(SINIF) tÃ¼m 14 kayÄ±tla hesaplanÄ±r. AÅŸaÄŸÄ±da koÅŸullu entropi yalnÄ±zca NÄ°TELÄ°K1 deÄŸeri bilinen 13 kayÄ±t Ã¼zerinde hesaplanÄ±r.

KoÅŸullu entropi (yalnÄ±z NÄ°TELÄ°K1â€™i bilinen 13 kayÄ±t):

\( H(\text{SINIF}\mid \text{NÄ°TELÄ°K1}) \approx 0{,}747 \)

Ã–lÃ§ek faktÃ¶rÃ¼ ve ayarlanmÄ±ÅŸ kazanÃ§:

$$
F = \tfrac{13}{14}, \quad \operatorname{Gain}(\text{NÄ°TELÄ°K1},\text{SINIF}) = F\,\bigl(0{,}961 - 0{,}747\bigr) \approx 0{,}199.
$$

### Ezber (Overfitting) ve Budama

TÃ¼m makine Ã¶ÄŸrenmesi yÃ¶ntemlerinde olduÄŸu gibi, karar aÄŸaÃ§larÄ± da uygun Ã¶nlemler alÄ±nmadÄ±ÄŸÄ±nda ezber (overfitting) yapabilir. Bu nedenle aÄŸaÃ§ oluÅŸturulurken/sonrasÄ±nda budama teknikleri (Ã¶r. cost-complexity pruning, minimum yaprak Ã¶rneÄŸi, maksimum derinlik) kullanÄ±lmalÄ±dÄ±r.

> ![Overfitting ve budama â€” gÃ¶rsel 103](Images/103.jpg)
>
> AÃ§Ä±klama: GÃ¶rsel 103 â€” AÅŸÄ±rÄ± Ã¶ÄŸrenme etkisi ve budama ile modelin sadeleÅŸtirilmesi.

#### AÄŸaÃ§ Budama

Budama, sÄ±nÄ±flandÄ±rmaya kayda deÄŸer katkÄ±sÄ± olmayan bÃ¶lÃ¼mlerin karar aÄŸacÄ±ndan Ã§Ä±karÄ±lmasÄ± iÅŸlemidir. Bu sayede aÄŸaÃ§ hem daha sade hem de daha anlaÅŸÄ±labilir hale gelir. Ä°ki temel budama yaklaÅŸÄ±mÄ± vardÄ±r:

- Ã–n budama (pre-pruning): AÄŸaÃ§ oluÅŸturulurken uygulanÄ±r. BÃ¶lÃ¼nme, belirlenen bir eÅŸik saÄŸlanmÄ±yorsa (Ã¶r. minimum bilgi kazancÄ±, minimum Ã¶rnek, maksimum derinlik, hata toleransÄ±) o noktada durdurulur ve eldeki alt kÃ¼menin baskÄ±n sÄ±nÄ±f etiketi yaprak olarak atanÄ±r.
- Sonradan budama (post-pruning): AÄŸaÃ§ tamamen kurulduktan sonra uygulanÄ±r. Alt aÄŸaÃ§larÄ± silerek yaprak oluÅŸturma (subtree replacement), alt aÄŸaÃ§larÄ± yÃ¼kseltme (subtree raising) ve dal kesme gibi tekniklerle sadeleÅŸtirme yapÄ±lÄ±r; karar genellikle doÄŸrulama kÃ¼mesi performansÄ± ya da maliyetâ€“karmaÅŸÄ±klÄ±k (cost-complexity) Ã¶lÃ§Ã¼tÃ¼ne gÃ¶re verilir.

##### Ã–rnek: Hata toleransÄ± ile budama

> ![AÄŸaÃ§ budama â€” gÃ¶rsel 104](Images/104.jpg)
>
> AÃ§Ä±klama: Hata toleransÄ± %33 seÃ§ilirse â€œNemâ€ dÃ¼ÄŸÃ¼mÃ¼nÃ¼n alt dallarÄ±ndaki â€œEvetâ€ oranÄ± %30â€™dur. Bu yÃ¼zden â€œNemâ€ dÃ¼ÄŸÃ¼mÃ¼ budanÄ±r ve yerine â€œHayÄ±râ€ yapraÄŸÄ± konur.


